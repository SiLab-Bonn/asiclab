{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"admin/","text":"\"Numbers everyone should know\" Action Time (ns) Time (ms) L1 cache reference 0.5 ns Branch mispredict 5 ns L2 cache reference 7 ns Mutex lock/unlock 100 ns Main memory reference 100 ns Compress 1K bytes with Zippy 10,000 ns 0.01 ms Send 1K bytes over 1 Gbps network 10,000 ns 0.01 ms Read 1 MB sequentially from memory 250,000 ns 0.25 ms Round trip within same datacenter 500,000 ns 0.5 ms Disk seek 10,000,000 ns 10 ms Read 1 MB sequentially from network 10,000,000 ns 10 ms Read 1 MB sequentially from disk 30,000,000 ns 30 ms Send packet CA->Netherlands->CA 150,000,000 ns 150 ms Admin Experiments Untarring a 650 GB file on Penelope produces the following system usage: The read/write I/O jumps to 40MB/s and 150 MB/S. But essentially no CPU or RAM (<5%) is used. Comparing against our Toshiba MG08 drives, which have these specs: Available Capacities 16 TB, 14 TB Form Factor 3.5-inch Buffer Size 512 MiB Rotation Speed 7200rpm Maximum Data Transper Speed (Sustained)(Typ.) 16 TB: 262 MiB/s 14 TB: 248 MiB/s Power Consumption ( Idle - A ) SATA: 4.00W SAS: 4.46W MTTF/MTBF 2 500 000 h Weight ( Max ) 720g Thus we're using ~60% of our max transfer speed? Do we have to include Read and Write? How does Raid6 config affect this? Untarring a 650 GB file took the following time: 628GiB 8:13:34 [21.7MiB/s] Solution to slowdown, and updating To upgrade to new version of Fedora Server: sudo dnf upgrade --refresh sudo dnf install dnf-plugin-system-upgrade # was already there sudo dnf system-upgrade download --releasever=38 # may need --allow-erasing for 3rd party packages This link contains info about a similar problem, but I don't want to follow the approach of increaing the volume. [asiclab@penelope home]$ sudo pvdisplay [sudo] password for asiclab: --- Physical volume --- PV Name /dev/sda3 VG Name fedora_penelope PV Size 893.25 GiB / not usable 0 Allocatable yes PE Size 4.00 MiB Total PE 228672 Free PE 224832 Allocated PE 3840 PV UUID kJwKhC-leMj-VY6E-adbz-LRvT-Uczc-u1dQds [asiclab@penelope home]$ sudo vgdisplay --- Volume group --- VG Name fedora_penelope System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 2 VG Access read/write VG Status resizable MAX LV 0 Cur LV 1 Open LV 1 Max PV 0 Cur PV 1 Act PV 1 VG Size 893.25 GiB PE Size 4.00 MiB Total PE 228672 Alloc PE / Size 3840 / 15.00 GiB Free PE / Size 224832 / 878.25 GiB VG UUID 7pNs2s-G7fX-blRP-GDlM-YGFH-fNLa-o0v1uX [asiclab@penelope home]$ sudo lvdisplay --- Logical volume --- LV Path /dev/fedora_penelope/root LV Name root VG Name fedora_penelope LV UUID 74tyF1-DvwS-9ODy-agNq-XcAN-4OOS-MaVAU1 LV Write Access read/write LV Creation host, time penelope.physik.uni-bonn.de, 2023-01-13 18:29:57 +0100 LV Status available # open 1 LV Size 15.00 GiB Current LE 3840 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 253:0 Other useful information can be found here: [asiclab@penelope home]$ df -hT Filesystem Type Size Used Avail Use% Mounted on devtmpfs devtmpfs 4.0M 0 4.0M 0% /dev tmpfs tmpfs 32G 0 32G 0% /dev/shm tmpfs tmpfs 13G 1.6M 13G 1% /run /dev/mapper/fedora_penelope-root xfs 15G 15G 638M 96% / tmpfs tmpfs 32G 0 32G 0% /tmp /dev/sda2 xfs 960M 320M 641M 34% /boot /dev/md127 ext4 44T 9.9T 32T 24% /mnt/md127 tmpfs tmpfs 6.3G 4.0K 6.3G 1% /run/user/1000 [asiclab@asiclabwin001 ~]$ df -hT # note this is for other server Filesystem Type Size Used Avail Use% Mounted on devtmpfs devtmpfs 4.0M 0 4.0M 0% /dev tmpfs tmpfs 7.8G 1.1G 6.7G 14% /dev/shm tmpfs tmpfs 3.1G 1.3M 3.1G 1% /run /dev/mapper/fedora_asiclabwin001-root xfs 15G 2.6G 13G 18% / tmpfs tmpfs 7.8G 48K 7.8G 1% /tmp /dev/sda2 xfs 960M 280M 681M 30% /boot /dev/sda1 vfat 599M 7.1M 592M 2% /boot/efi tmpfs tmpfs 1.6G 0 1.6G 0% /run/user/1000 From this link : To see what file systems are mounted and how much they\u2019re being used: df -hT To get an overview of all your block devices and logical volumes and where they all reside: lsblk When working with Logical Volumes you will see the following terms: PV (Physical Volume): This refers to a physical disk, or a partition on the disk. If you used a tool like fdisk to make 2 partitions on a disk they will show as two PVs. You can list them with the command pvs or get more info with pvdisplay VG (Volume Group): You add your PVs to volume groups and with will be the pool used for the logical volumes in that group. You can list your VGs with vgs and get more info with vgdisplay LV (Logical Volume): This is where the magic happens. An LV can be resized, extended, span multiple disks. Once you create an LV you would next format it with a file system. You can list them with lvs`` and get more info with lvdisplay```. To answer your question, I\u2019d recommend extending / to like 20G and then creating a new LV for the rest of your data you plan to store via SAMBA. To extend root to 20G. The -r option tells it to resize the file system too: lvextend -L20G -r /dev/mapper/fedora_fedora-root To create a new VG called \u201cfedora-shares\u201d: lvcreate -L 190G -n fedora-shares fedora Extend it to use the full disk: lvextend -l+100%FREE /dev/mapper/fedora_fedora-shares Format is with a file system: mkfs.xfs /dev/mapper/fedora_fedora-shares Create a mountpoint and add it to your /etc/fstab: mkdir -p /srv/shares echo \"/dev/mapper/fedora_fedora-shares /srv/shares xfs defaults 0 0\" >> /etc/fstab Test it by mounting it: mount -a Finally, use df and lsblk to verify the new LV is mounted and ready to use Why is the volume, mounted at root \"\\\" so small, only 15GB in Fedora? The shortcut is lvextend -L +10G /dev/mapper/fedora_fedora-root to make it bigger, but maybe this isn't needed? This is default behaviour of Fedora Server -- the root filesystem will be 15 GiB and rest of the disk space is left unused for the user to either resize the root logical volume or use for different use case (for /var or virtualization etc.). If you want a different storage layout, you need to use the custom partitioning in the installer and create the mountpoints manually . One of the reasons is that the XFS filesystem used by Fedora Server (and only server, Workstation and other flavours use btrfs) cannot be shrunk so if the installer uses the entire free space, it will be really hard to change the default layout. If you want to resize your root filesystem you can use lvextend -L+<size> --resizefs fedora_fedora/root . Where <size> can be for example 50G for 50 GiB. Edit: The --resizefs is important, without this the lvresize command will resize only the volume and not the filesystem on it. If you run lvresize without the --resizefs option you can resize the filesystem afterwards with xfs_growfs /dev/mapper/fedora_fedora-root . It\u2019s possibly worth also noting that 15GB is not really all that \u2018small\u2019 either for a root filesystem. Not counting application data and swap space, it\u2019s not unusual for a server system to need less than 4 GB of disk space. Fedora will run just fine in that 15GB of space as long as you split out application data properly and make sure old logs are getting cleaned up. Alrighty, so looking at my files, I should just figure out where the 15GB are being used. At least in my ~/home/asiclab directory, I have the large apptainer file. Also, looking at /var I see lots of big stuff! [asiclab@penelope ~]$ sudo du -sh bag3++_centos7.sif 3.7G bag3++_centos7.sif [asiclab@penelope ~]$ sudo du -shc /var/* 0 /var/account 0 /var/adm 0 /var/apptainer 507M /var/cache 0 /var/crash 0 /var/db 0 /var/empty 0 /var/ftp 0 /var/games 0 /var/kerberos 2.2G /var/lib 0 /var/local 0 /var/lock 1.8G /var/log 0 /var/mail 0 /var/nis 0 /var/opt 0 /var/preserve 0 /var/run 2.5M /var/spool 248M /var/tmp 0 /var/yp 4.7G total Okay, and checking /var/ showed the biggest thing is my journal: 4.6G /var 2.2G /var/lib 1.7G /var/log 1.5G /var/log/journal/2a72480699d641d881e7666c5429fb5e 1.5G /var/log/journal 1.1G /var/lib/plocate 1.1G /var/lib/dnf/system-upgrade 1.1G /var/lib/dnf 630M /var/lib/dnf/system-upgrade/updates-b7ba662710b98f1a 617M /var/lib/dnf/system-upgrade/updates-b7ba662710b98f1a/packages An confirming this: [asiclab@penelope 2a72480699d641d881e7666c5429fb5e]$ journalctl --disk-usage Archived and active journals take up 1.4G in the file system. The files from /var/log/journal directory can be removed. The nicest method I've found is: sudo journalctl --vacuum-size=100M which deletes old log-files from /var/log/journal until total size of the directory becomes under specified threshold (500 megabytes in this example). Vacuuming done, freed 1.3G of archived journals from /var/log/journal/2a72480699d641d881e7666c5429fb5e. Sweet! Making it permanent Wow, wait this article pretty much contains everything I needed, in the first place! https://superuser.com/questions/1470997/available-space-on-but-home-is-running-out-of-space https://serverfault.com/questions/848914/how-to-reconfigure-dev-mapper-space https://unix.stackexchange.com/questions/728955/why-is-the-root-filesystem-so-small-on-a-clean-fedora-37-install LVM is somewhat different from regular partitions. With regular partitions resizing is a PITA because they must be contigious. LVM logical volumes do not have to be contiguous so you can have multiple logical volumes and expand them as and when needed. Returning to the DNF upgrade https://docs.fedoraproject.org/en-US/quick-docs/dnf-system-upgrade/ Installed the new version for Fedora, which took like 25 minutes to reboot. Auto-removed some old packages. Left the two older kernel versions alone, as they only take up like 300MB total. Now checking: [asiclab@penelope ~]$ df -hT Filesystem Type Size Used Avail Use% Mounted on devtmpfs devtmpfs 4.0M 0 4.0M 0% /dev tmpfs tmpfs 32G 0 32G 0% /dev/shm tmpfs tmpfs 13G 1.6M 13G 1% /run /dev/mapper/fedora_penelope-root xfs 15G 12G 3.2G 79% / tmpfs tmpfs 32G 0 32G 0% /tmp /dev/sda2 xfs 960M 323M 638M 34% /boot /dev/md127 ext4 44T 9.9T 32T 24% /mnt/md127 tmpfs tmpfs 6.3G 4.0K 6.3G 1% /run/user/1000 We can do better. /var/lib/plocate is 1.1G. Its a database of all files in your root directory. It is used by locate utility. It's big because I have a whole CentOS7 container in my home directory. Cleaning it with sudo updatedb then cleaned the size of plocate directory. Doubling server speed, with network bonding? https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_networking/configuring-network-bonding_configuring-and-managing-networking https://jfearn.fedorapeople.org/fdocs/en-US/Fedora/20/html/Networking_Guide/sec-Using_Channel_Bonding.html https://docs.fedoraproject.org/en-US/fedora-coreos/sysconfig-network-configuration/ Hardware settings and config The older machines are Lenovo E30 machines, with a IS6XM mother board, with a i7-2600 CPU. The 256 SSH should be connected to the first sata port: SATA 1. In the boot configuration, the USB key should be first, and SSD should be second. BIOS updates can be found here: https://support.lenovo.com/my/en/downloads/DS018245 Press CTRL ALT delete to get back to splash screen from error 1962. Ideally I would enable Compatibility Support Module (CSM) but I guess this isn't supported in my BIOS version. This answer indicates I probably need to update the BIOS: https://askubuntu.com/questions/1414366/no-operating-system-found-after-clean-install-of-ubuntu-22-04 fdisk and gdisk could be a good way to examine the structure of the boot disk. I need to understand what a EFI partition is. The reason this process worked on asiclab00 is because it has slightly older firmware, I believe, and it didn't install a EFI partition. It installed as a legacy system. I don't think this can be done on Note that fwupgrade requires UEFI, and sudo efibootmgr -c -L \"Windows Boot Manager\" -l \"\\EFI\\fedora\\grubx64.efi\" sudo efibootmgr -c -d /dev/sdX -p Y -L \"Windows Boot Manager\" -l \"\\EFI\\path\\file.efi\" sudo efibootmgr -c -d /dev/sda1 -p 1 -L \"Windows Boot Manager\" -l \"\\EFI\\boot\\grubx64.efi\" default is sda so, -d flat isn't really needed. Also default partition -p is 1 so also not needed The command that appears to work, assuming /dev/sda1 is the partition of interest: sudo efibootmgr -c -L \"Windows Boot Manager\" -l \"\\EFI\\fedora\\shimx64.efi\" https://forums.fedoraforum.org/showthread.php?317210-EFI-boot-issues-Lenovo-AIO-Error-1962-No-Operating-System-Found https://unix.stackexchange.com/questions/324753/pernicious-1962-error-installing-fedora-server-24-no-operating-system-found https://www.reddit.com/r/ManjaroLinux/comments/e682d6/fixing_lenovos_error_code_1962_by_spoofing_the/ https://superuser.com/questions/913779/how-can-i-know-which-partition-is-efi-system-partition make sure I can still ping asiclab010, as this is yannik's machine?","title":"Admin"},{"location":"admin/#admin-experiments","text":"Untarring a 650 GB file on Penelope produces the following system usage: The read/write I/O jumps to 40MB/s and 150 MB/S. But essentially no CPU or RAM (<5%) is used. Comparing against our Toshiba MG08 drives, which have these specs: Available Capacities 16 TB, 14 TB Form Factor 3.5-inch Buffer Size 512 MiB Rotation Speed 7200rpm Maximum Data Transper Speed (Sustained)(Typ.) 16 TB: 262 MiB/s 14 TB: 248 MiB/s Power Consumption ( Idle - A ) SATA: 4.00W SAS: 4.46W MTTF/MTBF 2 500 000 h Weight ( Max ) 720g Thus we're using ~60% of our max transfer speed? Do we have to include Read and Write? How does Raid6 config affect this?","title":"Admin Experiments"},{"location":"admin/#untarring-a-650-gb-file-took-the-following-time","text":"628GiB 8:13:34 [21.7MiB/s]","title":"Untarring a 650 GB file took the following time:"},{"location":"admin/#solution-to-slowdown-and-updating","text":"","title":"Solution to slowdown, and updating"},{"location":"admin/#to-upgrade-to-new-version-of-fedora-server","text":"sudo dnf upgrade --refresh sudo dnf install dnf-plugin-system-upgrade # was already there sudo dnf system-upgrade download --releasever=38 # may need --allow-erasing for 3rd party packages This link contains info about a similar problem, but I don't want to follow the approach of increaing the volume. [asiclab@penelope home]$ sudo pvdisplay [sudo] password for asiclab: --- Physical volume --- PV Name /dev/sda3 VG Name fedora_penelope PV Size 893.25 GiB / not usable 0 Allocatable yes PE Size 4.00 MiB Total PE 228672 Free PE 224832 Allocated PE 3840 PV UUID kJwKhC-leMj-VY6E-adbz-LRvT-Uczc-u1dQds [asiclab@penelope home]$ sudo vgdisplay --- Volume group --- VG Name fedora_penelope System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 2 VG Access read/write VG Status resizable MAX LV 0 Cur LV 1 Open LV 1 Max PV 0 Cur PV 1 Act PV 1 VG Size 893.25 GiB PE Size 4.00 MiB Total PE 228672 Alloc PE / Size 3840 / 15.00 GiB Free PE / Size 224832 / 878.25 GiB VG UUID 7pNs2s-G7fX-blRP-GDlM-YGFH-fNLa-o0v1uX [asiclab@penelope home]$ sudo lvdisplay --- Logical volume --- LV Path /dev/fedora_penelope/root LV Name root VG Name fedora_penelope LV UUID 74tyF1-DvwS-9ODy-agNq-XcAN-4OOS-MaVAU1 LV Write Access read/write LV Creation host, time penelope.physik.uni-bonn.de, 2023-01-13 18:29:57 +0100 LV Status available # open 1 LV Size 15.00 GiB Current LE 3840 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 253:0 Other useful information can be found here: [asiclab@penelope home]$ df -hT Filesystem Type Size Used Avail Use% Mounted on devtmpfs devtmpfs 4.0M 0 4.0M 0% /dev tmpfs tmpfs 32G 0 32G 0% /dev/shm tmpfs tmpfs 13G 1.6M 13G 1% /run /dev/mapper/fedora_penelope-root xfs 15G 15G 638M 96% / tmpfs tmpfs 32G 0 32G 0% /tmp /dev/sda2 xfs 960M 320M 641M 34% /boot /dev/md127 ext4 44T 9.9T 32T 24% /mnt/md127 tmpfs tmpfs 6.3G 4.0K 6.3G 1% /run/user/1000 [asiclab@asiclabwin001 ~]$ df -hT # note this is for other server Filesystem Type Size Used Avail Use% Mounted on devtmpfs devtmpfs 4.0M 0 4.0M 0% /dev tmpfs tmpfs 7.8G 1.1G 6.7G 14% /dev/shm tmpfs tmpfs 3.1G 1.3M 3.1G 1% /run /dev/mapper/fedora_asiclabwin001-root xfs 15G 2.6G 13G 18% / tmpfs tmpfs 7.8G 48K 7.8G 1% /tmp /dev/sda2 xfs 960M 280M 681M 30% /boot /dev/sda1 vfat 599M 7.1M 592M 2% /boot/efi tmpfs tmpfs 1.6G 0 1.6G 0% /run/user/1000 From this link : To see what file systems are mounted and how much they\u2019re being used: df -hT To get an overview of all your block devices and logical volumes and where they all reside: lsblk When working with Logical Volumes you will see the following terms: PV (Physical Volume): This refers to a physical disk, or a partition on the disk. If you used a tool like fdisk to make 2 partitions on a disk they will show as two PVs. You can list them with the command pvs or get more info with pvdisplay VG (Volume Group): You add your PVs to volume groups and with will be the pool used for the logical volumes in that group. You can list your VGs with vgs and get more info with vgdisplay LV (Logical Volume): This is where the magic happens. An LV can be resized, extended, span multiple disks. Once you create an LV you would next format it with a file system. You can list them with lvs`` and get more info with lvdisplay```. To answer your question, I\u2019d recommend extending / to like 20G and then creating a new LV for the rest of your data you plan to store via SAMBA. To extend root to 20G. The -r option tells it to resize the file system too: lvextend -L20G -r /dev/mapper/fedora_fedora-root To create a new VG called \u201cfedora-shares\u201d: lvcreate -L 190G -n fedora-shares fedora Extend it to use the full disk: lvextend -l+100%FREE /dev/mapper/fedora_fedora-shares Format is with a file system: mkfs.xfs /dev/mapper/fedora_fedora-shares Create a mountpoint and add it to your /etc/fstab: mkdir -p /srv/shares echo \"/dev/mapper/fedora_fedora-shares /srv/shares xfs defaults 0 0\" >> /etc/fstab Test it by mounting it: mount -a Finally, use df and lsblk to verify the new LV is mounted and ready to use Why is the volume, mounted at root \"\\\" so small, only 15GB in Fedora? The shortcut is lvextend -L +10G /dev/mapper/fedora_fedora-root to make it bigger, but maybe this isn't needed? This is default behaviour of Fedora Server -- the root filesystem will be 15 GiB and rest of the disk space is left unused for the user to either resize the root logical volume or use for different use case (for /var or virtualization etc.). If you want a different storage layout, you need to use the custom partitioning in the installer and create the mountpoints manually . One of the reasons is that the XFS filesystem used by Fedora Server (and only server, Workstation and other flavours use btrfs) cannot be shrunk so if the installer uses the entire free space, it will be really hard to change the default layout. If you want to resize your root filesystem you can use lvextend -L+<size> --resizefs fedora_fedora/root . Where <size> can be for example 50G for 50 GiB. Edit: The --resizefs is important, without this the lvresize command will resize only the volume and not the filesystem on it. If you run lvresize without the --resizefs option you can resize the filesystem afterwards with xfs_growfs /dev/mapper/fedora_fedora-root . It\u2019s possibly worth also noting that 15GB is not really all that \u2018small\u2019 either for a root filesystem. Not counting application data and swap space, it\u2019s not unusual for a server system to need less than 4 GB of disk space. Fedora will run just fine in that 15GB of space as long as you split out application data properly and make sure old logs are getting cleaned up. Alrighty, so looking at my files, I should just figure out where the 15GB are being used. At least in my ~/home/asiclab directory, I have the large apptainer file. Also, looking at /var I see lots of big stuff! [asiclab@penelope ~]$ sudo du -sh bag3++_centos7.sif 3.7G bag3++_centos7.sif [asiclab@penelope ~]$ sudo du -shc /var/* 0 /var/account 0 /var/adm 0 /var/apptainer 507M /var/cache 0 /var/crash 0 /var/db 0 /var/empty 0 /var/ftp 0 /var/games 0 /var/kerberos 2.2G /var/lib 0 /var/local 0 /var/lock 1.8G /var/log 0 /var/mail 0 /var/nis 0 /var/opt 0 /var/preserve 0 /var/run 2.5M /var/spool 248M /var/tmp 0 /var/yp 4.7G total Okay, and checking /var/ showed the biggest thing is my journal: 4.6G /var 2.2G /var/lib 1.7G /var/log 1.5G /var/log/journal/2a72480699d641d881e7666c5429fb5e 1.5G /var/log/journal 1.1G /var/lib/plocate 1.1G /var/lib/dnf/system-upgrade 1.1G /var/lib/dnf 630M /var/lib/dnf/system-upgrade/updates-b7ba662710b98f1a 617M /var/lib/dnf/system-upgrade/updates-b7ba662710b98f1a/packages An confirming this: [asiclab@penelope 2a72480699d641d881e7666c5429fb5e]$ journalctl --disk-usage Archived and active journals take up 1.4G in the file system. The files from /var/log/journal directory can be removed. The nicest method I've found is: sudo journalctl --vacuum-size=100M which deletes old log-files from /var/log/journal until total size of the directory becomes under specified threshold (500 megabytes in this example). Vacuuming done, freed 1.3G of archived journals from /var/log/journal/2a72480699d641d881e7666c5429fb5e. Sweet! Making it permanent Wow, wait this article pretty much contains everything I needed, in the first place! https://superuser.com/questions/1470997/available-space-on-but-home-is-running-out-of-space https://serverfault.com/questions/848914/how-to-reconfigure-dev-mapper-space https://unix.stackexchange.com/questions/728955/why-is-the-root-filesystem-so-small-on-a-clean-fedora-37-install LVM is somewhat different from regular partitions. With regular partitions resizing is a PITA because they must be contigious. LVM logical volumes do not have to be contiguous so you can have multiple logical volumes and expand them as and when needed.","title":"To upgrade to new version of Fedora Server:"},{"location":"admin/#returning-to-the-dnf-upgrade","text":"https://docs.fedoraproject.org/en-US/quick-docs/dnf-system-upgrade/ Installed the new version for Fedora, which took like 25 minutes to reboot. Auto-removed some old packages. Left the two older kernel versions alone, as they only take up like 300MB total. Now checking: [asiclab@penelope ~]$ df -hT Filesystem Type Size Used Avail Use% Mounted on devtmpfs devtmpfs 4.0M 0 4.0M 0% /dev tmpfs tmpfs 32G 0 32G 0% /dev/shm tmpfs tmpfs 13G 1.6M 13G 1% /run /dev/mapper/fedora_penelope-root xfs 15G 12G 3.2G 79% / tmpfs tmpfs 32G 0 32G 0% /tmp /dev/sda2 xfs 960M 323M 638M 34% /boot /dev/md127 ext4 44T 9.9T 32T 24% /mnt/md127 tmpfs tmpfs 6.3G 4.0K 6.3G 1% /run/user/1000 We can do better. /var/lib/plocate is 1.1G. Its a database of all files in your root directory. It is used by locate utility. It's big because I have a whole CentOS7 container in my home directory. Cleaning it with sudo updatedb then cleaned the size of plocate directory.","title":"Returning to the DNF upgrade"},{"location":"admin/#doubling-server-speed-with-network-bonding","text":"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_networking/configuring-network-bonding_configuring-and-managing-networking https://jfearn.fedorapeople.org/fdocs/en-US/Fedora/20/html/Networking_Guide/sec-Using_Channel_Bonding.html https://docs.fedoraproject.org/en-US/fedora-coreos/sysconfig-network-configuration/","title":"Doubling server speed, with network bonding?"},{"location":"admin/#hardware-settings-and-config","text":"The older machines are Lenovo E30 machines, with a IS6XM mother board, with a i7-2600 CPU. The 256 SSH should be connected to the first sata port: SATA 1. In the boot configuration, the USB key should be first, and SSD should be second. BIOS updates can be found here: https://support.lenovo.com/my/en/downloads/DS018245 Press CTRL ALT delete to get back to splash screen from error 1962. Ideally I would enable Compatibility Support Module (CSM) but I guess this isn't supported in my BIOS version. This answer indicates I probably need to update the BIOS: https://askubuntu.com/questions/1414366/no-operating-system-found-after-clean-install-of-ubuntu-22-04 fdisk and gdisk could be a good way to examine the structure of the boot disk. I need to understand what a EFI partition is. The reason this process worked on asiclab00 is because it has slightly older firmware, I believe, and it didn't install a EFI partition. It installed as a legacy system. I don't think this can be done on Note that fwupgrade requires UEFI, and sudo efibootmgr -c -L \"Windows Boot Manager\" -l \"\\EFI\\fedora\\grubx64.efi\" sudo efibootmgr -c -d /dev/sdX -p Y -L \"Windows Boot Manager\" -l \"\\EFI\\path\\file.efi\" sudo efibootmgr -c -d /dev/sda1 -p 1 -L \"Windows Boot Manager\" -l \"\\EFI\\boot\\grubx64.efi\" default is sda so, -d flat isn't really needed. Also default partition -p is 1 so also not needed The command that appears to work, assuming /dev/sda1 is the partition of interest: sudo efibootmgr -c -L \"Windows Boot Manager\" -l \"\\EFI\\fedora\\shimx64.efi\" https://forums.fedoraforum.org/showthread.php?317210-EFI-boot-issues-Lenovo-AIO-Error-1962-No-Operating-System-Found https://unix.stackexchange.com/questions/324753/pernicious-1962-error-installing-fedora-server-24-no-operating-system-found https://www.reddit.com/r/ManjaroLinux/comments/e682d6/fixing_lenovos_error_code_1962_by_spoofing_the/ https://superuser.com/questions/913779/how-can-i-know-which-partition-is-efi-system-partition make sure I can still ping asiclab010, as this is yannik's machine?","title":"Hardware settings and config"},{"location":"bag_notes_delete_me/","text":"Bag 3 Install Container Container .def file: Bootstrap: docker From: centos:7 # build me with the command: apptainer build /tmp/virtuoso_centos7.sif ./bag3++_centos7.def # and start me with: apptainer shell -B /tools,/users /tmp/bag3++_centos7.sif %setup #run on the host system after base image. Filepaths are relative to host. mkdir ${APPTAINER_ROOTFS}/users mkdir ${APPTAINER_ROOTFS}/tools #used to mount scratch disks mkdir ${APPTAINER_ROOTFS}/run/media %post #CentOS 7 image on dockerhub isn't updated, so run this first yum -y update && yum clean all # Extras repo needed to provide some packages below yum install -y centos-release-scl centos-release-scl-rh # rh-git29 isn't in official repos anymore, so use this instead yum install -y devtoolset-8 httpd24-curl httpd24-libcurl rh-git218 # additional tools yum install -y curl make wget rh-git218 (git with nice visual colors; newer git versions don't track symlinks) Note: rh-git29 is not longer available, so update to rh-git218. Note, these packages are available in the SCL repository, which must be installed: yum install centos-release-scl-rh centos-release-scl rh-git29 didn't exist: https://www.softwarecollections.org/en/scls/rhscl/rh-git29/ Immutable mode Build the container, in an immutable mode: sudo apptainer build /tmp/virtuoso_centos7.sif ./bag3++_centos7.def Run the immutable container, and start a shell inside: apptainer shell -B /tools,/users /tmp/bag3++_centos7.sif Sandbox mode for prototyping, I make a container in sandbox mode: sudo apptainer build --sandbox /tmp/bag3++_centos7_sandbox.sif ./bag3++_centos7.def And start a shell inside this writable container, as root, so that you can install and modify contents: apptainer shell -B /tools,/users --writable /tmp/bag3++_centos7_sandbox.sif/ Building Container: kcaisley > sudo apptainer build bag3++_centos7_test.sif bag3++_centos7.def works, with permissions kcaisley:base kcaisley > sudo apptainer build --sandbox bag3++_centos7_sandbox.sif bag3++_centos7.def works, but makes with permissions root:root. Can this be run with sudo shell? targeting the main .sif with this command seems to work?: apptainer shell -B /tools,/users --writable-tmpfs bag3++_centos7.sif okay let's try two things: asiclab > apptainer build bag3++_centos7.sif bag3++_centos7.def works and makes permissions asiclab:asiclab asiclab > apptainer build --sandbox bag3++_centos7_sandbox.sif bag3++_centos7.def works and makes with permissions asiclab:asiclab WARNING: The sandbox contain files/dirs that cannot be removed with 'rm'. WARNING: Use 'chmod -R u+rwX' to set permissions that allow removal with 'rm -rf' WARNING: Use the '--fix-perms' option to 'apptainer build' to modify permissions at build time. Final build (in /scratch and as kcaisley) sudo apptainer build --force bag3++_centos7.sif bag3++_centos7.def then just apptainer shell -B /tools,/users,/run/media bag3++_centos7.sif I left out the idea of building a sandbox, and just debugged what I needed before building Server Environment I need to get conda, which isn't easily available from pip, as it appears corrupted. Therefore: From inside the container, and while at the working dir Apptainer > wget https://repo.anaconda.com/miniconda/Miniconda3-py37_23.1.0-1-Linux-x86_64.sh run the script, from working dir Apptainer > bash Miniconda3-py37_23.1.0-1-Linux-x86_64.sh -b -f -p ./miniconda3 and manually set the install location Miniconda3 will now be installed into this location: /users/kcaisley/miniconda3 - Press ENTER to confirm the location - Press CTRL-C to abort the installation - Or specify a different location below [/users/kcaisley/miniconda3] >>> /tools/foss/bag/miniconda3 conda install doesn't work well: https://github.com/ContinuumIO/anaconda-issues/issues/9480 Run this from the working directory, where the conda environmental.yml file is. ./miniconda3/bin/conda env create -f environment.yml --force -p ./miniconda3/envs/bag_python37 Okay, this works! autoreconf fixed for libfyaml: https://askubuntu.com/questions/27677/cannot-find-install-sh-install-sh-or-shtool-in-ac-aux autreconf failing, needed libtool: https://www.cyberithub.com/solved-autoreconf-automake-failed-with-exit-status-1/ autoreconf -vif For Bash, I add the line: using python : 3.7 : /run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c : /run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/include/python3.7m ; notes for things to change in install instructions: fix typo 'many Python and C++ depe' fix alignment in point 4 explain that devtoolset should be 'scl enable' into order to compile with it perhaps update git version, but does the version I picked track symlinks? write out the command to the conda install prefix in the conda env create clean up the confusion between /path/to/conda/env vs /path/to/programs It's HDF5, not the other way arround. Fix this in the install instructions add note about requiring openssl-devel package for building cmake Workspace .bashrc_bag export BAG_WORK_DIR=$(pwd) export BAG_TOOLS_ROOT=/run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c export BAG_FRAMEWORK=${BAG_WORK_DIR}/BAG_framework export BAG_TECH_CONFIG_DIR=${BAG_WORK_DIR}/cds_ff_mpt export BAG_TEMP_DIR=/run/media/kcaisley/scratch/space export IPYTHONDIR=${BAG_WORK_DIR}/.ipython # disable hash-salting. We need stable hashing across sessions for caching purposes. export PYTHONHASHSEED=0 # set program locations export BAG_PYTHON=${BAG_TOOLS_ROOT}/bin/python3 # set location of BAG configuration file export BAG_CONFIG_PATH=${BAG_WORK_DIR}/bag_config.yaml # setup pybag export PYBAG_PYTHON=${BAG_PYTHON} .bashrc export PYTHONPATH=\"\" ### Setup BAG source .bashrc_bag # location of various tools export CDS_INST_DIR=/tools/cadence/IC618 export SPECTRE_HOME=/tools/cadence/2020-21/RHELx86/SPECTRE_20.10.073 export QRC_HOME=/tools/cadence/EXT191 export CMAKE_HOME=/run/media/kcaisley/scratch/cmake-3.17.0 export CDSHOME=${CDS_INST_DIR} export MMSIM_HOME=${SPECTRE_HOME} export PVS_HOME=/tools/cadence/2019-20/RHELx86/PVS_19.10.000 # OA settings export OA_CDS_ROOT=${CDS_INST_DIR}/oa_v22.60.063 export OA_PLUGIN_PATH=${OA_CDS_ROOT}/data/plugins:${OA_PLUGIN_PATH:-} export OA_BIT=64 # PATH setup export PATH=${PVS_HOME}/bin:${PATH} export PATH=${QRC_HOME}/bin:${PATH} export PATH=${CDS_INST_DIR}/tools/plot/bin:${PATH} export PATH=${CDS_INST_DIR}/tools/dfII/bin:${PATH} export PATH=${CDS_INST_DIR}/tools/bin:${PATH} export PATH=${MMSIM_HOME}/bin:${PATH} export PATH=${CMAKE_HOME}/bin:${PATH} export PATH=${BAG_TOOLS_ROOT}/bin:${PATH} # LD_LIBRARY_PATH setup export LD_LIBRARY_PATH=${BAG_WORK_DIR}/cadence_libs:${LD_LIBRARY_PATH} export LD_LIBRARY_PATH=${BAG_TOOLS_ROOT}/lib:${LD_LIBRARY_PATH} export LD_LIBRARY_PATH=${BAG_TOOLS_ROOT}/lib64:${LD_LIBRARY_PATH} # Virtuoso options export SPECTRE_DEFAULTS=-E export CDS_Netlisting_Mode=\"Analog\" export CDS_AUTO_64BIT=ALL # License setup # source /tools/flexlm/flexlm.sh #May still need to update this? export LM_LICENSE_FILE=8000@faust02.physik.uni-bonn.de # This is for cluseter computing # Setup LSF # source /tools/support/lsf/conf/profile.lsf #May still need to update this? # export LBS_BASE_SYSTEM=LBS_LSF # Enable devtoolset source /opt/rh/devtoolset-8/enable source /opt/rh/rh-git218/enable source /opt/rh/httpd24/enable # pybag compiler settings export CMAKE_PREFIX_PATH=${BAG_TOOLS_ROOT} in .cdsenv add line: asimenv.startup simulator string \"spectre\" asimenv.startup projectDir string \"/run/media/kcaisley/scratch/space\" then fixed the symbolic link at: /users/kcaisley/cadence/bag3_ams_cds_ff_mpt/cds_ff_mpt/workspace_setup lrwxrwxrwx. 1 kcaisley base 37 Jun 13 17:12 PDK -> /tools/kits/CADENCE/cds_ff_mpt_v_1.1/ Starting Container/Workspace: xhost + apptainer shell -B /tools,/users,/run/media /tools/containers/bag_centos7.sif cd ~/cadence/bag3_ams_cds_ff_mpt source .bashrc virtuoso & Deprecated starting steps: Start container apptainer shell -B /tools,/users,/run/media /run/media/kcaisley/scratch/bag3++_centos7.sif Then to enable scl environments (not necessary, as it's added to path in bashrc script) scl enable devtoolset-8 bash scl enable rh-git218 bash clearing and issue with: (not an issue if you don't have conda init put stuff in bashrc) __vte_prompt_command() { true; } To activate this environment, use (not necessary, as it's added to path in bashrc script) source ~/.bashrc #not necessary if done correctly conda activate /run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c To deactivate conda deactivate But I don't want to have to have conda init or conda in my path, so this is my case: Link to source of material below If conda is on your path: (not necessary, as it's added to path in bashrc script) source activate <env name> && python xxx.py && source deactivate If conda isn't on your path but is installed: (not necessary, as it's added to path in bashrc script) source /path/to/conda/bin/activate /path/to/desired/env_name/ && python xxx.py && source deactivate The following command will activate the environment: (not necessary, as it's added to path in bashrc script) source /run/media/kcaisley/scratch/miniconda3/bin/activate /run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/ cd ~/cadence/bag3_ams_cds_ff_mpt/ Debugging Generator First warning that was noticed was that rpm stopped working. This is because the rpm library depending on liblzma, and the LD_LIBRARY_PATH had some paths added to it in the conda install, which constains a incompatible version. RPM and LibLZMA issue rpm: /tools/packages/miniconda3/envs/bag_py3d7_c/lib/liblzma.so.5: version `XZ_5.1.2alpha' not found (required by /lib64/librpmio.so.3) Apptainer> ldd /lib64/librpmio.so.3 /lib64/librpmio.so.3: /tools/packages/miniconda3/envs/bag_py3d7_c/lib/liblzma.so.5: version `XZ_5.1.2alpha' not found (required by /lib64/librpmio.so.3) linux-vdso.so.1 => (0x00007fffa8be8000) libnss3.so => /lib64/libnss3.so (0x00007fd35c800000) libbz2.so.1 => /lib64/libbz2.so.1 (0x00007fd35c400000) libz.so.1 => /tools/packages/miniconda3/envs/bag_py3d7_c/lib/libz.so.1 (0x00007fd35d356000) libelf.so.1 => /lib64/libelf.so.1 (0x00007fd35c000000) libpopt.so.0 => /lib64/libpopt.so.0 (0x00007fd35bc00000) liblzma.so.5 => /tools/packages/miniconda3/envs/bag_py3d7_c/lib/liblzma.so.5 (0x00007fd35d32c000) liblua-5.1.so => /lib64/liblua-5.1.so (0x00007fd35b800000) libm.so.6 => /lib64/libm.so.6 (0x00007fd35b400000) libaudit.so.1 => /lib64/libaudit.so.1 (0x00007fd35b000000) libdl.so.2 => /lib64/libdl.so.2 (0x00007fd35ac00000) libpthread.so.0 => /lib64/libpthread.so.0 (0x00007fd35a800000) libc.so.6 => /lib64/libc.so.6 (0x00007fd35a400000) libnssutil3.so => /lib64/libnssutil3.so (0x00007fd35a000000) libplc4.so => /lib64/libplc4.so (0x00007fd359c00000) libplds4.so => /lib64/libplds4.so (0x00007fd359800000) libnspr4.so => /lib64/libnspr4.so (0x00007fd359400000) /lib64/ld-linux-x86-64.so.2 (0x00007fd35d000000) librt.so.1 => /lib64/librt.so.1 (0x00007fd359000000) libcap-ng.so.0 => /lib64/libcap-ng.so.0 (0x00007fd358c00000) This is where liblzma.so.5 should be coming from: Apptainer> ldconfig -p | grep liblzma liblzma.so.5 (libc6,x86-64) => /lib64/liblzma.so.5 liblzma.so.5 (libc6) => /lib/liblzma.so.5 This isn't an issue for BAG, but it does mean that Virtuoso will issue a warning, as RPM doesn't work, and therefor the checksystem version script fails. To temporarily make RPM work (at the expense of BAG), simply remove the BAG conda env locations from $LD_LIBRARY_PATH. Qt Plugin Issue When running ./meas_cell.sh I'm getting some issues with a Qt GUI starting: [2023-06-16 10:17:12.535] [sim_db] [info] Returning previous simulation data This application failed to start because it could not find or load the Qt platform plugin \"xcb\" in \"\". Available platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, xcb. Reinstalling the application may fix this problem. [2023-06-16 10:17:12.544] [cbag] [critical] stack dump [1] /lib64/libpthread.so.0+0xf630 [0x7f08ce00f630] From this page , before you start (dangerously) messing around with symlinks to shared libraries, I strongly suggest that you run export QT_DEBUG_PLUGINS=1 and then run your failing executable again in the Terminal. Read the actual error message thrown by QT, since none of the above solutions addressed the cause of this error in my case. This gave me: Got keys from plugin meta data (\"xcb\") QFactoryLoader::QFactoryLoader() checking directory path \"/run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/bin/platforms\" ... Cannot load library /run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/lib/python3.7/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so: (/run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/lib/libQt5XcbQpa.so.5: symbol _ZNK15QPlatformWindow15safeAreaMarginsEv, version Qt_5_PRIVATE_API not defined in file libQt5Gui.so.5 with link time reference) QLibraryPrivate::loadPlugin failed on \"/run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/lib/python3.7/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so\" : \"Cannot load library /run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/lib/python3.7/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so: (/run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/lib/libQt5XcbQpa.so.5: symbol _ZNK15QPlatformWindow15safeAreaMarginsEv, version Qt_5_PRIVATE_API not defined in file libQt5Gui.so.5 with link time reference)\" This application failed to start because it could not find or load the Qt platform plugin \"xcb\" in \"\". The relevant part is: symbol _ZNK15QPlatformWindow15safeAreaMarginsEv, version Qt_5_PRIVATE_API not defined in file libQt5Gui.so.5 with link time reference Googling this, lead me to this: https://superuser.com/questions/1397366/what-is-wrong-with-my-qt-environment-it-reports-could-not-find-or-load-the-qt It looks like you have a Qt libraries version mismatch. I see Qt libraries coming from both /usr/bin/platforms/libqeglfs.so and /usr/lib/x86_64-linux-gnu/qt5/plugins/platforms , which are probably not consistent. You might need to specify the path like: export LD_LIBRARY_PATH:/path/to/right/libs:$LD_LIBRARY_PATH Sure enough, I'm checking my error log, and I see references to both: /run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/lib/python3.7/site-packages/PyQt5/Qt/plugins/platforms/ /run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/bin/platforms But check it, there is also: /run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/lib/python3.7/site-packages/PyQt5/Qt/lib/libQt5XcbQpa.so.5 The solution was modifying the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=/run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/lib/python3.7/site-packages/PyQt5/Qt/lib:$LD_LIBRARY_PATH But I think I should just fix the bashrc file? Maybe add things to the end of the path. XDG_RUNTIME_PATH issue QStandardPaths: XDG_RUNTIME_DIR points to non-existing path '/run/user/2002', please create it with 0700 permissions. These were fixed by: https://unix.stackexchange.com/questions/162900/what-is-this-folder-run-user-1000 export XDG_RUNTIME_DIR=/run/media/kcaisley/scratch/space Session Manager Issue Qt: Session management error: None of the authentication protocols specified are supported https://stackoverflow.com/questions/59057653/qt-session-management-error-none-of-the-authentication-protocols-specified-are unset SESSION_MANAGER Now, nothing is appearing graphically, but I see: {'delay_fall': array([[1.44678203e-11, 2.12270059e-11, 2.79859170e-11], [1.90778681e-11, 2.83911787e-11, 3.75218904e-11], [1.19660825e-11, 1.76068935e-11, 2.31339914e-11]]), 'delay_rise': array([[1.84293439e-11, 2.77868968e-11, 3.69646259e-11], [2.38278466e-11, 3.62280028e-11, 4.84479263e-11], [1.69927221e-11, 2.59050590e-11, 3.46498162e-11]]), 'pwr_avg': array([[1.62404387e-05, 2.72441252e-05, 3.80507290e-05], [1.30442844e-05, 2.19406111e-05, 3.05181711e-05], [2.07834708e-05, 3.39967189e-05, 4.71144087e-05]]), 'sim_data': {'ff_125': <bag.simulation.data.SimData object at 0x7f14efdebb90>, 'ss_m40': <bag.simulation.data.SimData object at 0x7f14efdd7990>, 'tt_25': <bag.simulation.data.SimData object at 0x7f14efdd7890>}, 'sim_envs': ['tt_25', 'ss_m40', 'ff_125'], 'trans_fall': array([[1.68837592e-11, 2.92186103e-11, 4.21613994e-11], [2.26158479e-11, 3.93678371e-11, 5.56614052e-11], [1.42056520e-11, 2.44467640e-11, 3.53796199e-11]]), 'trans_rise': array([[2.44673691e-11, 4.21319603e-11, 5.96973391e-11], [3.17268196e-11, 5.53270956e-11, 7.88278387e-11], [2.25445377e-11, 3.87765963e-11, 5.49322180e-11]])} {'beta': 1.375, 'delay_error_avg': 0.000919131708073776, 'seg_n': 8, 'seg_p': 11} I suppose it looks like it's working now? Generator Workflow Flow ./gen_cell.sh , and it's kind are simple wrapper scripts, which just call their corresponding python equivalents, in BAG_framework/run_scripts/ dsn_cell.py extract_cell.py gen_cell.py import_sch_cellview.py meas_cell.py sim_cell.py There are also additional .py file there: export_layout.py copy_pytest_outputs.py gds_filter.py gds_import.py generate_verilog.py gen_wrapper.py import_layout.py meas_cell_old.py netlist_config.py reformat_schematic.py run_em_cell.py setup_submodules.py verify.py write_tech_info.py \u251c\u2500\u2500 data (process specific data, in submodules, for corresponding generators) \u2502 \u251c\u2500\u2500 aib_ams \u2502 \u251c\u2500\u2500 bag3_digital \u2502 \u2514\u2500\u2500 bag3_testbenches \u251c\u2500\u2500 gen_libs (init empty, holds generated OA cell views) \u2502 \u2514\u2500\u2500 AAA_INV \u251c\u2500\u2500 gen_outputs (initially empty) \u2502 \u251c\u2500\u2500 inv3 \u2502 \u2514\u2500\u2500 mos_char_nch_18n \u251c\u2500\u2500 gen_outputs_scratch -> /run/media/kcaisley/scratch/space/simulations/gen_outputs Let's first consider the input files, for an inverter schematic generator: Running a schematic generator of a simple inverter: $ ./gen_cell.sh data/bag3_digital/specs_blk/inv/gen_sch.yaml This first script is just a wrapper for the basic run_bag.sh script: ./run_bag.sh BAG_framework/run_scripts/gen_cell.py $@ This script, in turn, is just a wrapper for executing the subsequent python script with the environment's version of python: source .bashrc_pypath if [ -z ${BAG_PYTHON} ] then echo \"BAG_PYTHON is unset\" exit 1 fi exec ${BAG_PYTHON} $@ So for example, the expanded command would be: /usr/bin/python gen_cell.py gen_sch.yaml -raw -mod -lef Python is used to executed the runscript gen_cell.py : import argparse from bag.io import read_yaml from bag.core import BagProject from bag.util.misc import register_pdb_hook register_pdb_hook() def parse_options() -> argparse.Namespace: parser = argparse.ArgumentParser(description='Generate cell from spec file.') parser.add_argument('specs', help='YAML specs file name.') parser.add_argument('-d', '--drc', dest='run_drc', action='store_true', default=False, help='run DRC.') parser.add_argument('-v', '--lvs', dest='run_lvs', action='store_true', default=False, help='run LVS.') parser.add_argument('-x', '--rcx', dest='run_rcx', action='store_true', default=False, help='run RCX.') parser.add_argument('-raw', dest='raw', action='store_true', default=False, help='generate GDS/netlist files instead of OA cellviews.') parser.add_argument('-flat', dest='flat', action='store_true', default=False, help='generate flat netlist.') parser.add_argument('-lef', dest='gen_lef', action='store_true', default=False, help='generate LEF.') parser.add_argument('-hier', '--gen-hier', dest='gen_hier', action='store_true', default=False, help='generate Hierarchy.') parser.add_argument('-mod', '--gen-model', dest='gen_mod', action='store_true', default=False, help='generate behavioral model files.') parser.add_argument('-sim', dest='gen_sim', action='store_true', default=False, help='generate simulation netlist instead.') parser.add_argument('-shell', dest='gen_shell', action='store_true', default=False, help='generate verilog shell file.') parser.add_argument('-lay', dest='export_lay', action='store_true', default=False, help='export layout file.') parser.add_argument('-netlist', dest='gen_netlist', action='store_true', default=False, help='generate netlist file.') parser.add_argument('--no-layout', dest='gen_lay', action='store_false', default=True, help='disable layout.') parser.add_argument('--no-sch', dest='gen_sch', action='store_false', default=True, help='disable schematic.') args = parser.parse_args() return args def run_main(prj: BagProject, args: argparse.Namespace) -> None: specs = read_yaml(args.specs) prj.generate_cell(specs, raw=args.raw, gen_lay=args.gen_lay, run_drc=args.run_drc, gen_sch=args.gen_sch, run_lvs=args.run_lvs, run_rcx=args.run_rcx, gen_lef=args.gen_lef, flat=args.flat, sim_netlist=args.gen_sim, gen_hier=args.gen_hier, gen_model=args.gen_mod, gen_shell=args.gen_shell, export_lay=args.export_lay, gen_netlist=args.gen_netlist) if __name__ == '__main__': _args = parse_options() local_dict = locals() if 'bprj' not in local_dict: print('creating BAG project') _prj = BagProject() else: print('loading BAG project') _prj = local_dict['bprj'] run_main(_prj, _args) Next, examining the agument gen_sch.yaml file, we see the following: dut_class: bag3_digital.schematic.inv.bag3_digital__inv impl_lib: AAA_INV impl_cell: AA_inv root_dir: gen_outputs/inv params: lch: 36 w_p: 4 w_n: 4 th_p: standard th_n: standard seg_p: 4 seg_n: 4 This, in turn gives the dot notation identifier of the class in: ./bag3_digital/src/bag3_digital/schematic/inv.py , which looks like the following: from typing import Dict, Any, List, Optional import os import pkg_resources from pybag.enum import TermType from bag.design.module import Module from bag.design.database import ModuleDB from bag.util.immutable import Param # noinspection PyPep8Naming # This class inherits from the Module superclass class bag3_digital__inv(Module): \"\"\"Module for library bag3_digital cell inv. Fill in high level description here. \"\"\" yaml_file = pkg_resources.resource_filename(__name__, os.path.join('netlist_info', 'inv.yaml')) # constructor method automatically called when object is created from class # we can see that the super class Module.__init__ method is also called # self is passed as argument so that method can access other class components # arguments have type annotations (eg :Param) for the expected type/class # there is no return, to 'None' is type hint def __init__(self, database: ModuleDB, params: Param, **kwargs: Any) -> None: Module.__init__(self, self.yaml_file, database, params, **kwargs) @classmethod def get_params_info(cls) -> Dict[str, str]: return dict( lch='channel length in resolution units.', w_p='pmos width, in number of fins or resolution units.', w_n='nmos width, in number of fins or resolution units.', th_p='pmos threshold flavor.', th_n='nmos threshold flavor.', seg='segments of transistors', seg_p='segments of pmos', seg_n='segments of nmos', stack_p='number of transistors in a stack.', stack_n='number of transistors in a stack.', p_in_gate_numbers='a List indicating input number of the gate', n_in_gate_numbers='a List indicating input number of the gate', has_vtop='True if PMOS drain is not connected to VDD, but instead VTOP', has_vbot='True if NMOS drain is not connected to VSS, but instead VBOT', ) @classmethod def get_default_param_values(cls) -> Dict[str, Any]: return dict( seg=-1, seg_p=-1, seg_n=-1, stack_p=1, stack_n=1, p_in_gate_numbers=None, n_in_gate_numbers=None, has_vtop=False, has_vbot=False, ) def design(self, seg: int, seg_p: int, seg_n: int, lch: int, w_p: int, w_n: int, th_p: str, th_n: str, stack_p: int, stack_n: int, has_vtop: bool, has_vbot: bool, p_in_gate_numbers: Optional[List[int]] = None, n_in_gate_numbers: Optional[List[int]] = None) -> None: if seg_p <= 0: seg_p = seg if seg_n <= 0: seg_n = seg if seg_p <= 0 or seg_n <= 0: raise ValueError('Cannot have negative number of segments.') self.instances['XN'].design(w=w_n, lch=lch, seg=seg_n, intent=th_n, stack=stack_n) self.instances['XP'].design(w=w_p, lch=lch, seg=seg_p, intent=th_p, stack=stack_p) self._reconnect_gate('XP', stack_p, p_in_gate_numbers, 'VSS') self._reconnect_gate('XN', stack_n, n_in_gate_numbers, 'VDD') if has_vbot: self.reconnect_instance_terminal('XN', 's', 'VBOT') self.add_pin('VBOT', TermType.inout) if has_vtop: self.reconnect_instance_terminal('XP', 's', 'VTOP') self.add_pin('VTOP', TermType.inout) def _reconnect_gate(self, inst_name: str, stack: int, idx_list: Optional[List[int]], sup: str ) -> None: if stack > 1: g_term = f'g<{stack - 1}:0>' if idx_list: glist = [sup] * stack for i in idx_list: glist[i] = 'in' self.reconnect_instance_terminal(inst_name, g_term, ','.join(glist)) else: self.reconnect_instance_terminal(inst_name, g_term, 'in') else: self.reconnect_instance_terminal(inst_name, 'g', 'in') Play-by-play breakdown of execution: The order of execution flow between the Python files and the YAML file is as follows: The bash command python gen_cell.py gen_sch.yaml -raw -mod -lef is executed in the command line. The command line arguments ( gen_sch.yaml , -raw , -mod , -lef ) are passed to the gen_cell.py script. The gen_cell.py script imports necessary modules and defines functions and classes. The register_pdb_hook() function is called which sets the standard exception behavior for batch mode (but not interactive). In other words, it's setting the unhandled exception hook to called the default Python Debugger pdb for post mortem analysis. The parse_options() function is called to parse the command line arguments and return an argparse.Namespace object containing the parsed arguments, which is saved in _args variable. The parser.parse_args() function in the argparse module parses the command-line arguments based on the defined arguments in the ArgumentParser object ( parser ). Although parse_args() doesn't take any arguments explicitly, it accesses the command-line arguments provided to the script when it is executed. When you call parser.parse_args() , the function internally reads the command-line arguments from sys.argv , which is a list that contains the command-line arguments passed to the script. It uses the argument definitions in the ArgumentParser object to determine how to parse and interpret the command-line arguments. Here's a breakdown of how parse_args() works: When you execute the Python script with command-line arguments, for example: $ python script.py arg1 arg2 --option1 value1 The parse_args() function internally reads the command-line arguments from sys.argv . It considers sys.argv[1:] , excluding the script name itself ( script.py ), as the list of arguments to parse. parse_args() looks at the argument definitions specified in the ArgumentParser object ( parser ) to determine how to interpret each argument. It identifies positional arguments based on their order and assigns them to the corresponding attributes of the args object. It identifies optional arguments (those with flags like -f or --flag ) and their corresponding values, and assigns them to the appropriate attributes of the args object. Once all the arguments have been parsed and assigned, parse_args() returns the populated args object. By defining the arguments using add_argument() on the ArgumentParser object before calling parse_args() , you provide instructions to parse_args() on how to parse the command-line arguments and store them in the args object. Note that parse_args() can raise an error if the command-line arguments are not valid according to the defined arguments in the ArgumentParser . It performs argument type validation, checks for missing or incorrect arguments, and provides error messages accordingly. The run_main() function is called with the BagProject object (_prj) and the parsed arguments (_args). Note, this is defined inside the same run script still. _prj is the current project being worked on, which is gathered from the environment variable. _args is the YAML file and additional flags like -raw . The run_main() function reads the YAML file specified in the args.specs argument using the read_yaml() function. The prj.generate_cell() method is called with the YAML specifications, as well as other arguments based on the command line options ( args ). This is the point at which the runscript's role ends. prj.generate_cell(specs, raw=args.raw, gen_lay=args.gen_lay, run_drc=args.run_drc, gen_sch=args.gen_sch, run_lvs=args.run_lvs, run_rcx=args.run_rcx, gen_lef=args.gen_lef, flat=args.flat, sim_netlist=args.gen_sim, gen_hier=args.gen_hier, gen_model=args.gen_mod, gen_shell=args.gen_shell, export_lay=args.export_lay, gen_netlist=args.gen_netlist) Now inside core.py which defines prj.generate_cell() , the specified YAML file is used to generate a cell. The specs and other options are passed to the generate_cell() method of the BagProject object. The generate_cell() method in BagProject class is responsible for generating the cell based on the specifications. During cell generation, the YAML parameters are used to create an instance of the bag3_digital__inv class from the bag3_digital.schematic.inv module. The bag3_digital__inv class is a subclass of the Module class, and its init method is called to initialize the instance with the specified parameters. The design() method of the bag3_digital__inv class is called to perform the design of the cell using the provided parameters. The design() method implements the logic to design the cell based on the given parameters and connections. After the cell is generated, further processing may be performed based on the command line options, such as running DRC, LVS, generating GDS/netlist files, etc. In summary, the execution flow starts with the bash command, passes the command line arguments to the Python script, which then reads the YAML file, generates a cell based on the specifications, and performs additional processing as specified by the command line options. The YAML file provides the specifications for the cell design, and the Python script controls the execution flow and interacts with the BagProject and Module classes to generate the desired cell. Example Generate Scripts From the AIB instruction notes, I found: For each block, there will be a gds (.gds), lef (.lef), lib (.lib), netlist (.net or .cdl), model (.v or .sv) and a shell (.v). There will also be log files and some extra files containing the data necessary to generate these results, but this document only shows the final outputs. Example: DCC Delay Line Gen cell command: ./run_bag.sh BAG_framework/run_scripts/gen_cell.py data/aib_ams/specs_ip/dcc_delay_cell.yaml -raw -mod -lef Gen Lib command: ./run_bag.sh bag3_digital/scripts_util/gen_lib.py data/aib_ams/specs_ip/dcc_delay_cell_lib.yaml Output files: gen_outputs/ip_blocks/dcc_delay_cell/ Should have the files: dcc_delay_cell.gds dcc_delay_cell_shell.v dcc_delay_cell_tt_25_0p900_0p800.lib dcc_delay_cell.cdl dcc_delay_cell.lef dcc_delay_cell.sv Example Design Scripts: A design script will run through a design procedure for the given block, and if successful in execution will generate similar collateral to the previous generation scripts. Please note that since we are reusing the lib file generation from the gen cell commands, the lib file is generated in the same location as it was previously. All the commands will follow the format of: ./run_bag.sh BAG_framework/run_scripts/dsn_cell.py data/aib_ams/specs_dsn/\\*.yaml With the exact full command and output files detailed below. Example: DCC Delay Line: Yaml file: dcc_delay_line.yaml Full command: ./run_bag.sh BAG_framework/run_scripts/dsn_cell.py data/aib_ams/specs_dsn/dcc_delay_line.yaml Folder: gen_outputs/dsn_delay_line_final Generated collateral: - aib_delay_line.gds aib_delay_line.cdl aib_delay_line.lef aib_delay_line_shell.v Lib file location: gen_outputs/ip_blocks/dcc_delay_line/dcc_delay_line_tt_25_0p900_0p800.lib And here is the inv.py file: Gen Cells Explanation Reproduce a copy of the gen_cell.generate_cell() method, but only including the sections that would run if gen_sch boolean is true, and all the other booleans are false. Okay, so by default gen_lay and gen_sch are true, and all the other booleans are false. That includes raw, run_drc, run_lvs, run_rcx, lay_db, sch_db, gen_lef, sim_netlist, flat, gen_hier, gen_model, mismatch, gen_shell, export_lay, and gen_netlist. As there are very few parameters in the YAML file, and no arguments are passed. root_dir: It will be set to the value of the key root_dir from the YAML, which is \"gen_outputs/inv\". impl_lib: It will be set to the value of the key impl_lib from the YAML, which is \"AAA_INV\". impl_cell: It will be set to the value of the key impl_cell from the YAML, which is \"AA_inv\". params: It will be set to the value of the key params from the YAML, which is a dictionary containing various parameter settings: This line is critical, it checks the class of the dut_class and layout_cls key value pairs in the specs.yaml file, and returns some booleans. has_lay, lay_cls, sch_cls = self.get_dut_class_info(specs) In this case, since dut_class is specfied as a subclass of Module e.g. it is a schematic generator. Therefore has_lay = False , lay_cls=None , and sch_cls=Module . Next, there is a set of code which uses the DesignOutputs enum: class DesignOutput(IntEnum): LAYOUT = 0 GDS = 1 SCHEMATIC = 2 YAML = 3 CDL = 4 VERILOG = 5 SYSVERILOG = 6 SPECTRE = 7 OASIS = 8 To set some file types: lay_type_specs: Union[str, List[str]] = specs.get('layout_type', 'GDS') mod_type: DesignOutput = DesignOutput[mod_type_str] #...etc... if isinstance(lay_type_specs, str): lay_type_list: List[DesignOutput] = [DesignOutput[lay_type_specs]] else: lay_type_list: List[DesignOutput] = [DesignOutput[v] for v in lay_type_specs] In this code, lay_type_list will at least contain GDS and mod_type will be SYSVERILOG by default. Next, there are a bunch of variables which are skipped, in our simple case, related to DRC, layout, models, Verilog, etc. Despite the fact that gen_lay=True , if is constantly A big if-else pair evaluates, and: if has_lay: #etc else: sch_params = params Keep in mind there an instances where a schematic isn't explicitly generated, where having one would still be necessary. These are, for example, when LVS or RCX extraction are enabled.","title":"Bag 3 Install"},{"location":"bag_notes_delete_me/#bag-3-install","text":"","title":"Bag 3 Install"},{"location":"bag_notes_delete_me/#container","text":"","title":"Container"},{"location":"bag_notes_delete_me/#container-def-file","text":"Bootstrap: docker From: centos:7 # build me with the command: apptainer build /tmp/virtuoso_centos7.sif ./bag3++_centos7.def # and start me with: apptainer shell -B /tools,/users /tmp/bag3++_centos7.sif %setup #run on the host system after base image. Filepaths are relative to host. mkdir ${APPTAINER_ROOTFS}/users mkdir ${APPTAINER_ROOTFS}/tools #used to mount scratch disks mkdir ${APPTAINER_ROOTFS}/run/media %post #CentOS 7 image on dockerhub isn't updated, so run this first yum -y update && yum clean all # Extras repo needed to provide some packages below yum install -y centos-release-scl centos-release-scl-rh # rh-git29 isn't in official repos anymore, so use this instead yum install -y devtoolset-8 httpd24-curl httpd24-libcurl rh-git218 # additional tools yum install -y curl make wget rh-git218 (git with nice visual colors; newer git versions don't track symlinks) Note: rh-git29 is not longer available, so update to rh-git218. Note, these packages are available in the SCL repository, which must be installed: yum install centos-release-scl-rh centos-release-scl rh-git29 didn't exist: https://www.softwarecollections.org/en/scls/rhscl/rh-git29/","title":"Container .def file:"},{"location":"bag_notes_delete_me/#immutable-mode","text":"Build the container, in an immutable mode: sudo apptainer build /tmp/virtuoso_centos7.sif ./bag3++_centos7.def Run the immutable container, and start a shell inside: apptainer shell -B /tools,/users /tmp/bag3++_centos7.sif","title":"Immutable mode"},{"location":"bag_notes_delete_me/#sandbox-mode","text":"for prototyping, I make a container in sandbox mode: sudo apptainer build --sandbox /tmp/bag3++_centos7_sandbox.sif ./bag3++_centos7.def And start a shell inside this writable container, as root, so that you can install and modify contents: apptainer shell -B /tools,/users --writable /tmp/bag3++_centos7_sandbox.sif/","title":"Sandbox mode"},{"location":"bag_notes_delete_me/#building-container","text":"kcaisley > sudo apptainer build bag3++_centos7_test.sif bag3++_centos7.def works, with permissions kcaisley:base kcaisley > sudo apptainer build --sandbox bag3++_centos7_sandbox.sif bag3++_centos7.def works, but makes with permissions root:root. Can this be run with sudo shell? targeting the main .sif with this command seems to work?: apptainer shell -B /tools,/users --writable-tmpfs bag3++_centos7.sif okay let's try two things: asiclab > apptainer build bag3++_centos7.sif bag3++_centos7.def works and makes permissions asiclab:asiclab asiclab > apptainer build --sandbox bag3++_centos7_sandbox.sif bag3++_centos7.def works and makes with permissions asiclab:asiclab WARNING: The sandbox contain files/dirs that cannot be removed with 'rm'. WARNING: Use 'chmod -R u+rwX' to set permissions that allow removal with 'rm -rf' WARNING: Use the '--fix-perms' option to 'apptainer build' to modify permissions at build time. Final build (in /scratch and as kcaisley) sudo apptainer build --force bag3++_centos7.sif bag3++_centos7.def then just apptainer shell -B /tools,/users,/run/media bag3++_centos7.sif I left out the idea of building a sandbox, and just debugged what I needed before building","title":"Building Container:"},{"location":"bag_notes_delete_me/#server-environment","text":"I need to get conda, which isn't easily available from pip, as it appears corrupted. Therefore: From inside the container, and while at the working dir Apptainer > wget https://repo.anaconda.com/miniconda/Miniconda3-py37_23.1.0-1-Linux-x86_64.sh run the script, from working dir Apptainer > bash Miniconda3-py37_23.1.0-1-Linux-x86_64.sh -b -f -p ./miniconda3 and manually set the install location Miniconda3 will now be installed into this location: /users/kcaisley/miniconda3 - Press ENTER to confirm the location - Press CTRL-C to abort the installation - Or specify a different location below [/users/kcaisley/miniconda3] >>> /tools/foss/bag/miniconda3 conda install doesn't work well: https://github.com/ContinuumIO/anaconda-issues/issues/9480 Run this from the working directory, where the conda environmental.yml file is. ./miniconda3/bin/conda env create -f environment.yml --force -p ./miniconda3/envs/bag_python37 Okay, this works! autoreconf fixed for libfyaml: https://askubuntu.com/questions/27677/cannot-find-install-sh-install-sh-or-shtool-in-ac-aux autreconf failing, needed libtool: https://www.cyberithub.com/solved-autoreconf-automake-failed-with-exit-status-1/ autoreconf -vif For Bash, I add the line: using python : 3.7 : /run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c : /run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/include/python3.7m ; notes for things to change in install instructions: fix typo 'many Python and C++ depe' fix alignment in point 4 explain that devtoolset should be 'scl enable' into order to compile with it perhaps update git version, but does the version I picked track symlinks? write out the command to the conda install prefix in the conda env create clean up the confusion between /path/to/conda/env vs /path/to/programs It's HDF5, not the other way arround. Fix this in the install instructions add note about requiring openssl-devel package for building cmake","title":"Server Environment"},{"location":"bag_notes_delete_me/#workspace","text":".bashrc_bag export BAG_WORK_DIR=$(pwd) export BAG_TOOLS_ROOT=/run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c export BAG_FRAMEWORK=${BAG_WORK_DIR}/BAG_framework export BAG_TECH_CONFIG_DIR=${BAG_WORK_DIR}/cds_ff_mpt export BAG_TEMP_DIR=/run/media/kcaisley/scratch/space export IPYTHONDIR=${BAG_WORK_DIR}/.ipython # disable hash-salting. We need stable hashing across sessions for caching purposes. export PYTHONHASHSEED=0 # set program locations export BAG_PYTHON=${BAG_TOOLS_ROOT}/bin/python3 # set location of BAG configuration file export BAG_CONFIG_PATH=${BAG_WORK_DIR}/bag_config.yaml # setup pybag export PYBAG_PYTHON=${BAG_PYTHON} .bashrc export PYTHONPATH=\"\" ### Setup BAG source .bashrc_bag # location of various tools export CDS_INST_DIR=/tools/cadence/IC618 export SPECTRE_HOME=/tools/cadence/2020-21/RHELx86/SPECTRE_20.10.073 export QRC_HOME=/tools/cadence/EXT191 export CMAKE_HOME=/run/media/kcaisley/scratch/cmake-3.17.0 export CDSHOME=${CDS_INST_DIR} export MMSIM_HOME=${SPECTRE_HOME} export PVS_HOME=/tools/cadence/2019-20/RHELx86/PVS_19.10.000 # OA settings export OA_CDS_ROOT=${CDS_INST_DIR}/oa_v22.60.063 export OA_PLUGIN_PATH=${OA_CDS_ROOT}/data/plugins:${OA_PLUGIN_PATH:-} export OA_BIT=64 # PATH setup export PATH=${PVS_HOME}/bin:${PATH} export PATH=${QRC_HOME}/bin:${PATH} export PATH=${CDS_INST_DIR}/tools/plot/bin:${PATH} export PATH=${CDS_INST_DIR}/tools/dfII/bin:${PATH} export PATH=${CDS_INST_DIR}/tools/bin:${PATH} export PATH=${MMSIM_HOME}/bin:${PATH} export PATH=${CMAKE_HOME}/bin:${PATH} export PATH=${BAG_TOOLS_ROOT}/bin:${PATH} # LD_LIBRARY_PATH setup export LD_LIBRARY_PATH=${BAG_WORK_DIR}/cadence_libs:${LD_LIBRARY_PATH} export LD_LIBRARY_PATH=${BAG_TOOLS_ROOT}/lib:${LD_LIBRARY_PATH} export LD_LIBRARY_PATH=${BAG_TOOLS_ROOT}/lib64:${LD_LIBRARY_PATH} # Virtuoso options export SPECTRE_DEFAULTS=-E export CDS_Netlisting_Mode=\"Analog\" export CDS_AUTO_64BIT=ALL # License setup # source /tools/flexlm/flexlm.sh #May still need to update this? export LM_LICENSE_FILE=8000@faust02.physik.uni-bonn.de # This is for cluseter computing # Setup LSF # source /tools/support/lsf/conf/profile.lsf #May still need to update this? # export LBS_BASE_SYSTEM=LBS_LSF # Enable devtoolset source /opt/rh/devtoolset-8/enable source /opt/rh/rh-git218/enable source /opt/rh/httpd24/enable # pybag compiler settings export CMAKE_PREFIX_PATH=${BAG_TOOLS_ROOT} in .cdsenv add line: asimenv.startup simulator string \"spectre\" asimenv.startup projectDir string \"/run/media/kcaisley/scratch/space\" then fixed the symbolic link at: /users/kcaisley/cadence/bag3_ams_cds_ff_mpt/cds_ff_mpt/workspace_setup lrwxrwxrwx. 1 kcaisley base 37 Jun 13 17:12 PDK -> /tools/kits/CADENCE/cds_ff_mpt_v_1.1/","title":"Workspace"},{"location":"bag_notes_delete_me/#starting-containerworkspace","text":"xhost + apptainer shell -B /tools,/users,/run/media /tools/containers/bag_centos7.sif cd ~/cadence/bag3_ams_cds_ff_mpt source .bashrc virtuoso &","title":"Starting Container/Workspace:"},{"location":"bag_notes_delete_me/#deprecated-starting-steps","text":"Start container apptainer shell -B /tools,/users,/run/media /run/media/kcaisley/scratch/bag3++_centos7.sif Then to enable scl environments (not necessary, as it's added to path in bashrc script) scl enable devtoolset-8 bash scl enable rh-git218 bash clearing and issue with: (not an issue if you don't have conda init put stuff in bashrc) __vte_prompt_command() { true; } To activate this environment, use (not necessary, as it's added to path in bashrc script) source ~/.bashrc #not necessary if done correctly conda activate /run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c To deactivate conda deactivate But I don't want to have to have conda init or conda in my path, so this is my case: Link to source of material below If conda is on your path: (not necessary, as it's added to path in bashrc script) source activate <env name> && python xxx.py && source deactivate If conda isn't on your path but is installed: (not necessary, as it's added to path in bashrc script) source /path/to/conda/bin/activate /path/to/desired/env_name/ && python xxx.py && source deactivate The following command will activate the environment: (not necessary, as it's added to path in bashrc script) source /run/media/kcaisley/scratch/miniconda3/bin/activate /run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/ cd ~/cadence/bag3_ams_cds_ff_mpt/","title":"Deprecated starting steps:"},{"location":"bag_notes_delete_me/#debugging-generator","text":"First warning that was noticed was that rpm stopped working. This is because the rpm library depending on liblzma, and the LD_LIBRARY_PATH had some paths added to it in the conda install, which constains a incompatible version.","title":"Debugging Generator"},{"location":"bag_notes_delete_me/#rpm-and-liblzma-issue","text":"rpm: /tools/packages/miniconda3/envs/bag_py3d7_c/lib/liblzma.so.5: version `XZ_5.1.2alpha' not found (required by /lib64/librpmio.so.3) Apptainer> ldd /lib64/librpmio.so.3 /lib64/librpmio.so.3: /tools/packages/miniconda3/envs/bag_py3d7_c/lib/liblzma.so.5: version `XZ_5.1.2alpha' not found (required by /lib64/librpmio.so.3) linux-vdso.so.1 => (0x00007fffa8be8000) libnss3.so => /lib64/libnss3.so (0x00007fd35c800000) libbz2.so.1 => /lib64/libbz2.so.1 (0x00007fd35c400000) libz.so.1 => /tools/packages/miniconda3/envs/bag_py3d7_c/lib/libz.so.1 (0x00007fd35d356000) libelf.so.1 => /lib64/libelf.so.1 (0x00007fd35c000000) libpopt.so.0 => /lib64/libpopt.so.0 (0x00007fd35bc00000) liblzma.so.5 => /tools/packages/miniconda3/envs/bag_py3d7_c/lib/liblzma.so.5 (0x00007fd35d32c000) liblua-5.1.so => /lib64/liblua-5.1.so (0x00007fd35b800000) libm.so.6 => /lib64/libm.so.6 (0x00007fd35b400000) libaudit.so.1 => /lib64/libaudit.so.1 (0x00007fd35b000000) libdl.so.2 => /lib64/libdl.so.2 (0x00007fd35ac00000) libpthread.so.0 => /lib64/libpthread.so.0 (0x00007fd35a800000) libc.so.6 => /lib64/libc.so.6 (0x00007fd35a400000) libnssutil3.so => /lib64/libnssutil3.so (0x00007fd35a000000) libplc4.so => /lib64/libplc4.so (0x00007fd359c00000) libplds4.so => /lib64/libplds4.so (0x00007fd359800000) libnspr4.so => /lib64/libnspr4.so (0x00007fd359400000) /lib64/ld-linux-x86-64.so.2 (0x00007fd35d000000) librt.so.1 => /lib64/librt.so.1 (0x00007fd359000000) libcap-ng.so.0 => /lib64/libcap-ng.so.0 (0x00007fd358c00000) This is where liblzma.so.5 should be coming from: Apptainer> ldconfig -p | grep liblzma liblzma.so.5 (libc6,x86-64) => /lib64/liblzma.so.5 liblzma.so.5 (libc6) => /lib/liblzma.so.5 This isn't an issue for BAG, but it does mean that Virtuoso will issue a warning, as RPM doesn't work, and therefor the checksystem version script fails. To temporarily make RPM work (at the expense of BAG), simply remove the BAG conda env locations from $LD_LIBRARY_PATH.","title":"RPM and LibLZMA issue"},{"location":"bag_notes_delete_me/#qt-plugin-issue","text":"When running ./meas_cell.sh I'm getting some issues with a Qt GUI starting: [2023-06-16 10:17:12.535] [sim_db] [info] Returning previous simulation data This application failed to start because it could not find or load the Qt platform plugin \"xcb\" in \"\". Available platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, xcb. Reinstalling the application may fix this problem. [2023-06-16 10:17:12.544] [cbag] [critical] stack dump [1] /lib64/libpthread.so.0+0xf630 [0x7f08ce00f630] From this page , before you start (dangerously) messing around with symlinks to shared libraries, I strongly suggest that you run export QT_DEBUG_PLUGINS=1 and then run your failing executable again in the Terminal. Read the actual error message thrown by QT, since none of the above solutions addressed the cause of this error in my case. This gave me: Got keys from plugin meta data (\"xcb\") QFactoryLoader::QFactoryLoader() checking directory path \"/run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/bin/platforms\" ... Cannot load library /run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/lib/python3.7/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so: (/run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/lib/libQt5XcbQpa.so.5: symbol _ZNK15QPlatformWindow15safeAreaMarginsEv, version Qt_5_PRIVATE_API not defined in file libQt5Gui.so.5 with link time reference) QLibraryPrivate::loadPlugin failed on \"/run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/lib/python3.7/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so\" : \"Cannot load library /run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/lib/python3.7/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so: (/run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/lib/libQt5XcbQpa.so.5: symbol _ZNK15QPlatformWindow15safeAreaMarginsEv, version Qt_5_PRIVATE_API not defined in file libQt5Gui.so.5 with link time reference)\" This application failed to start because it could not find or load the Qt platform plugin \"xcb\" in \"\". The relevant part is: symbol _ZNK15QPlatformWindow15safeAreaMarginsEv, version Qt_5_PRIVATE_API not defined in file libQt5Gui.so.5 with link time reference Googling this, lead me to this: https://superuser.com/questions/1397366/what-is-wrong-with-my-qt-environment-it-reports-could-not-find-or-load-the-qt It looks like you have a Qt libraries version mismatch. I see Qt libraries coming from both /usr/bin/platforms/libqeglfs.so and /usr/lib/x86_64-linux-gnu/qt5/plugins/platforms , which are probably not consistent. You might need to specify the path like: export LD_LIBRARY_PATH:/path/to/right/libs:$LD_LIBRARY_PATH Sure enough, I'm checking my error log, and I see references to both: /run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/lib/python3.7/site-packages/PyQt5/Qt/plugins/platforms/ /run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/bin/platforms But check it, there is also: /run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/lib/python3.7/site-packages/PyQt5/Qt/lib/libQt5XcbQpa.so.5 The solution was modifying the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=/run/media/kcaisley/scratch/miniconda3/envs/bag_py3d7_c/lib/python3.7/site-packages/PyQt5/Qt/lib:$LD_LIBRARY_PATH But I think I should just fix the bashrc file? Maybe add things to the end of the path.","title":"Qt Plugin Issue"},{"location":"bag_notes_delete_me/#xdg_runtime_path-issue","text":"QStandardPaths: XDG_RUNTIME_DIR points to non-existing path '/run/user/2002', please create it with 0700 permissions. These were fixed by: https://unix.stackexchange.com/questions/162900/what-is-this-folder-run-user-1000 export XDG_RUNTIME_DIR=/run/media/kcaisley/scratch/space","title":"XDG_RUNTIME_PATH issue"},{"location":"bag_notes_delete_me/#session-manager-issue","text":"Qt: Session management error: None of the authentication protocols specified are supported https://stackoverflow.com/questions/59057653/qt-session-management-error-none-of-the-authentication-protocols-specified-are unset SESSION_MANAGER Now, nothing is appearing graphically, but I see: {'delay_fall': array([[1.44678203e-11, 2.12270059e-11, 2.79859170e-11], [1.90778681e-11, 2.83911787e-11, 3.75218904e-11], [1.19660825e-11, 1.76068935e-11, 2.31339914e-11]]), 'delay_rise': array([[1.84293439e-11, 2.77868968e-11, 3.69646259e-11], [2.38278466e-11, 3.62280028e-11, 4.84479263e-11], [1.69927221e-11, 2.59050590e-11, 3.46498162e-11]]), 'pwr_avg': array([[1.62404387e-05, 2.72441252e-05, 3.80507290e-05], [1.30442844e-05, 2.19406111e-05, 3.05181711e-05], [2.07834708e-05, 3.39967189e-05, 4.71144087e-05]]), 'sim_data': {'ff_125': <bag.simulation.data.SimData object at 0x7f14efdebb90>, 'ss_m40': <bag.simulation.data.SimData object at 0x7f14efdd7990>, 'tt_25': <bag.simulation.data.SimData object at 0x7f14efdd7890>}, 'sim_envs': ['tt_25', 'ss_m40', 'ff_125'], 'trans_fall': array([[1.68837592e-11, 2.92186103e-11, 4.21613994e-11], [2.26158479e-11, 3.93678371e-11, 5.56614052e-11], [1.42056520e-11, 2.44467640e-11, 3.53796199e-11]]), 'trans_rise': array([[2.44673691e-11, 4.21319603e-11, 5.96973391e-11], [3.17268196e-11, 5.53270956e-11, 7.88278387e-11], [2.25445377e-11, 3.87765963e-11, 5.49322180e-11]])} {'beta': 1.375, 'delay_error_avg': 0.000919131708073776, 'seg_n': 8, 'seg_p': 11} I suppose it looks like it's working now?","title":"Session Manager Issue"},{"location":"bag_notes_delete_me/#generator-workflow-flow","text":"./gen_cell.sh , and it's kind are simple wrapper scripts, which just call their corresponding python equivalents, in BAG_framework/run_scripts/ dsn_cell.py extract_cell.py gen_cell.py import_sch_cellview.py meas_cell.py sim_cell.py There are also additional .py file there: export_layout.py copy_pytest_outputs.py gds_filter.py gds_import.py generate_verilog.py gen_wrapper.py import_layout.py meas_cell_old.py netlist_config.py reformat_schematic.py run_em_cell.py setup_submodules.py verify.py write_tech_info.py \u251c\u2500\u2500 data (process specific data, in submodules, for corresponding generators) \u2502 \u251c\u2500\u2500 aib_ams \u2502 \u251c\u2500\u2500 bag3_digital \u2502 \u2514\u2500\u2500 bag3_testbenches \u251c\u2500\u2500 gen_libs (init empty, holds generated OA cell views) \u2502 \u2514\u2500\u2500 AAA_INV \u251c\u2500\u2500 gen_outputs (initially empty) \u2502 \u251c\u2500\u2500 inv3 \u2502 \u2514\u2500\u2500 mos_char_nch_18n \u251c\u2500\u2500 gen_outputs_scratch -> /run/media/kcaisley/scratch/space/simulations/gen_outputs Let's first consider the input files, for an inverter schematic generator: Running a schematic generator of a simple inverter: $ ./gen_cell.sh data/bag3_digital/specs_blk/inv/gen_sch.yaml This first script is just a wrapper for the basic run_bag.sh script: ./run_bag.sh BAG_framework/run_scripts/gen_cell.py $@ This script, in turn, is just a wrapper for executing the subsequent python script with the environment's version of python: source .bashrc_pypath if [ -z ${BAG_PYTHON} ] then echo \"BAG_PYTHON is unset\" exit 1 fi exec ${BAG_PYTHON} $@ So for example, the expanded command would be: /usr/bin/python gen_cell.py gen_sch.yaml -raw -mod -lef Python is used to executed the runscript gen_cell.py : import argparse from bag.io import read_yaml from bag.core import BagProject from bag.util.misc import register_pdb_hook register_pdb_hook() def parse_options() -> argparse.Namespace: parser = argparse.ArgumentParser(description='Generate cell from spec file.') parser.add_argument('specs', help='YAML specs file name.') parser.add_argument('-d', '--drc', dest='run_drc', action='store_true', default=False, help='run DRC.') parser.add_argument('-v', '--lvs', dest='run_lvs', action='store_true', default=False, help='run LVS.') parser.add_argument('-x', '--rcx', dest='run_rcx', action='store_true', default=False, help='run RCX.') parser.add_argument('-raw', dest='raw', action='store_true', default=False, help='generate GDS/netlist files instead of OA cellviews.') parser.add_argument('-flat', dest='flat', action='store_true', default=False, help='generate flat netlist.') parser.add_argument('-lef', dest='gen_lef', action='store_true', default=False, help='generate LEF.') parser.add_argument('-hier', '--gen-hier', dest='gen_hier', action='store_true', default=False, help='generate Hierarchy.') parser.add_argument('-mod', '--gen-model', dest='gen_mod', action='store_true', default=False, help='generate behavioral model files.') parser.add_argument('-sim', dest='gen_sim', action='store_true', default=False, help='generate simulation netlist instead.') parser.add_argument('-shell', dest='gen_shell', action='store_true', default=False, help='generate verilog shell file.') parser.add_argument('-lay', dest='export_lay', action='store_true', default=False, help='export layout file.') parser.add_argument('-netlist', dest='gen_netlist', action='store_true', default=False, help='generate netlist file.') parser.add_argument('--no-layout', dest='gen_lay', action='store_false', default=True, help='disable layout.') parser.add_argument('--no-sch', dest='gen_sch', action='store_false', default=True, help='disable schematic.') args = parser.parse_args() return args def run_main(prj: BagProject, args: argparse.Namespace) -> None: specs = read_yaml(args.specs) prj.generate_cell(specs, raw=args.raw, gen_lay=args.gen_lay, run_drc=args.run_drc, gen_sch=args.gen_sch, run_lvs=args.run_lvs, run_rcx=args.run_rcx, gen_lef=args.gen_lef, flat=args.flat, sim_netlist=args.gen_sim, gen_hier=args.gen_hier, gen_model=args.gen_mod, gen_shell=args.gen_shell, export_lay=args.export_lay, gen_netlist=args.gen_netlist) if __name__ == '__main__': _args = parse_options() local_dict = locals() if 'bprj' not in local_dict: print('creating BAG project') _prj = BagProject() else: print('loading BAG project') _prj = local_dict['bprj'] run_main(_prj, _args) Next, examining the agument gen_sch.yaml file, we see the following: dut_class: bag3_digital.schematic.inv.bag3_digital__inv impl_lib: AAA_INV impl_cell: AA_inv root_dir: gen_outputs/inv params: lch: 36 w_p: 4 w_n: 4 th_p: standard th_n: standard seg_p: 4 seg_n: 4 This, in turn gives the dot notation identifier of the class in: ./bag3_digital/src/bag3_digital/schematic/inv.py , which looks like the following: from typing import Dict, Any, List, Optional import os import pkg_resources from pybag.enum import TermType from bag.design.module import Module from bag.design.database import ModuleDB from bag.util.immutable import Param # noinspection PyPep8Naming # This class inherits from the Module superclass class bag3_digital__inv(Module): \"\"\"Module for library bag3_digital cell inv. Fill in high level description here. \"\"\" yaml_file = pkg_resources.resource_filename(__name__, os.path.join('netlist_info', 'inv.yaml')) # constructor method automatically called when object is created from class # we can see that the super class Module.__init__ method is also called # self is passed as argument so that method can access other class components # arguments have type annotations (eg :Param) for the expected type/class # there is no return, to 'None' is type hint def __init__(self, database: ModuleDB, params: Param, **kwargs: Any) -> None: Module.__init__(self, self.yaml_file, database, params, **kwargs) @classmethod def get_params_info(cls) -> Dict[str, str]: return dict( lch='channel length in resolution units.', w_p='pmos width, in number of fins or resolution units.', w_n='nmos width, in number of fins or resolution units.', th_p='pmos threshold flavor.', th_n='nmos threshold flavor.', seg='segments of transistors', seg_p='segments of pmos', seg_n='segments of nmos', stack_p='number of transistors in a stack.', stack_n='number of transistors in a stack.', p_in_gate_numbers='a List indicating input number of the gate', n_in_gate_numbers='a List indicating input number of the gate', has_vtop='True if PMOS drain is not connected to VDD, but instead VTOP', has_vbot='True if NMOS drain is not connected to VSS, but instead VBOT', ) @classmethod def get_default_param_values(cls) -> Dict[str, Any]: return dict( seg=-1, seg_p=-1, seg_n=-1, stack_p=1, stack_n=1, p_in_gate_numbers=None, n_in_gate_numbers=None, has_vtop=False, has_vbot=False, ) def design(self, seg: int, seg_p: int, seg_n: int, lch: int, w_p: int, w_n: int, th_p: str, th_n: str, stack_p: int, stack_n: int, has_vtop: bool, has_vbot: bool, p_in_gate_numbers: Optional[List[int]] = None, n_in_gate_numbers: Optional[List[int]] = None) -> None: if seg_p <= 0: seg_p = seg if seg_n <= 0: seg_n = seg if seg_p <= 0 or seg_n <= 0: raise ValueError('Cannot have negative number of segments.') self.instances['XN'].design(w=w_n, lch=lch, seg=seg_n, intent=th_n, stack=stack_n) self.instances['XP'].design(w=w_p, lch=lch, seg=seg_p, intent=th_p, stack=stack_p) self._reconnect_gate('XP', stack_p, p_in_gate_numbers, 'VSS') self._reconnect_gate('XN', stack_n, n_in_gate_numbers, 'VDD') if has_vbot: self.reconnect_instance_terminal('XN', 's', 'VBOT') self.add_pin('VBOT', TermType.inout) if has_vtop: self.reconnect_instance_terminal('XP', 's', 'VTOP') self.add_pin('VTOP', TermType.inout) def _reconnect_gate(self, inst_name: str, stack: int, idx_list: Optional[List[int]], sup: str ) -> None: if stack > 1: g_term = f'g<{stack - 1}:0>' if idx_list: glist = [sup] * stack for i in idx_list: glist[i] = 'in' self.reconnect_instance_terminal(inst_name, g_term, ','.join(glist)) else: self.reconnect_instance_terminal(inst_name, g_term, 'in') else: self.reconnect_instance_terminal(inst_name, 'g', 'in')","title":"Generator Workflow Flow"},{"location":"bag_notes_delete_me/#play-by-play-breakdown-of-execution","text":"The order of execution flow between the Python files and the YAML file is as follows: The bash command python gen_cell.py gen_sch.yaml -raw -mod -lef is executed in the command line. The command line arguments ( gen_sch.yaml , -raw , -mod , -lef ) are passed to the gen_cell.py script. The gen_cell.py script imports necessary modules and defines functions and classes. The register_pdb_hook() function is called which sets the standard exception behavior for batch mode (but not interactive). In other words, it's setting the unhandled exception hook to called the default Python Debugger pdb for post mortem analysis. The parse_options() function is called to parse the command line arguments and return an argparse.Namespace object containing the parsed arguments, which is saved in _args variable. The parser.parse_args() function in the argparse module parses the command-line arguments based on the defined arguments in the ArgumentParser object ( parser ). Although parse_args() doesn't take any arguments explicitly, it accesses the command-line arguments provided to the script when it is executed. When you call parser.parse_args() , the function internally reads the command-line arguments from sys.argv , which is a list that contains the command-line arguments passed to the script. It uses the argument definitions in the ArgumentParser object to determine how to parse and interpret the command-line arguments. Here's a breakdown of how parse_args() works: When you execute the Python script with command-line arguments, for example: $ python script.py arg1 arg2 --option1 value1 The parse_args() function internally reads the command-line arguments from sys.argv . It considers sys.argv[1:] , excluding the script name itself ( script.py ), as the list of arguments to parse. parse_args() looks at the argument definitions specified in the ArgumentParser object ( parser ) to determine how to interpret each argument. It identifies positional arguments based on their order and assigns them to the corresponding attributes of the args object. It identifies optional arguments (those with flags like -f or --flag ) and their corresponding values, and assigns them to the appropriate attributes of the args object. Once all the arguments have been parsed and assigned, parse_args() returns the populated args object. By defining the arguments using add_argument() on the ArgumentParser object before calling parse_args() , you provide instructions to parse_args() on how to parse the command-line arguments and store them in the args object. Note that parse_args() can raise an error if the command-line arguments are not valid according to the defined arguments in the ArgumentParser . It performs argument type validation, checks for missing or incorrect arguments, and provides error messages accordingly. The run_main() function is called with the BagProject object (_prj) and the parsed arguments (_args). Note, this is defined inside the same run script still. _prj is the current project being worked on, which is gathered from the environment variable. _args is the YAML file and additional flags like -raw . The run_main() function reads the YAML file specified in the args.specs argument using the read_yaml() function. The prj.generate_cell() method is called with the YAML specifications, as well as other arguments based on the command line options ( args ). This is the point at which the runscript's role ends. prj.generate_cell(specs, raw=args.raw, gen_lay=args.gen_lay, run_drc=args.run_drc, gen_sch=args.gen_sch, run_lvs=args.run_lvs, run_rcx=args.run_rcx, gen_lef=args.gen_lef, flat=args.flat, sim_netlist=args.gen_sim, gen_hier=args.gen_hier, gen_model=args.gen_mod, gen_shell=args.gen_shell, export_lay=args.export_lay, gen_netlist=args.gen_netlist) Now inside core.py which defines prj.generate_cell() , the specified YAML file is used to generate a cell. The specs and other options are passed to the generate_cell() method of the BagProject object. The generate_cell() method in BagProject class is responsible for generating the cell based on the specifications. During cell generation, the YAML parameters are used to create an instance of the bag3_digital__inv class from the bag3_digital.schematic.inv module. The bag3_digital__inv class is a subclass of the Module class, and its init method is called to initialize the instance with the specified parameters. The design() method of the bag3_digital__inv class is called to perform the design of the cell using the provided parameters. The design() method implements the logic to design the cell based on the given parameters and connections. After the cell is generated, further processing may be performed based on the command line options, such as running DRC, LVS, generating GDS/netlist files, etc. In summary, the execution flow starts with the bash command, passes the command line arguments to the Python script, which then reads the YAML file, generates a cell based on the specifications, and performs additional processing as specified by the command line options. The YAML file provides the specifications for the cell design, and the Python script controls the execution flow and interacts with the BagProject and Module classes to generate the desired cell.","title":"Play-by-play breakdown of execution:"},{"location":"bag_notes_delete_me/#example-generate-scripts","text":"From the AIB instruction notes, I found: For each block, there will be a gds (.gds), lef (.lef), lib (.lib), netlist (.net or .cdl), model (.v or .sv) and a shell (.v). There will also be log files and some extra files containing the data necessary to generate these results, but this document only shows the final outputs. Example: DCC Delay Line Gen cell command: ./run_bag.sh BAG_framework/run_scripts/gen_cell.py data/aib_ams/specs_ip/dcc_delay_cell.yaml -raw -mod -lef Gen Lib command: ./run_bag.sh bag3_digital/scripts_util/gen_lib.py data/aib_ams/specs_ip/dcc_delay_cell_lib.yaml Output files: gen_outputs/ip_blocks/dcc_delay_cell/ Should have the files: dcc_delay_cell.gds dcc_delay_cell_shell.v dcc_delay_cell_tt_25_0p900_0p800.lib dcc_delay_cell.cdl dcc_delay_cell.lef dcc_delay_cell.sv","title":"Example Generate Scripts"},{"location":"bag_notes_delete_me/#example-design-scripts","text":"A design script will run through a design procedure for the given block, and if successful in execution will generate similar collateral to the previous generation scripts. Please note that since we are reusing the lib file generation from the gen cell commands, the lib file is generated in the same location as it was previously. All the commands will follow the format of: ./run_bag.sh BAG_framework/run_scripts/dsn_cell.py data/aib_ams/specs_dsn/\\*.yaml With the exact full command and output files detailed below. Example: DCC Delay Line: Yaml file: dcc_delay_line.yaml Full command: ./run_bag.sh BAG_framework/run_scripts/dsn_cell.py data/aib_ams/specs_dsn/dcc_delay_line.yaml Folder: gen_outputs/dsn_delay_line_final Generated collateral: - aib_delay_line.gds aib_delay_line.cdl aib_delay_line.lef aib_delay_line_shell.v Lib file location: gen_outputs/ip_blocks/dcc_delay_line/dcc_delay_line_tt_25_0p900_0p800.lib And here is the inv.py file:","title":"Example Design Scripts:"},{"location":"bag_notes_delete_me/#gen-cells-explanation","text":"Reproduce a copy of the gen_cell.generate_cell() method, but only including the sections that would run if gen_sch boolean is true, and all the other booleans are false. Okay, so by default gen_lay and gen_sch are true, and all the other booleans are false. That includes raw, run_drc, run_lvs, run_rcx, lay_db, sch_db, gen_lef, sim_netlist, flat, gen_hier, gen_model, mismatch, gen_shell, export_lay, and gen_netlist. As there are very few parameters in the YAML file, and no arguments are passed. root_dir: It will be set to the value of the key root_dir from the YAML, which is \"gen_outputs/inv\". impl_lib: It will be set to the value of the key impl_lib from the YAML, which is \"AAA_INV\". impl_cell: It will be set to the value of the key impl_cell from the YAML, which is \"AA_inv\". params: It will be set to the value of the key params from the YAML, which is a dictionary containing various parameter settings: This line is critical, it checks the class of the dut_class and layout_cls key value pairs in the specs.yaml file, and returns some booleans. has_lay, lay_cls, sch_cls = self.get_dut_class_info(specs) In this case, since dut_class is specfied as a subclass of Module e.g. it is a schematic generator. Therefore has_lay = False , lay_cls=None , and sch_cls=Module . Next, there is a set of code which uses the DesignOutputs enum: class DesignOutput(IntEnum): LAYOUT = 0 GDS = 1 SCHEMATIC = 2 YAML = 3 CDL = 4 VERILOG = 5 SYSVERILOG = 6 SPECTRE = 7 OASIS = 8 To set some file types: lay_type_specs: Union[str, List[str]] = specs.get('layout_type', 'GDS') mod_type: DesignOutput = DesignOutput[mod_type_str] #...etc... if isinstance(lay_type_specs, str): lay_type_list: List[DesignOutput] = [DesignOutput[lay_type_specs]] else: lay_type_list: List[DesignOutput] = [DesignOutput[v] for v in lay_type_specs] In this code, lay_type_list will at least contain GDS and mod_type will be SYSVERILOG by default. Next, there are a bunch of variables which are skipped, in our simple case, related to DRC, layout, models, Verilog, etc. Despite the fact that gen_lay=True , if is constantly A big if-else pair evaluates, and: if has_lay: #etc else: sch_params = params Keep in mind there an instances where a schematic isn't explicitly generated, where having one would still be necessary. These are, for example, when LVS or RCX extraction are enabled.","title":"Gen Cells Explanation"},{"location":"bag_old_notes_delete_me/","text":"BAG Analog Generator Crossely ICCAD'13 produce only sized schematic, the topology is up to the designer Good for: technology characterization schematic and testbench translation simulator interfacing physical verification and extraction parameterized layout creation for common styles of layout. BAG, in it's basic form belongs to the 'knowledge-based' design automation class, although specific sub-circuits area free to be optimized with some algorithm. This is especially useful with BAG, as we already have the ability to produce a high-quality subset of design to choose between. Knowledge-based design scripts are especially useful, as they self-document the design process. They also keep the designer 'in control' of the process. a completed BAG script helps with top level design, as you can specify the top level parameters, and then recursively instantiate the subblocks \"In our approach to analog circuit automation, a designer\u2019s deliverable is not a single instance of a sized schematic and clean layout for a particular circuit, but rather a generator for a desired class of circuits that can replicate, in an automated fashion, the design procedure that would have been used for a traditional, manual design.\" Each realized circuit Generator much implement the methods of the interface ReadSpecification() DesignSchematic() DesignLayout() VerifyArchitecture() WritePerformance() Because tasks are often the same between circuit generators though, there are abstract classes which provide connections to common actions in the design process, and to connect to tools. CodeStubGeneration() RunOptimizer() LaunchSimulations() RunDRCandLVS() IO_OpenAccess() BAG Install Notes BAG_prim is cadence library, with device used to build schematic template (called a schematic generator) Schematic generators are schematic templates are used as in input to design modules, which then produce new design instances as an output Drop in replacement for instances in a schematic generator Monday June 27 I'm at the point in the tutorial where I've installed the necessary python packages, and now it's instructing me on how to set up the 'configuration file. But I don't understand where the Configuration file is? Actually wait, it seems that I need to make these config files, and that they aren't BAG specific. I just need to 'point' to them from within the BAG database, so that BAG knows where they are? env_file, lvs_runset, rcx_runset, and cell_map all appear to be files that are used normally for Cadence work. Okay, so then the page with 'BAG Configuration File' listed must be listing items that I should interact with through the BAG interactive session? Okay, so do I need to import python, and then try to import the BAG library? I think I understand now; I need to load the bag2 code base from within python3, and use the setup.py and __init__.py file as my starting point for library import. To install this module, I really want to use pip, but as I have multiple python version on the remote, I should launch pip from the anaconda python environment: https://stackoverflow.com/questions/40392499/why-is-m-needed-for-python-m-pip-install https://stackoverflow.com/questions/25749621/whats-the-difference-between-pip-install-and-python-m-pip-install Installing BAG Environment Start by getting the latest version of Anaconda, using this page: https://linuxize.com/post/how-to-install-anaconda-on-centos-7/ This is the version of Anaconda I selected: Anaconda3-2022.05-Linux-x86_64.sh After anaconda was installed, I tried to setup the BAG3 code, but the source for the documentation didn't exist, so I also downloaded the BAG2 code, to try and generate the documentation. I got the following error trying to build the sphinx documentation: (base) bash-4.2$ make html sphinx-build -b html -d build/doctrees source build/html Running Sphinx v4.4.0 making output directory... done WARNING: sphinx_rtd_theme (< 0.3.0) found. It will not be available since Sphinx-6.0 Theme error: no theme named 'sphinx_rtd_theme' found (missing theme.conf?) make: *** [html] Error 2 I think I could try installing this with pip, but because I'm relying on Anaconda for this installation, I searched for a method using anaconda. https://anaconda.org/conda-forge/sphinx_rtd_theme To install this package with conda run one of the following: conda install -c conda-forge sphinx_rtd_theme Rerunning 'make html' I get the following: (It looks like it worked? Build finished. The HTML pages are in build/html. the documentation is working! It looks like I will be using a lot of Git, so I figured it would be best if I just updated my version of git: https://computingforgeeks.com/how-to-install-latest-version-of-git-git-2-x-on-centos-7/ Getting ready: Update git version on asiclab008 Configure easy ssh access with key authentication Building the documentation Mounting the remote drive as a SMB share, so that it's locally available on mac os (also considered StrongSync) Cadence Crashing Notes: For Friday: x Figure out if Cadence is actually crashing (yes, it is!) x test linux config, to find a way to not make it crash. (Just use IC618) The current version of my redhat/centOS distribution can be checked via: cat /etc/redhat-release To check Cadence config: /cadence/cadence/IC618/tools.lnx86/bin/checkSysConf IC6.1.8 output of this indicated that the following packages are needed: sudo yum install xorg-x11-fonts-ISO8859-1-75dpi redhat-lsb xorg-x11-server-Xvfb Now everything is passing for IC618! Let's examine our two startup scripts: /faust/user/kcaisley/cadence/tsmc65/cdr/tsmc_crn65lp_1.7a_rd53b -starts with /bin/csh -f -setenv CDSDIR /cadence/cadence/IC617 -finishes with 'virtuoso' the other script is, completely: #!/bin/bash export DMC_RUN_DIR=$(pwd) export DMC_SOS_DIR=/faust/user/kcaisley/designs/dmc65 /cadence/local/bin/tsmc_crn65lp_1.7a_rd53 & examining this other nested script tsmc_crn65lp_1.7a_rd53 We want to create a new startup script for running with cadence, we will put it in our personal directories for now. Changing the following lines: setenv CDSDIR /cadence/cadence/IC617 -> setenv CDSDIR /cadence/cadence/IC618 virtuoso -> vituoso & This works now, as long as I start it as an executable './script_name' In hind sight, it looks like we might not have some support for IC617, including no ASSURA618 version. Therefore, I should probably just use the startup script tsmc_crn65lp_1.7a_rd53, and call it from piotrs script, as this adds additional DMC-specific variables. Nope, it just freezes when it starts, so I recopied the central script, made the two changes, and will work from there. Anaconda Install: Cheatsheet for using Conda: https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf Conda Environments Guide: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html open anaconda navigator: anaconda-navigator & listing conda environments: conda env list changing between conda envs: conda active bag3 create conda new env: conda create --name [myenv] August 26: to figure out current tty number w to show which tty are being used ps -e | grep tty to kill another tty pkill -9 -t tty1 to show process numbers ps -f another tool which is useful for identifying and killing processes is top/htop. i think that top can do everything htop can, but more easily perhaps i should learn to play with top more in the future Thursday June 23 \ud83c\udf4e\ud83c\udf4e BAG_prim is cadence library, with device used to build schematic template (called a schematic generator) Schematic generators are schematic templates are used as in input to design modules, which then produce new design instances as an output Drop in replacement for instances in a schematic generator Monday June 27 \ud83c\udf4e\ud83c\udf4e\ud83c\udf4e\ud83c\udf4e\ud83c\udf4e I'm at the point in the tutorial where I've installed the necessary python packages, and now it's instructing me on how to set up the 'configuration file. But I don't understand where the Configuration file is? Actually wait, it seems that I need to make these config files, and that they aren't BAG specific. I just need to 'point' to them from within the BAG database, so that BAG knows where they are? env_file , lvs_runset , rcx_runset , and cell_map all appear to be files that are used normally for Cadence work. Okay, so then the page with 'BAG Configuration File' listed must be listing items that I should interact with through the BAG interactive session? Okay, so do I need to import python, and then try to import the BAG library? Found this online: Package - A folder/directory that contains __init__.py file. Module - A valid python file with .py extension. Distribution - How one package relates to other packages and modules . I think I understand now; I need to load the BAG3 code base from within Python3, and use the setup.py and __init__.py file as my starting point for library import. To install this package, I really want to use pip , but as I have multiple python version on the remote, I should launch pip from the anaconda python environment: It still wasn't working, but it's because I had to target the directory explicitily, and not the setup.py file. The command that appears to have finally worked for me is python -m pip install /faust/user/kcaisley/bag3 It's important to remember that a 'module' in Python is the name for any file ending in a .py extension. It appears that I could have actually just ran pip install . while inside the bag3 directory. Oh well \ud83d\udc81\ud83c\udffc\u200d\u2642\ufe0f Mark helped me understand that it's better to use conda virtual environements. We built a python environement with \u200b conda create -n bag3 python=3.9.12 \u200b conda activate bag3 And then we reinstalled our BAG3 package in this virtual environement with the following command, run in our /faust/user/kcaisley/bag3 directory: python setup.py develop My next task is to figure out how to edit the configuration options of BAG, and how to start using it in an environement like JupyterNotebooks. It's obvious that the BAG environment is running as a sqlite database, and that we need to initialize that database to interact with it. Other BAG thoughts: Make sure all packages are installed as listed in the BAG_framework documentation. The two needed, sqlitedict and openmdao are installed via pip, not anaconda, as they aren't on the conda repo (but are on Pypi repo) lauch the setup.py file with \"pip install .\", while inside the folder. As stated online, it's better to avoid directly calling the setup.py file. Create conda environment: conda create --name bag conda activate bag Install in current env: conda install ..... conda info Getting the repository: git clone -b develop https://github.com/ucb-art/bag.git --recurse-submodules Figuring out what changes I've made to local: git status git log git info git fetch --dry-run git fetch --dry-run --all git remote update git show-branch *develop git config --global user.name \"Kennedy Caisley\" git config --global user.email kcaisley@uni-bonn.de git config --global core.editor vim To copy down submodule, if forgot to do during cloning: git submodule update --init --recursive --remote ERROR: Cloning into 'pybag'... Permission denied (publickey). fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. Clone of 'git@github.com:ucb-art/pybag.git' into submodule path 'pybag' failed To reset all unstaged/uncommitted change: git checkout . git pull = git fetch + git merge https://github.com/ucb-art/pybag.git August 27: List of packages: apipkg appdirs attrs backcall cycler decorator distlib execnet filelock h5py importlib-metadata ipython ipython-genutils jedi Jinja2 kiwisolver MarkupSafe matplotlib more-itertools networkx numpy packaging pandocfilters parso pexpect pickleshare pluggy prompt-toolkit ptyprocess py Pygments pyparsing PyQt5 PyQt5-sip pytest pytest-forked pytest-xdist python-dateutil pyzmq ruamel.yaml ruamel.yaml.clib scipy six sortedcontainers traitlets virtualenv wcwidth zipp zmq August 28: I need to make sure that I use BAG_framework as my directory name, and not just BAG. Delete the /designs/demo_ffmpt/ file and start over, by remaking a new empty one. First, start a new project repo with these steps: https://github.com/ucb-art/cds_ff_mpt-bag3 Then start up cadence and jupyter notebook with steps at the end of: https://github.com/ucb-art/BAG2_cds_ff_mpt/ (This may not be 100% accurate.) Workspace setup phase: Several differnt things need to be modified: Consult the bag2 ffmpt library to look at the configuration for Jupyter that is read in, before starting ipython_config.py is stored inside the .ipython\\profile_default directory. The bag3 ffmpt library was only the primitives directory on the lower level, but it does contain this file at ../../cds_ff_mpt/workspace_setup/ipython_config.py . But the bag3 ffmpt library didn't have the xbase the ipython_config.py file call bag_config.yaml bag_submodules.yaml ipython_config.py leBindKeys Needed changes: In ./cds_ff_mpt/workspace_setup/ipython_config.py , needed to a ## List of files to run at IPython startup. c.InteractiveShellApp.exec_files = [ os.path.join(os.environ['BAG_WORK_DIR'], 'bag_startup.py'), ] This then calls ./bag_startup.py , which I had to create, as is design specific to let Jupyter Notebook/Python know where to look. It's contents was: # -*- coding: utf-8 -*- import os import sys sys.path.append(os.environ['BAG_FRAMEWORK']) sys.path.append(os.environ['BAG_TECH_CONFIG_DIR']) sys.path.append(os.path.join(os.environ['BAG_WORK_DIR'], 'BAG2_TEMPLATES_EC')) sys.path.append(os.path.join(os.environ['BAG_WORK_DIR'], 'BAG_XBase_demo')) sys.path.append(os.path.join(os.environ['BAG_WORK_DIR'], 'bag_advanced_examples')) sys.path.append(os.path.join(os.environ['BAG_WORK_DIR'], 'bag_testbenches')) This, of course, then leads me to consider if I have all the necessary project files mentioned above put in place. [x] BAG_FRAMEWORK (bag code base) [x] BAG_TECH_CONFIG_DIR (BAG PDK primitives folder, containing PDK softlink, among other things) [X] BAG2_TEMPLATES_EC ... had to grab this online, added via simple git clone: https://github.com/ucb-art/BAG2_TEMPLATES_EC.git [X] BAG_XBase_demo.... https://github.com/ucb-art/BAG_XBase_demo/tree/master [X] bag_advanced_examples... same, had to clone from https://github.com/ucb-art/bag_advanced_examples.git [X] bag_testbenches... yep, got from https://github.com/ucb-art/bag_testbenches.git This completed now, but I still have unanswered questions about several other files. Explaining everything below: Chronological origin of files in top level directory, during installation: Following steps from: https://github.com/ucb-art/cds_ff_mpt-bag3 Manually making new git repo, and adding git modules, and updating them BAG_framework manually git moduled from my local copy cds_ff_mpt-bag3 manually git moduled from local copy (this is primitives tech repo) .gitmodules .git Running install.sh in workspace directory (many of these files had to be prepared) bag_submodules.yaml copied from worksapce setup, via install.sh .cdsenv.personal copied from worksapce setup, via install.sh .cdsinit.personal copied from worksapce setup, via install.sh bag_config.yaml link from workspace setup, via install.sh, multipurpose config file, used by setup_submodules.py during installation, and during BAG runtime. .bashrc link from workspace setup, via install.sh .bashrc_bag link from workspace setup, via install.sh .cdsenv link from workspace setup, via install.sh .cdsinit link from workspace setup, via install.sh cds.lib.core link from workspace setup, via install.sh, included by cds.lib on startup .cshrc link from workspace setup, via install.sh, unused .cshrc_bag link from workspace setup, via install.sh, unused display.drf link from workspace setup, via install.sh .gitignore link from workspace setup, via install.sh models link from workspace setup, via install.sh (is a directory) pvtech.lib link from workspace setup, via install.sh leBindKeys.il link from workspace setup, via install.sh start_tutorial.sh link from workspace setup, via install.sh tutorial_files link from workspace setup, via install.sh .ipython created by install.sh (is a diretory, which contains ipython_config.py, which in turn is a link from workspace setup, via install.sh) gen_libs created by install.sh cds.lib created by install.sh (just points to cds.lib.core, which in turn points to ) run_bag.sh link from BAG framework, via install.sh, core script which actually runs BAG, called from core BAG scripts. setup_submodules.py link from BAG framework, via install.sh, executed in the next step to actually start_bag.il link from BAG framework, via install.sh, written in SKILL, seems to call virt_server.sh , which in turn starts stuff in framework. start_bag.sh link from BAG framework, via install.sh, very similar to run_bag.sh but appears unused virt_server.sh link from BAG framework, via install.sh, called by start_bag.il , contains 1-line command to start server in BAG framework. Running setup_submodules.py, which references bag_modules.yaml, both copied/link in the step above. bag3_analog automatic git module added by setup_submodules.py, from bag_modules.yaml bag3_digital automatic git module added by setup_submodules.py, from bag_modules.yaml xbase automatic git module added by setup_submodules.py, from bag_modules.yaml .bashrc_pypath created by setup_submodules.py ; contains PYTHONPATH env var python copies to sys.path on startup. Later exported by run_bag.sh and start_bag.sh . bag_libs.def created by setup_submodules.py with plaintext list of OA BAG libraries. cds.lib.bag created by setup_submodules.py , and included by cds.lib.core, which in turn is included by cds.lib.core This finishes all the general BAG3 workspace setup, but for the tutorial, more was needed: Following steps at at https://github.com/ucb-art/BAG2_cds_ff_mpt/ BAG2_TEMPLATES_EC manually git cloned from Github bag_advanced_examples manually git cloned from Github bag_testbenches manually git cloned from Github BAG_XBase_demo manually git cloned from Github bag_startup.py manually created, and made ipython_config.py call it, based on BAG2 example, adding libraries to pythonpath Finally, starting up Virtuso conda activate bag source .bashrc virtuoso & ./start_tutorial.sh this starts up iPython/Jupyter, which in turn calls the .ipython/ipython_config file, which then calls bag_startup.py libManager.log Log file created by Cadence Virtuoso on each startup. PYTHONPATH and sys.path understanding: https://www.devdungeon.com/content/python-import-syspath-and-pythonpath-tutorial Changes to accomodate: renamed ~/eda/ to ~/packages/. Best to check for issues moved demo_ffmpt folder to ~/cadence/ folder, rather than /designs/. Check for issues. moved cds_ff_mpt-bag3 to ~/packages/ from ~/cadence/ moved cds_ff_mpt_v_1.1 to ~/packages/ from ~/cadence/ so had to update PDK symlink. Potential outstanding problems: My reference to the cds_ff_mpd-bag module is broken in git, as it's still looking for an origin master in ~/cadence/. See above, as it's moved to ~/packages/ As a solution for now, I've just deleted the remote reference. It shouldn't really matter, as this is the only project I'll need which will use this cds_ff_mpt PDK Is anything calling .bashrc_pypath ? I know run_bag.sh exports it; but I don't see the latter running anytime? Will the Jupyter notebook embedded in VScode read my .ipython/profile_default/ipython_config.py file on startup, like a standard Jupyter notebook would? I may just delete all references to git, and move on with my life in this set of modules. Friday Sep 9 I need Jupyter to know where all my files are. Opening a Jupyter notebook in vscode will allow me to select a Anaconda environment, but it doesn't easily let me run the .ipython file, which calls the bag_startup.py to append to my Once my Jupyter notebook in started in vscode, I can't simply call bash scripts to modify the environment variables. Plus, the sys.path has already been initialized by the integrated ipykernel upon startup, and no more interaction with the environement variable ipython Jupyter comes with various components. The notebook server is language agnostic, and servers as the front end. iPython is a terminal only front end, which isn't used if I'm using the GUI jupyter notebook. The kernel is used by both, but is language specific. I need to understand the difference between the notebook server and the jupyter kernel. It seems the kernel is what needs to be associated with my anaconda env, and not the notebook server?? These are shown in two different places, Kernel in the top left, and server in the bottom right. I want to server to be started in a way that it is concious of all the environement variables I export when also starting cadence. I think part of the problem I was facing before was that the local jupyter server was starting with it's home following the jupyter notebook symlink. And so this causes problems, becuase I To start jupyter in a certain environment, simple follow basic method here, before he introduces his package: http://stuartmumford.uk/blog/jupyter-notebook-and-conda.html (as I don't need to switch my env dynamically) Monday Sep 12 Figure out why does pybag.core not exist? There is a core.pyi file and core.cpp file. Maybe it's pointing at a C function! Seach 'How to import C function in An amazing note on relative imports: https://stackoverflow.com/questions/14132789/relative-imports-for-the-billionth-time Tuesday Sep 13 My virtual env setup is really confusing the hell out of me. I decided to strip out Anaconda and just use Python -m venv. This can can't cross versions though, and so I need to install the latest version of python on my machine, 3.10, as i only have 3.6 as the system python.... Actually, I tried following this tutorial: https://linuxstans.com/how-to-install-python-centos/ but it didn't work A method that seemed to work istead was to create a local install of python: https://codeghar.wordpress.com/2013/09/26/install-python-3-locally-under-home-directory-in-centos-6-4/ This works. which python3.10 points in my PATH. Except now, the version of OpenSSL that I have doesn't work with Python3.10, and so I need to instead install python 3.9.14 with this method. Creating a venv: https://docs.python.org/3/library/venv.html python3 -m venv /path/to/new/virtual/environment activating a venv: source venv/bin/activate installing packages: pip install -r ./requirements.txt for some reason, numpy, scipy, matplotlib, and m5py needed to be installed manually, as they were needing compiling and crashing if included in the above file. This all works now though, in my new venv. Wednesday Sep 14 No I'm going to copy over the bag version, and see if I can get this sorted out, with the pybag.core problem. I tried installing the packages pybind11, into pybind_generics, into pybag into bag, in that order, but I started hitting compile errors about Cmake. My Cmake version is okay, but apparently GCC was also outdated: https://stackoverflow.com/questions/47238577/target-requires-the-language-dialect-cxx17-with-compiler-extensions-but-cma However, at the bottom of this document, it points out that Centos has a package called devtoolset-7. By checking the command 'scl -l' I found that I already have this package. Running 'scl enable devtoolset-7 bash' was enough to get the gcc version updated! I think it just applies to the current terminal though. This is explained here: https://www.softwarecollections.org/en/scls/rhscl/devtoolset-7/ Thursday 15 September How can I deal with installing Pybag? Is it a wheel package? Friday 16 Sep I inspected the logs from running pip install -e . and found complaints about the packages fmt and spdlog being missing. I installed the two of these from yum, and tried to proceed. CMake has stopped complaining about fmt, but still is asking for spdlog to be located. A useful command for searching all files for some text: grep -rnw '/path/to/somewhere/' -e 'pattern' ... For example: grep -rnw '.' -e 'CMAKE_PREFIX_PATH' Cleaned out my venv folder once again, and tried to follow the instructions from yrrapt I've recloned the bag3 develop branch on two separate machines, one with CentOS 7.9 and the other with Ubuntu 20.04. On each, I built a fresh python venv, and tried compiling the bag and pybag modules. In each case, the bag modules built properly, but the sub-module pybag is problematic. I've tried to read the CMakeLists.txt and follow the error build.log, to find clues about what packages might be missing or variables not configured, but no luck so far. My machines have the following: CentOS 7.9 Python 3.9.14 (built from source) CMake 3.17.5 fmt 8.22 Boost 1.53.0 gcc 7.3.1 Ubuntu 20.04 Python 3.8.10 CMake 3.16.3 fmt 8.30 Boost 1.71.0 gcc 9.4.0 Running python -m pip install . , I'm seeing build logs that contain the error below. After, Installed CMake with apt, wheel with pip, sudo apt install libboost1.71-all-dev, but still got: ERROR: Command errored out with exit status 1: command: /home/silab/delete_me_please/bag/venv/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-allzw648/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-allzw648/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-2t7f35c2 cwd: /tmp/pip-req-build-allzw648/ CMake Error at cbag/CMakeLists.txt:94 (find_package): Could not find a package configuration file provided by \"fmt\" with any of the following names: fmtConfig.cmake fmt-config.cmake Add the installation prefix of \"fmt\" to CMAKE_PREFIX_PATH or set \"fmt_DIR\" to a directory containing one of the above files. If \"fmt\" provides a separate development package or SDK, be sure it has been installed. I think I need to point CMake toward my fmt and spdlog installs. Will investigate further. Some quick notes on setting ENV variables: To set variable only for current shell: VARNAME=\"my value\" To set it for current shell and all processes started from current shell: export VARNAME=\"my value\" # shorter, less portable version To set it permanently for all future bash sessions add such line above to your .bashrc file in your $HOME directory. Can be found here To add something to the beginning of a path variable: export CMAKE_PREFIX_PATH=____:${CMAKE_PREFIX_PATH} where ___ is the thing you want to add To get libfmt: find_package did not find the CMake package of fmt. It comes with the -dev variant of fmt's Ubuntu package. If you check the CI code, it installs libfmt-dev via apt: sudo apt install libfmt-dev Now it's pretty clear that I need the .cmake file from this package, and to figure out where it is I can use: dpkg -L libfmt-dev Yes! Now I can just get this file: /usr/lib/x86_64-linux-gnu/cmake/fmt/fmt-config.cmake I'm not sure what 'depth' I have to point it at, but I will try all the way to /fmt. (I was right) Looks like I will also be missing spdlog, so I will installed the development flavor with: sudo apt install libspdlog-dev dpkg -L libspdlog-dev which gives: /usr/lib/x86_64-linux-gnu/cmake/spdlog/spdlogConfig.cmake Will point at this below Let's try it with yrrapt's official repo, just making sure we enable OA: git clone -b develop https://github.com/yrrapt/bag.git --recurse-submodules From inside bag dir python3.9 -m venv ~/designs/bag3_pll_verfication/venv source venv/bin/activate vim pybag/setup.cfg and deleted line openaccess-disable = True so that it could build with open access export PYBAG_PYTHON=$HOME/temp/bag/venv/bin/python python -m pip install wheel to install wheel packages python -m pip install . to build bag package export CMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu/cmake/fmt:${CMAKE_PREFIX_PATH} export CMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu/cmake/spdlog:${CMAKE_PREFIX_PATH} export CMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu/cmake/yaml-cpp:${CMAKE_PREFIX_PATH} cd inside pybag dir, and python -m pip install . again to build pybag package Nope, still errors: WARNING: building without OpenAccess support. -- Found Boost: /usr/lib/x86_64-linux-gnu/cmake/Boost-1.71.0/BoostConfig.cmake (found version \"1.71.0\") found components: serialization CMake Error at pybind11_generics/CMakeLists.txt:46: Parse error. Expected a command name, got unquoted argument with text \"<<<<<<<\". Checking this file, we find that in fact, that should probably be deleted haha. grep -rnw '.' -e 'CMAKE_PREFIX_PATH' CMake Error at pybind11_generics/pybind11/tools/pybind11Tools.cmake:17 (find_package): By not providing \"FindPythonLibsNew.cmake\" in CMAKE_MODULE_PATH this project has asked CMake to find a package configuration file provided by \"PythonLibsNew\", but CMake did not find one. Could not find a package configuration file provided by \"PythonLibsNew\" with any of the following names: PythonLibsNewConfig.cmake pythonlibsnew-config.cmake Add the installation prefix of \"PythonLibsNew\" to CMAKE_PREFIX_PATH or set \"PythonLibsNew_DIR\" to a directory containing one of the above files. If \"PythonLibsNew\" provides a separate development package or SDK, be sure it has been installed. Call Stack (most recent call first): pybind11_generics/pybind11/tools/pybind11Common.cmake:201 (include) pybind11_generics/pybind11/CMakeLists.txt:169 (include) Now it's asking me to find a .cmake file for PythonLibsNewConfig, but apt list doesn't reveal anything to install. But this already lives at ..... ./pybag/pybind11_generics/pybind11/tools/FindPythonLibsNew.cmake Can I try to hack it? export CMAKE_PREFIX_PATH=/home/silab/temp/bag/pybag/pybind11_generics/pybind11/tools:${CMAKE_PREFIX_PATH} Okay... that's enough for today. Compacting the lines from fefore: * export CMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu/cmake/fmt:/usr/lib/x86_64-linux-gnu/cmake/spdlog:CMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu/cmake/yaml-cpp:${CMAKE_PREFIX_PATH} Sat 17 Sep Perfect. Building your modified repostory I was able to avoid the CMake issues, after I installed libspdlog-dev , fmt-config, and libyaml-cpp-dev to provide the necessary .cmake files) Simpler than manually adding them to the CMake path via, as I was doing before: export CMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu/cmake/fmt:/usr/lib/x86_64-linux-gnu/cmake/spdlog:/usr/lib/x86_64-linux-gnu/cmake/yaml-cpp:${CMAKE_PREFIX_PATH} Case study to be shared In above case sutdy did they get this working? Maybe check with Mirjana/Abassi first? Did they get the Open Access stuff? Ask yrrapt about \"<<<<<<\" HEAD\" bug, and also about the FindPythonLibsNew.cmake bug. Ask yrrap how I can contribute, and if I should move discussions into the Git log tracker. Sunday 18 Sep I really want to apply for this competition: https://github.com/sscs-ose/sscs-ose-code-a-chip.github.io Its submission deadline is November 21, which would give me a perfect outlet for completing a functioning Jupyter notebook in a reasonable amount of time. I've already been here for 133 days, and I feel as though I haven't accomplished anything. For now though, don't tell Hans about this conference. I want to learn how to use BAG, because OpenFASOC isn't really meant to work with analog chip design. The issue is that BAG3 doesn't support open source tools, and that access for the necessary OpenAccess library is apparently very difficult to get anyways. I want to use BAG3, as BAG2 is not going to be further developed. My main question then is what are the major differences between the two? I'm meeting with Thomas tomorrow, and he seems to understand the better the status of BAG. So what if I did the following.... built a generator which is able to produce a functioning VCO layout.. or even PLL, and do it in a way that uses the toolchain of BAG3? If I accomplished this, I would be the first person to successfully use BAG3 outside of Berkeley. Other papers, few of which exist, have used BAG2 only. The issue now, is that I'm not sure if I can really build a functioning tool chain with BAG3. It doesn't support open source tools yet, and getting it to even work with Cadence seems nigh impossible for those outside Berkeley/BlueCheetah. I should confirm this by reading papers, tomorrow, of course. I think I can get through a lot of material by the time I meet with Thomas Parry at 3pm. One thing I'm realizing though is that a lot of the design process doesn't really require me to have a function BAG installation. XBase seems like it is very tied to BAG, but I think LayGO2 is less BAG-centric? I should really check with JD Han to figure out how I'm expected to produce a layout in 65nm if I don't have this special Cadence OpenAccess library. Perhaps I can w Mirjana is a member of the committee which decides the winners of the code-a-chip competition, and so if I were to develop a notebook with her guidance, then there is a very good chance of being accepted. Another note, I should consider giving a group presentation on Sep 29th, because I told Hans that I would. Maybe I should break this promise though. I should explain to Hans that understanding the status of all this open source code has taken significantly more time than I anticipated. Explain that I've made connections with two important people though! If I don't use BAG, then how do I work with the schematics needed to compare against LayGO layouts during LVS? If I want to design a process portable generator, do I target the layout constraints of a small-fill factor 28nm design, and then expect it to scale to a 65nm, or even 130nm process? I think this makes more sense, as there are less degrees of freedom in 28nm. Can this tool (LayGO2) be used in isolation, without BAG? I think it can! One thing that is amazing about this, is that Laygo is written entirely in Python, and so I shouldn't have any huge problems getting it to run. No compilation required, etc. It's a 1-way path for generation. Also, much of the design work 'back of the envelope work' can be done in Python scripts, without any circuit simulation at all. But, then, once I get to circuit simulation, I wonder if I am able to run parasitic extraction with Magic on open source Tools on 65nm? If think seems to work well, then I think I can trust it enough to do large cell design without re-running a DRC/LVS ruleset in Calibrew. To be double sure, I can simply run a final Parasitic extraction and DRC and LVS test against Cadence. There's also no reason why my schematics would necessarily need to be be designed in cadence source tools, right? If I am looking for process portability though, I think that I need to use something like BAG, where I am am able to make schematic and test bench generators. So one question is: If I'm not using BAG... how do I achieve process portability in my schematics, such that I can quickly compare them against laygo? I think that the basic Laygo workflow uses hand-crafted primitives, which aren't process agnostic. On the other hand, I think that Lay I can slowly replace every part of the 65nm workflow in Cadence, with equivalents in 65nm technology. Monday 19 Sep I think the functionality is primarily good for the Analog Chip Bottom, where the SAR ADC, the Bandgap references, the Analog Multiplexer, and monitor ADCs, the Calibration Injection Voltages, the Serdes, and the CDR and TX/RX Circuits live. This is where documentation, reusability, comprehension, and testability are much, much more important that cutting edge performance. Generators are also best suited for instances where you will need to do many different iterations, in possilbly different BAG3 is a major upgrade, as it reworks the way layouts are generated in the tool. There is no AnalogBase/DigitalBase constructs anymore, just MOSBase. As long as devices share the same 'row' information, you can tile NMOs, PMOS, and TAPs right next to each other. Don't specify the width of each wire, have 'wire classes' which have preset widths. MOSBase just places the drawn transistors, and isn't necessarily DRC clean. MOSBase Wrapper then comes back and fill in the dummy devices, extension regions, and boundaries. In the YAML file, we have to define the properties of each row. It's more complicated in BAG3. The layouts may only have one type of tile, or you may have a more complicated multi-tile type layout. In BAG2 AnalogBase, you only have access to M3 and 4, (maybe more in Digital Base). But in BAG3 Zhaokai Liu showed a BAG3 repository example on Github, but it's a private repo right now, and we can't see it. Bulk contacts are completed with TAP devices in BAG3. Different length devices are implemented with stacking devices, to increase effective channel length. To place transistors, you first specify the tile you are placing in, and then use XY coordinate (e.g.) within the tile. It defaults to position 0 for X and Y. BAG2 only supports ADEXL. BAG3 doesn't support any ADE package, Assembler, XL, or otherwise. Design variables are returned, but Stimuli and Data post processing must be done in Python, as we are just directly calling Spectre. Notice how all the BAG developers are also using BAG to build real circuits, meaning they have real design cases. BAG3 includes BAG2 Behavioral models of base circuits are completed manually, but then the hierarchical arrangement of them is done automatically, tracking whatever is done at in the real schematic implementation. Meeting with Thomas Parry Marco in Finland Marjana is moving to Infineon Infineon guys also putting money into Matthias is from Klayout, and he was able to implement a lot of the functions for BAG2 compatibility very quickly. Tuesday 20 Sep continued with Ayan Biswas: Manually create leaf behavioural cells, and then allow BAg to generate the heirarchy. One thing is that schematic and modeling hierarchy should be identical, but layout heirarchy my be different, as for example, a large chain of CS amplifiers would generated all the resistors in one heirarchy. Recap: To be honest I have not produce much at all, but it hasn't been for lack of trying or time invested. It turns out that this is way harder than I thought. I've worked on: * BAG project repository organization and setup * Python package management * Review of OOP programming in Python (To understand BAG codebase) * Jupyter Notebook Usage and Server Setup * CMake, Make, and other build related tooling (for building Python, BAG, PyBAG, XBase, etc) * Tools are 'complete' but not distributed in a complete form, so they have to be built One thing that is a bit annoying is that I have several different places where I could develop my code. I think to submit to CAC23 I need to have a GitHub repo. My personal Github account (kottenforst, or kennedycaisley, or kcaisley?) Things to understand from Hans: Can the models of TSMC65 be using in ngSPICE? Can LayGO2 be used without BAG? I think that the goal of the MOSAIC group is to modify the BAG3 codebase such that it can be used with open source tools like Xschem/Magic/Klayout. This is different from the goals of my group. My goal of my meeting with Mirjana is to figure out where my design work can overlap with the interests of MOSAIC. I don't have the time and the energy and You can use BAG3 for Measurements, without needing to have the openaccess library tooling. I think that I should maybe start here, and build a BAG3 test bench for my PLL is 65 nm. Skill is signifigantly slower than OA, but Mario Weiss also pointed out that GDS based generation is available in BAG2/BAG3. Some flows work without Virtuoso, but some functions require OA layout. Meeting with Marjana: Programatic IC design is necessary Talk to Chris Mayberry, BAG3, as he may be interested in helping Watch his 'Cascode Labs Presentation' UC Berkeley prof Vladamir S. it coming to Europe, Marjana may be able to talk with him Meeting with Hans: Recap what I've been working on, and what I've learned: Discovered a whole bunch of disorganized project folder for BAG2, BAG3, Xbase, Laygo, Laygo2 BAG project repository organization and setup (built my own repository) Python package management (built my own environment) Jupyter Notebook Usage and Server Setup OOP programming in Python (To understand BAG codebase) CMake, Make, and other build related tooling (for building Python, BAG, PyBAG, XBase, etc as Tools are not distributed built) Got very stuck 2 weeks ago, as I had spent many weeks now reading the code base, and trying to debug. I understand how the tool worked alot better, but I couldn't actually 'run' it full on anything. Went online (open source silicon slack channel), started connecting with people, and finding resources. I understand the lay of the land a lot better now, and the uses/limits of the tools, but I haven't made much progress on actual work (only basic prototyping of PLL specifications in Python). What I need help with is technology evaluation, and where to focus my efforts? I'm am very serious about this programmatic IC design, but it's early days. We for some of the features, we would be the first externel adopters (not Laygo1/2 or Bag2 though, but for BAG3 yes.) State of the art: BAG2 Workflow: Schematic in Cadence with generic primitives, connects with Cadence over SKILL API. Allows use of Laygo or Xbase. Status: Stable but slower, and no longer developed by UC berkeley. It does however has BAG3 Workflow: Allows use of up Status: Streamlined, actively developed, several features improved, and many aspects of the workflow have bee People: Chris Mayberry Marjana Thomas Parry In git, what is main, origin, HEAD, origin is the default name given to the remote repository that a local repository is tied to. One can change this name, and can also add two different remotes to pull changes from. This is why we need aliases for remote repositories, so that we can distinguish between two remote codebases that are often nearly identical. BAG3 Thoughts: I think the functionality is primarily good for the Analog Chip Bottom, where the SAR ADC, the Bandgap references, the Analog Multiplexer, and monitor ADCs, the Calibration Injection Voltages, the Serdes, and the CDR and TX/RX Circuits live. This is where documentation, reusability, comprehension, and testability are much, much more important that cutting edge performance. BAG3 is a major upgrade, as it reworks the way layouts are generated in the tool. There is no AnalogBase/DigitalBase constructs anymore, just MOSBase. As long as devices share the same 'row' information, you can tile NMOs, PMOS, and TAPs right next to each other. Don't specify the width of each wire, have 'wire classes' which have preset widths. MOSBase just places the drawn transistors, and isn't necessarily DRC clean. MOSBase Wrapper then comes back and fill in the dummy devices, extension regions, and boundaries. In the YAML file, we have to define the properties of each row. It's more complicated in BAG3. The layouts may only have one type of tile, or you may have a more complicated multi-tile type layout. In BAG2 AnalogBase, you only have access to M3 and 4, (maybe more in Digital Base). But in BAG3 Zhaokai Liu showed a BAG3 repository example on Github, but it's a private repo right now, and we can't see it. Hi Thomas and Mirjana, After discussing with my co-workers + advisor, we've decided we want to identify and try acquiring, through official channels, the OpenAccess library used by BAG3 to interact with Virtuoso schematic and layout cellviews. If we succeed, we will document the steps we took to acquire the package, and share that knowledge with other groups. For those lacking the time or connections to acquire the dependency themselves, we would also be willing to take on the mantle of regularly compiling and distributing BAG3 with pybag/cbag binaries built against the OpenAccess component. Assuming it's legally allowed, I wonder if this would be useful for the subset of people primarily interested in just being users of BAG3 with Cadence. As the above step could take considerable time with NDAs, in parallel, I want to figure out if the component of BAG3 responsible for launching SPECTRE/SPICE simulations and returning the results is able to be used without the OpenAccess component. If so, I will compile a version of BAG with this reduced-functionality. Assuming I manage to make this work, I want to then use this for a project: My group invested significant time (4 revisions) designing this ^ 65nm clock-data recovery circuit for high radiation environments (total ionizing dose >1 Grad). The original design was done with a traditional industry workflow (Cadence Virtuoso, Calibre DRC/LVS/PEX, ADE XL testbenches, Spectre Simulation). Now that we have a silicon-verified design, though, we'd like to record that design knowledge in a BAG3 script for better documentation, modifications, and porting. Assuming BAG3's simulation component can be used as-in, I am going to start by re-implementing our verification framework for the PLL with the BAG3 API, and open source it. To Do Backlog [ ] Generate pre and post-extracted netlist of 65nm VCO [ ] Create a basic simulation of inverter, using tradition GUI Virtuoso Schematic/Layout -> PEX Extraction -> ADE XL -> Spectre -> ADE XL viewer. [ ] Reproduce basic INV simulation with SPICE netlist -> SPECTRE Command Line -> Flat File Output [ ] Reproduce basic INV simulation with: SPICE netlist -> Jupyter Notebook -> BAG3 -> SPECTRE -> BAG3 -> Jupyter Notebook. [ ] Start assembling list of tests to run on VCO. In Progress [ ] Identify what OpenAccess library is needed to properly compile and operate BAG3 (make issue of this) [ ] Move my venv over from my other venv, or just re-setup [ ] Figure out the ask to Ayan Biswas, Thomas Parry, or Marjana, to try and figure out exactly 'what' OpenAccess library is missing. Completed Notes Should only have a vscode and firefox window open. Nothing more. No readme file until done. No figures or equations until done (just look in Razavi) A decorator '@' is a design pattern in Python that allows a user to add new functionality to an existing object without modifying its structure. Decorators are usually called before the definition of a function you want to decorate. They support operations such as being passed as an argument, returned from a function, modified, and assigned to a variable. This is a fundamental concept to understand before we delve into creating Python decorators. This tutorial explains it best: https://www.datacamp.com/tutorial/decorators-python The class SimProcessManager an implementation of :class: SimAccess using :class: SubProcessManager I'm not really even sure what this means. SubProcessManager looks like it batches calls in an concurrent/asyncrhonous mannager, using the asyncio Python standard lib But it looks like we would never call these latter two components, as they are lower level. Inside of the Spectre.py function, the OpenAccess Dependency Research A static library format for OA would require the rest of BAG be built with the the same version of build tools as it. Build tools are availabe from the CentOS software collections, in particular the devtoolset-7 through devltoolset-11 packages. These can be installed from yum, and examined with: sudo yum list devtoolset\\* As each component is also available as it's own package, we can examine package contents via: sudo yum list devtoolset-8\\* You can get started in three easy steps: Install a package with repository for your system: On CentOS, install package centos-release-scl available in CentOS repository: $ sudo yum install centos-release-scl On RHEL, enable RHSCL repository for you system: $ sudo yum-config-manager --enable rhel-server-rhscl-7-rpms Install the collection: $ sudo yum install devtoolset-8 Start using software collections: $ scl enable devtoolset-8 bash At this point you should be able to use gcc and other tools just as a normal application. See examples below: $ gcc hello.c $ sudo yum install devtoolset-8-valgrind $ valgrind ./a.out $ gdb ./a.out As Ayan points out, his .bashrc points to the locations of the various libraries. I notice that the version of OA is 22.60, which appears to be built against GCC 8.3, which is contained in devtoolset-8. It looks like OA 22.60 is maybe the most recent version? https://si2.org/tag/oa-22-60/. In either case, it's obvious that the version matters. NEXT STEPS Contact Thomas first thing, share what I learned from Ayan, including imports in .bashrc, and what version of GCC is necessary, and version of OA. Next read the 6-7 tabs I have open about building from C++... link link link link Work in October oa_v22.60.063 Line 37 in this file: https://github.com/ucb-art/cds_ff_mpt-bag3/blob/master/workspace_setup/.bashrc Executables, Libaries, etc Both libraries and executables are compiled non-human readable binaries, but the difference is that an executable will be intended to be used by itself and have a defined starting point, whereas code compiled as a library will have multiple extry points, and will be intended to be used as part of a larger project. When linking in a library, whether or not is static vs dynamic matters. In either case though, code described as 'libraries' are not executible on their own. A static library .a (or .lib on windows) is combined with compiled .c or .cpp machine .o in a process called linking (ld program), to produce an executible file with an .out (or .exe, on windows) extension. Static libraries increase the size of the code in your binary. They're always loaded and whatever version of the code you compiled with is the version of the code that will run. Dynamic libraries, with a .so (or .dll extension on Windows) are stored and versioned separately. It's possible for a version of the dynamic library to be loaded that wasn't the original one that shipped with your code if the update is considered binary compatible with the original version. Additionally dynamic libraries aren't necessarily loaded -- they're usually loaded when first called -- and can be shared among components that use the same library (multiple data loads, one code load). Dynamic libraries were considered to be the better approach most of the time, but originally they had a major flaw (google DLL hell), which has all but been eliminated by more recent Windows OSes (Windows XP in particular) Work in November The OA binary libraries (.so) we have are for OA versions as recent as 22.60. In some instances we have their matching header (.h) files. But they have been compiled with GCC 4.4.x or older (probably corresponding to RHEL6). Our use case for these libraries is to link to them in from some C++17 source code. Here is a link which talks about if it's possible to link in libraries that were built with a different version of GCC/ C++ https://stackoverflow.com/questions/46746878/is-it-safe-to-link-c17-c14-and-c11-objects Response from Ayan: Hello Kennedy, The oa_v22.60.s007 is available in the Cadence Virtuoso library (IC618 or ICADVM181 or ICADVM201), as you mentioned. But BAG3 also requires the OA C++ libraries compiled for the OS (RHEL7) and C++ compiler (gcc 8) as you can see on lines 34-36 of the .bashrc that you linked. We were hoping to get the updated version of those libraries (raw or compiled). But it looks like you hit the same problems as us while trying to work out the deal with Si2. Some researchers in our group are trying to figure out ways to remove the OA dependence, or understand it a bit better by reading the available code base, in an effort to make BAG3 truly \"open source\" and accessible by people outside BWRC. I will let you know if those attempts converge to an acceptable solution. Message from me: This does match the stance on Si2's website, which says only \"a license-keyed binary version is available for research and teaching purposes.\" What new version of the OA C++ libraries are you trying to acquire? I see a reference to oa_v22.60.s007 in your workspace setup .bashrc file, and my understanding is OpenAccess 22.60 (DM6) is the latest release. As an aside, do you know if the OA API dynamic library .so files included in the standard Cadence Virtuoso install (at ../IC618/tools.lnx86/lib/64bit/ and ../IC618/oa_v22.60.063/lib/linux_rhel60_64/opt) are essentially what we'd have by building C++ source or getting the binary library from Si2? The directories don't include associated header files or exact compiler toolchain, but the file names do match the target link library references in BAG3's CBAG CMakeLists.txt file. Message from Hans: /cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa/22.43p006/include/oa/ /cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa/22.50p001/include/oa/ /cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa/2.2.6/include/oa/ /cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa/22.41.004/include/oa/ /cadence/mentor/ixl_cal_2016.1_14.11/shared/pkgs/icv_oa/22.43p006/include/oa/ /cadence/mentor/ixl_cal_2016.1_14.11/shared/pkgs/icv_oa/22.50p001/include/oa/ /cadence/mentor/ixl_cal_2016.1_14.11/shared/pkgs/icv_oa/2.2.6/include/oa/ /cadence/mentor/ixl_cal_2016.1_14.11/shared/pkgs/icv_oa/22.41.004/include/oa/ /cadence/mentor/ixl_cal_2012.4_25.21/shared/pkgs/icv_oa/2.2.6/include/oa/ /cadence/mentor/ixl_cal_2012.4_25.21/shared/pkgs/icv_oa/latest/include/oa/ I think the header files from the different versions (except the 2.x) are very similar. I just briefly checked a few and they only differ in adding an overloaded function here and there or changes to the include list. They might work for a wide range of lib versions. ETC setup.cfg is a config file, which is read in by setup.py and contains infor that setup.py otherwise could. (Setup.py is a replacement for setuptools, as it is being deprecated) config files like these oftentime also have the .ini file type or the the .conf file type. Building Pybag: OA_LINK_DIR To run CMake (make sure venv is active first) python -m pip install . Reading setup.py and both CMakeLists.txt files at bottom of setup.py, setup() function starts process, ext_modules and cmdclass looks to be important setup.py is a older and more featured setup script, and setup.cfg and pyproject.toml are newer versions that are simpler and recommended unless you need the features of the older project. Setuptools can build C/C++ extension modules. The keyword argument ext_modules of setup() should be a list of instances of the setuptools.Extension class. So set the ext_modules keyword and inherit this class from the setuptools.Extension class, where we set a class CMakePyBind11Extension(Extension): def __init__(self, name, sourcedir=''): Extension.__init__(self, name, sources=[]) self.sourcedir = str(Path(sourcedir).resolve()) setup( package_dir{'': 'src'}, // fed to the ext_modules command below, as a base direcotory for extensions ext_modules=[CMakePyBind11Extension('all')], // this lists the specifics of extensions what is to be built cmdclass={'build_ext': CMakePyBind11Build}, // this actually runs the build extension operation ) Inside the CmakePyBind11Build object, there is a method called build_extension , which: calculates output directory for the build init cmake command w/: build source dir, temp build dir, output dir, build type, optional compiler launcher settings build cmake command run ./gen_stubs.sh Running setup.py in pybag library \"pip install .\" But build.sh appears to not follow this logic Instead, run build.sh: # this script builds the C++ extension if [ -z ${OA_LINK_DIR+x} ] then echo \"OA_LINK_DIR is unset\" exit 1 fi if [ -z ${PYBAG_PYTHON+x} ] then echo \"PYBAG_PYTHON is unset\" exit 1 fi ${PYBAG_PYTHON} setup.py build The if [ -z ${VAR} ] command evaluates to true if VAR is null, which is what the -z does. as [ ] is short for the test command in bash the ${...} construct is parameter expansion Ah shit, it looks like this +x nonsense was deprecated as of Bash v4.3, which is why this isn't working. Let's just run setup.py build directly. Therefore, it's obvious that we need OA_LINK_DIR and PYBAG_PYTHON export PYBAG_PYTHON=.... /bin/python3 CMAKE_PREFIX_PATH used by CMake, and is set to fmt,Boost,yamp-cpp, spdlog directories. It normally set at a high level by the output tool in .bashrc files of BAG project. OA_INCLUDE_DIR used in cbag CMakeLists.txt, set in .bashrc OA_LINK_DIR used in cbag CMakeLists.txt, set in .bashrc export OA_LINK_DIR=${OA_SRC_ROOT}/lib/linux_rhel70_gcc83x_64/opt `sudo dnf install cmake` `sudo dnf install fmt-devel boost-devel spdlog-devel yaml-cpp-devel` to get the packages I want: sudo yum install fmt-devel boost-devel spdlog-devel yaml-cpp-devel You must do the setup in this order: python -m venv ~/designs/dmc65v2/.venv/ source /env/bin/activate then scl enable devtoolset-8 bash otherwise, the venv will clear the scl env. export PYBAG_PYTHON=/faust/user/kcaisley/designs/dmc65v2/.venv/bin/python export OA_SRC_ROOT=/cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa/22.50p001 export OA_LINK_DIR=${OA_SRC_ROOT}/lib/linux_rhel50_gcc44x_64/opt #notice these are build for rhel5, with gcc4 export OA_INCLUDE_DIR=${OA_SRC_ROOT}/include export CMAKE_PREFIX_PATH=/faust/user/kcaisley/packages/spdlog-1.x/cmake:${CMAKE_PREFIX_PATH} export CMAKE_PREFIX_PATH=/faust/user/kcaisley/packages/yaml-cpp-yaml-cpp-0.7.0:${CMAKE_PREFIX_PATH} Fist we will need to following libraries: * Boost * fmt * spdlog > 1.x * yaml-cpp To make them accessible to the CMake tool, we first need to make sure they are on our system (and of the devel variety), and then add something to the beginning of the CMake path variable: export CMAKE_PREFIX_PATH=____:${CMAKE_PREFIX_PATH} where ___ is the thing you want to add We are looking for the files of type: /usr/lib/x86_64-linux-gnu/cmake/fmt/fmt-config.cmake In this case, we would target the parent directory: export CMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu/cmake/fmt:${CMAKE_PREFIX_PATH} Okay, so this strategy of trying to use a bunch of manual downloads in not going to work... Let's move to Fedora. Both the spdlog-devel and fmt-devel have their XXXConfig.cmake files right there. Trying again in Fedora: inside /pybag, make sure you delete the _build folder, as it contains a cache of settings NO, IT'S FINE, I JUST DIDN'T HAVE THE ENV VAR PROPERLY SET. build.sh run script is uses deprecated commands, so we will need to launch setup.py manually You must do the setup in this order: sudo dnf install cmake sudo dnf install fmt-devel boost-devel spdlog-devel yaml-cpp-devel sudo dnf install gcc_c++ sudo dnf install python3.10 as h5py doesn't work on python 3.11 yet python3.10 -m venv ~/designs/dmc65v2/.venv as h5py doesn't work on python 3.11 yet python -m pip install wheel source .venv/bin/activate to activate environment export PYBAG_PYTHON=/faust/user/kcaisley/designs/dmc65v2/.venv/bin/python export OA_SRC_ROOT=/cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa/22.50p001 export OA_LINK_DIR=${OA_SRC_ROOT}/lib/linux_rhel50_gcc44x_64/opt #notice these are build for rhel5, with gcc4 export OA_INCLUDE_DIR=${OA_SRC_ROOT}/include export CXX=g++ from here BAG To install the top level BAG python lib simply navigate to this directory and execute: python -m pip install . PyBAG + CBAG + pybind11 To install the lower level pybag package tools navigate to the pybag directory and execute: python -m pip install . --log build.log Diff of stuff changed in cbag diff --git a/CMakeLists.txt b/CMakeLists.txt index 6ea5233..67896a4 100644 --- a/CMakeLists.txt +++ b/CMakeLists.txt @@ -202,7 +202,7 @@ set(SRC_FILES_LIB_CBAG_OA ${CMAKE_CURRENT_SOURCE_DIR}/src/cbag/oa/write.cpp ) - + if(DEFINED ENV{OA_LINK_DIR}) message(\"OA include directory: \" $ENV{OA_INCLUDE_DIR}) message(\"OA link directory: \" $ENV{OA_LINK_DIR}) @@ -226,6 +226,7 @@ target_link_libraries(cbag oaCommon oaPlugIn PRIVATE + spdlog::spdlog #https://bugzilla.redhat.com/show_bug.cgi?id=1851497 stdc++fs ${Boost_LIBRARIES} yaml-cpp diff --git a/include/cbag/common/typedefs.h b/include/cbag/common/typedefs.h index 7ce06b1..86dbba1 100644 --- a/include/cbag/common/typedefs.h +++ b/include/cbag/common/typedefs.h @@ -49,6 +49,9 @@ limitations under the License. #include <cstdint> #include <tuple> +#include <limits> //https://stackoverflow.com/questions/71296302/numeric-limits-is-not-a-member-of-std +#include <optional> //based on above comment +#include <boost/geometry.hpp> //https://github.com/pgRouting/pgrouting/issues/1825 namespace cbag { diff --git a/src/cbag/layout/path_util.cpp b/src/cbag/layout/path_util.cpp index b9a0802..def66aa 100644 --- a/src/cbag/layout/path_util.cpp +++ b/src/cbag/layout/path_util.cpp @@ -46,6 +46,7 @@ limitations under the License. #include <fmt/core.h> #include <fmt/ostream.h> +#include <fmt/format.h> // trying to add this code #include <cbag/layout/path_util.h> you make use git clean -dfx , that will remove all files not under source control. Be careful with this though, as it can delete newly created files if they haven't been added to source control yet. this assumes an in-source build, of course, which is a bad habit Downgrading packages on Fedora: Error with {fmt} package during build /usr/include/fmt/core.h:1757:7: error: static assertion failed: Cannot format an argument. To make type T formattable provide a formatter<T> specialization: https://fmt.dev/latest/api.html#udt Solution: It looks like other people have have problems with fmt To downgrade to fmt 8.1, we can use the koji build system: https://koji.fedoraproject.org/koji/buildinfo?buildID=1927503 looks like the newest version of spdlog (1.11) supports fmt 9.1, and so we need to downgrade to spdlog 1.10 to support fmt 8.1.1. here's the fmt/fmt-devel packages: https://koji.fedoraproject.org/koji/buildinfo?buildID=1927503 here's the spdlog/spdlog-devel packages: If a package is still in the repos, here's how you find and target it: https://unix.stackexchange.com/questions/266888/can-i-force-dnf-to-install-an-old-version-of-a-package dnf --showduplicates list <package> List packages that depend on the package of choice: dnf repoquery --installed --whatrequires qemu-kvm manually downgrade to a different package version downloaded from this link sudo dnf downgrade ~/Downloads/fmt-8.1.1-5.fc37.x86_64.rpm Had to remove gnome boxes, as it depends on fmt lib: sudo dnf remove gnome-boxes sudo dnf remove fmt fmt-devel spdlog spdlog-devel (march 02) I knew I needed older version of spdlog to be compatible with my older fmt, based on this wget https://kojipkgs.fedoraproject.org//packages/fmt/8.1.1/4.fc37/x86_64/fmt-8.1.1-4.fc37.x86_64.rpm https://kojipkgs.fedoraproject.org//packages/fmt/8.1.1/4.fc37/x86_64/fmt-devel-8.1.1-4.fc37.x86_64.rpm https://kojipkgs.fedoraproject.org//packages/spdlog/1.10.0/1.fc37/x86_64/spdlog-1.10.0-1.fc37.x86_64.rpm https://kojipkgs.fedoraproject.org//packages/spdlog/1.10.0/1.fc37/x86_64/spdlog-devel-1.10.0-1.fc37.x86_64.rpm -P ~/Downloads/ then simply manually install all of these rpms, assuming nothing else is in the download path sudo dnf install ~/Downloads/*.rpm Error with Boost packages during build /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/spirit/range.cpp:53:1: required from here /usr/include/boost/spirit/home/x3/operator/detail/sequence.hpp:144:25: error: static assertion failed: Size of the passed attribute is bigger than expected. 144 | actual_size <= expected_size | ~~~~~~~~~~~~^~~~~~~~~~~~~~~~ Let's try downgrading Boost and Boost devel: > sudo dnf remove boost boost-devel > wget https://kojipkgs.fedoraproject.org//packages/boost/1.76.0/10.fc37/x86_64/boost-1.76.0-10.fc37.x86_64.rpm https://kojipkgs.fedoraproject.org//packages/boost/1.76.0/10.fc37/x86_64/boost-devel-1.76.0-10.fc37.x86_64.rpm -P ~/Downloads/ Now I can't just manually install these now, because a couple core system components, namely gnome-shell require other boost libraries, like boost-system which an older version of boost wouldn't be compatible with. fmt and spdlog fmt is a formatting library which provides a standardized, safe, fast way to construct strings. This is useful for output messages and dynamically generating filepaths in C++ code. spdlog, pronounced 'speed log', is a library built on top of {fmt} which allows for the putting more effor into understanding BAG: the implementation of cbag is as a header only library. It uses January 12: picking back up work My current status ilss that I need to compile and install pybag, which depends on a bunch of different packages. Building the cbag subrepo though requires fmt , spdlog , and a collection of boost libs. The specific versions needed can't be accessed easily via the Fedora fedora dnf repos, and so I fear I'll just need to manually download and build the projects from source. First, let's examine what we currently have installed, and specify exactly what we need to download. January 20: continuing with development I think looking at how cbag and cbag_polygon are built first might be helpful in this case. One must understand that CMake is a build system, in that it simple generates build files which are then compiles with a system's native build environment. Running just the CMake build command gives me several clues: In the CMakeLists.txt file, I see a control flow statement for: if(DEFINED ENV{OA_LINK_DIR}) I've check it, and it's not evaluating, the else() statment below is instead. Workflow: [kcaisley@asiclab008 ~]$ cd designs/dmc65v2/ [kcaisley@asiclab008 dmc65v2]$ source setup.sh (.venv) [kcaisley@asiclab008 dmc65v2]$ cd bag/pybag/cbag/ (.venv) [kcaisley@asiclab008 cbag]$ rm -r _build (.venv) [kcaisley@asiclab008 cbag]$ export BUILD_TYPE=${1:-Debug} (.venv) [kcaisley@asiclab008 cbag]$ cmake -H. -Bbuild -DCMAKE_BUILD_TYPE=${BUILD_TYPE} -DCMAKE_CXX_COMPILER_LAUNCHER=ccache Where -H This internal option is not documented but widely used by community and Has been replaced in 3.13 with the official source directory flag of -S. there is no space place after for the directory, so the . is the local location -B Starting with CMake 3.13, -B is an officially supported flag, can handle spaces correctly and can be used independently of the -S or -H options. again, there should be nospace, so _build is the build directory name -D -D : = Create a cmake cache entry. When cmake is first run in an empty build tree, it creates a CMakeCache.txt file and populates it with customizable settings for the project. This option may be used to specify a setting that takes priority over the project's default value. The option may be repeated for as many cache entries as desired. Again, no space! There are hundreds of options in this file, but this comman above is specifically setting: CMAKE_BUILD_TYPE which is being set to 'Debug' CMAKE_CXX_COMPILER_LAUNCHER which passes the 'ccache' option to the makefiles generator, no idea relaly that this does || exit suffix should be removed, as this is being run on the command line, and this will cause the terminal to exit if the command fails Now I just need to understand why the WARNING: building without OpenAccess support. path is running..... I think the issue is that there is some complexity to CMake variable/environment variable scope system. Yes, this is the issues. Those unix bash environment variables are not visible inside the CMake scope. Let's figure out how this can be set. It's not being passed down from higher level CMake script, so it must be in the way that the command to build is called. cmake -H. -B_build -DCMAKE_BUILD_TYPE=${BUILD_TYPE} -DCMAKE_CXX_COMPILER_LAUNCHER=ccache -DOA_LINK_DIR=/cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa/22.50p001/lib/linux_rhel50_gcc44x_64/opt The CMakeCache.txt File After the first configuration of a project, CMake persists variable information in a text file called CMakeCache.txt. Caches are used to improve. When CMake is re-run on a project the cache is read before starting so that some re-parsing time can be saved on CMakeLists.txt. Here is where passing parameters to CMake befuddles a beginner. If a variable is passed via the command line that variable is stored in the cache. Accessing that variable on future runs of CMake will always get the value stored inside the cache and new value passed through the command line are ignored. To make CMake take the new value passed through the command line the first value have to be explicitly undefined. Like so: cmake -U <previously defined variable> -D <previously defined variable>[=new value] (A better approach is to use CMake internal variables. For more information refer the CMake manual here.) The syntax if(DEFINED <name>|CACHE{<name>}|ENV{<name>}) is true if a variable, cache variable or environment variable with given is defined. The value of the variable does not matter. Environment Variables are like ordinary Variables, with the following differences: * Environment variables have global scope in a CMake process. They are never cached. * Variable References have the form $ENV{ }, using the ENV operator. * Initial values of the CMake environment variables are those of the calling process. Values can be changed using the set() and unset() commands. These commands only affect the running CMake process, not the system environment at large. Changed values are not written back to the calling process, and they are not seen by subsequent build or test processes. * See the cmake -E env command-line tool to run a command in a modified environment. * Source: https://cmake.org/cmake/help/latest/manual/cmake-language.7.html#cmake-language-environment-variables This isn't working because I'm setting regular variables, but the check is for an environment variable. Ohhhhh. If you set an vairable, just by writing it, it will only be valid in that shell. It's called a variable. If you write 'export' first though, it's now and 'environment variable' and will be active in all child processes. Now, if it's inside a script (which is run as a child process), and if you want to access it inside another child process, you need to make sure that first you 'source' the script, so that the contents of the script re available in the parent bash session, but then you also need to add the 'export' command, if you want it to be available in a child pocess like CMake. So in short the 'source' command makes bash script which would otherwise be child process, instead run in the parent bash session. And 'export' makes all variables defined in the parent process also defined in the child processes. Fully documented build problems: [kcaisley@asiclab008 ~]$ cd designs/dmc65v2/ [kcaisley@asiclab008 dmc65v2]$ source setup.sh (.venv) [kcaisley@asiclab008 dmc65v2]$ cd bag/pybag/cbag/ (.venv) [kcaisley@asiclab008 cbag]$ mkdir build (.venv) [kcaisley@asiclab008 cbag]$ rm -r build/* (.venv) [kcaisley@asiclab008 cbag]$ cd build (.venv) [kcaisley@asiclab008 cbag]$ cmake ../ (.venv) [kcaisley@asiclab008 cbag]$ cd .. (.venv) [kcaisley@asiclab008 cbag]$ cmake --build build Problem 1: (.venv) [kcaisley@asiclab008 build]$ cmake --build . [ 1%] Building CXX object CMakeFiles/cbag.dir/src/cbag/common/box_collection.cpp.o In file included from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/common/box_t.h:50, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/common/box_array.h:50, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/common/box_collection.h:23, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/common/box_collection.cpp:18: /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/common/typedefs.h:73:33: error: \u2018numeric_limits\u2019 is not a member of \u2018std\u2019 73 | constexpr auto COORD_MIN = std::numeric_limits<coord_t>::min(); | ^~~~~~~~~~~~~~ /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/common/typedefs.h:73:55: error: expected primary-expression before \u2018>\u2019 token 73 | constexpr auto COORD_MIN = std::numeric_limits<coord_t>::min(); | ^ compilation terminated due to -fmax-errors=2. gmake[2]: *** [CMakeFiles/cbag.dir/build.make:76: CMakeFiles/cbag.dir/src/cbag/common/box_collection.cpp.o] Error 1 gmake[1]: *** [CMakeFiles/Makefile2:124: CMakeFiles/cbag.dir/all] Error 2 gmake: *** [Makefile:136: all] Error 2 When including files, if it's just a simple world like #include <tuple> , then this is probably a C++ standard lib component. If it is formatted like #include <boost/geometry.hpp> , then this is a good clue that the library isn't standard due to the .hpp suffix, and also that the libary is written in C++. If you have one like #include <limits.h> , we know this is written in C, and there is a good chance this is actually part of the C standard library, which is included in the C++ std lib but is deprecated and not advisable to use. The fix was including the <limits> standard library header in the offending file, so that std::numeric_limits is defined. One thing I learned from the above error log is that we can see the trace of imports that lead to the offending file. We tried to compile src/cbag/common/box_collection.cpp but the error was in cbag/include/cbag/common/typedefs.h . We we read the \"build line first\", then the \"In file included from\" bottom to top\", then finally the lowest section with the \"error\" to find our problem. In this case, our solution was adding a single line: kcaisley@asiclab008 cbag]$ git diff include/cbag/common/typedefs.h diff --git a/include/cbag/common/typedefs.h b/include/cbag/common/typedefs.h index 7ce06b1..ed526cb 100644 --- a/include/cbag/common/typedefs.h +++ b/include/cbag/common/typedefs.h @@ -49,6 +49,7 @@ limitations under the License. #include <cstdint> #include <tuple> +#include <limits> //needed for std::numeric_limits NOTE: I believe that <cstdint> defines primitive types like int32_t . Like the other std lib headers, gcc stores them at /usr/include/c++/12 Problem #2 (.venv) [kcaisley@asiclab008 cbag]$ cmake --build build [ 1%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/main.cpp.o In file included from /usr/include/spdlog/common.h:45, from /usr/include/spdlog/spdlog.h:12, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/logging/spdlog.h:53, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/logging/logging.h:55, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/gdsii/read.h:58, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/gdsii/main.cpp:25: /usr/include/spdlog/fmt/fmt.h:27:14: fatal error: spdlog/fmt/bundled/core.h: No such file or directory 27 | # include <spdlog/fmt/bundled/core.h> | ^~~~~~~~~~~~~~~~~~~~~~~~~~~ compilation terminated. gmake[2]: *** [CMakeFiles/cbag.dir/build.make:104: CMakeFiles/cbag.dir/src/cbag/gdsii/main.cpp.o] Error 1 gmake[1]: *** [CMakeFiles/Makefile2:124: CMakeFiles/cbag.dir/all] Error 2 As mentioned at this link this error crops up if the CMakeLists.txt for a project doesn't correct set all the flags for spdlog and fmt. In this case, spdlog was erroneously omitted from the target_link_libraries section. A simple fix: diff --git a/CMakeLists.txt b/CMakeLists.txt index 6ea5233..e8f1408 100644 --- a/CMakeLists.txt +++ b/CMakeLists.txt @@ -201,8 +201,7 @@ set(SRC_FILES_LIB_CBAG_OA ${CMAKE_CURRENT_SOURCE_DIR}/src/cbag/oa/util.cpp ${CMAKE_CURRENT_SOURCE_DIR}/src/cbag/oa/write.cpp ) - - + if(DEFINED ENV{OA_LINK_DIR}) message(\"OA include directory: \" $ENV{OA_INCLUDE_DIR}) message(\"OA link directory: \" $ENV{OA_LINK_DIR}) @@ -226,6 +225,7 @@ target_link_libraries(cbag oaCommon oaPlugIn PRIVATE + spdlog::spdlog #https://bugzilla.redhat.com/show_bug.cgi?id=1851497 stdc++fs ${Boost_LIBRARIES} Problem #3 (.venv) [kcaisley@asiclab008 cbag]$ cmake --build build Interprocedural optimization disabled -- Found Boost: /usr/lib64/cmake/Boost-1.78.0/BoostConfig.cmake (found version \"1.78.0\") Interprocedural optimization disabled install prefix: /usr/local install rpath: OA include directory: /cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa/22.50p001/include OA link directory: /cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa/22.50p001/lib/linux_rhel50_gcc44x_64/opt -- Found Boost: /usr/lib64/cmake/Boost-1.78.0/BoostConfig.cmake (found version \"1.78.0\") found components: serialization -- Configuring done -- Generating done -- Build files have been written to: /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/build [ 1%] Building CXX object CMakeFiles/cbag.dir/src/cbag/common/box_collection.cpp.o [ 2%] Building CXX object CMakeFiles/cbag.dir/src/cbag/common/transformation_util.cpp.o [ 3%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/main.cpp.o [ 4%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/math.cpp.o [ 5%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/parse_map.cpp.o [ 7%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/read.cpp.o [ 8%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/read_util.cpp.o [ 9%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/write.cpp.o [ 10%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/write_util.cpp.o [ 11%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/blockage.cpp.o [ 12%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/boundary.cpp.o [ 14%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/cellview.cpp.o In file included from /usr/include/boost/geometry/index/rtree.hpp:30, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/cbag_polygon/include/cbag/polygon/geo_index.h:54, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/layout/cellview_fwd.h:59, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/layout/cellview.h:52, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/layout/cellview.cpp:55: /usr/include/boost/geometry/strategies/relate/services.hpp: In instantiation of \u2018struct boost::geometry::strategies::relate::services::default_strategy<boost::geometry::model::box<boost::geometry::model::point<int, 2, boost::geometry::cs::cartesian> >, cbag::polygon::rectangle_data<int>, boost::geometry::cartesian_tag, boost::geometry::cartesian_tag>\u2019: /usr/include/boost/geometry/algorithms/detail/disjoint/interface.hpp:92:21: required from \u2018static bool boost::geometry::resolve_strategy::disjoint<boost::geometry::default_strategy, false>::apply(const Geometry1&, const Geometry2&, boost::geometry::default_strategy) [with Geometry1 = boost::geometry::model::box<boost::geometry::model::point<int, 2, boost::geometry::cs::cartesian> >; Geometry2 = cbag::polygon::rectangle_data<int>]\u2019 /usr/include/boost/geometry/algorithms/detail/disjoint/interface.hpp:129:21: required from \u2018static bool boost::geometry::resolve_dynamic::disjoint<Geometry1, Geometry2, IsDynamic, IsCollection>::apply(const Geometry1&, const Geometry2&, const Strategy&) [with Strategy = boost::geometry::default_strategy; Geometry1 = boost::geometry::model::box<boost::geometry::model::point<int, 2, boost::geometry::cs::cartesian> >; Geometry2 = cbag::polygon::rectangle_data<int>; bool IsDynamic = false; bool IsCollection = false]\u2019 /usr/include/boost/geometry/algorithms/detail/disjoint/interface.hpp:231:21: required from \u2018bool boost::geometry::disjoint(const Geometry1&, const Geometry2&) [with Geometry1 = model::box<model::point<int, 2, cs::cartesian> >; Geometry2 = cbag::polygon::rectangle_data<int>]\u2019 /usr/include/boost/geometry/algorithms/detail/intersects/interface.hpp:108:32: required from \u2018bool boost::geometry::intersects(const Geometry1&, const Geometry2&) [with Geometry1 = model::box<model::point<int, 2, cs::cartesian> >; Geometry2 = cbag::polygon::rectangle_data<int>]\u2019 /usr/include/boost/geometry/index/detail/predicates.hpp:218:36: required from \u2018static bool boost::geometry::index::detail::spatial_predicate_intersects<G1, G2, Tag1, Tag2>::apply(const G1&, const G2&, const S&) [with S = boost::geometry::default_strategy; G1 = boost::geometry::model::box<boost::geometry::model::point<int, 2, boost::geometry::cs::cartesian> >; G2 = cbag::polygon::rectangle_data<int>; Tag1 = boost::geometry::box_tag; Tag2 = boost::geometry::box_tag]\u2019 /usr/include/boost/geometry/index/detail/predicates.hpp:243:59: [ skipping 5 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ] /usr/include/boost/geometry/index/detail/rtree/visitors/spatial_query.hpp:87:21: required from \u2018boost::geometry::index::detail::rtree::visitors::spatial_query<MembersHolder, Predicates, OutIter>::size_type boost::geometry::index::detail::rtree::visitors::spatial_query<MembersHolder, Predicates, OutIter>::apply(const MembersHolder&) [with MembersHolder = boost::geometry::index::rtree<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int>, boost::geometry::index::quadratic<32, 16>, boost::geometry::index::indexable<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >, boost::geometry::index::equal_to<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >, boost::container::new_allocator<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> > >::members_holder; Predicates = boost::geometry::index::detail::predicates::spatial_predicate<cbag::polygon::rectangle_data<int>, boost::geometry::index::detail::predicates::intersects_tag, false>; OutIter = cbag::util::lambda_output_iterator<cbag::polygon::index::geo_index<int>::get_intersect<cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> > >(cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> >, const box_type&, coordinate_type, coordinate_type, bool, const cbag::polygon::transformation<int>&) const::<lambda(const cbag::polygon::index::geo_index<int>::tree_value_type&)> >; size_type = long unsigned int]\u2019 /usr/include/boost/geometry/index/rtree.hpp:1861:27: required from \u2018boost::geometry::index::rtree<Value, Options, IndexableGetter, EqualTo, Allocator>::size_type boost::geometry::index::rtree<Value, Options, IndexableGetter, EqualTo, Allocator>::query_dispatch(const Predicates&, OutIter) const [with Predicates = boost::geometry::index::detail::predicates::spatial_predicate<cbag::polygon::rectangle_data<int>, boost::geometry::index::detail::predicates::intersects_tag, false>; OutIter = cbag::util::lambda_output_iterator<cbag::polygon::index::geo_index<int>::get_intersect<cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> > >(cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> >, const box_type&, coordinate_type, coordinate_type, bool, const cbag::polygon::transformation<int>&) const::<lambda(const cbag::polygon::index::geo_index<int>::tree_value_type&)> >; typename std::enable_if<(boost::geometry::index::detail::predicates_count_distance<Predicates>::value == 0), int>::type <anonymous> = 0; Value = std::pair<cbag::polygon::rectangle_data<int>, long unsigned int>; Parameters = boost::geometry::index::quadratic<32, 16>; IndexableGetter = boost::geometry::index::indexable<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; EqualTo = boost::geometry::index::equal_to<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; Allocator = boost::container::new_allocator<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; size_type = long unsigned int]\u2019 /usr/include/boost/geometry/index/rtree.hpp:1083:30: required from \u2018boost::geometry::index::rtree<Value, Options, IndexableGetter, EqualTo, Allocator>::size_type boost::geometry::index::rtree<Value, Options, IndexableGetter, EqualTo, Allocator>::query(const Predicates&, OutIter) const [with Predicates = boost::geometry::index::detail::predicates::spatial_predicate<cbag::polygon::rectangle_data<int>, boost::geometry::index::detail::predicates::intersects_tag, false>; OutIter = cbag::util::lambda_output_iterator<cbag::polygon::index::geo_index<int>::get_intersect<cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> > >(cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> >, const box_type&, coordinate_type, coordinate_type, bool, const cbag::polygon::transformation<int>&) const::<lambda(const cbag::polygon::index::geo_index<int>::tree_value_type&)> >; Value = std::pair<cbag::polygon::rectangle_data<int>, long unsigned int>; Parameters = boost::geometry::index::quadratic<32, 16>; IndexableGetter = boost::geometry::index::indexable<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; EqualTo = boost::geometry::index::equal_to<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; Allocator = boost::container::new_allocator<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; size_type = long unsigned int]\u2019 /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/cbag_polygon/include/cbag/polygon/geo_index.h:212:21: required from \u2018void cbag::polygon::index::geo_index<T>::get_intersect(OutIter, const box_type&, coordinate_type, coordinate_type, bool, const cbag::polygon::transformation<T>&) const [with OutIter = cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> >; T = int; box_type = cbag::polygon::rectangle_data<int>; coordinate_type = int]\u2019 /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/cbag_polygon/include/cbag/polygon/geo_index.h:244:22: required from \u2018void cbag::polygon::index::apply_intersect(const geo_index<T>&, Fun, const typename geo_index<T>::box_type&, T, T, bool, const cbag::polygon::transformation<T>&) [with T = int; Fun = cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)>; typename geo_index<T>::box_type = cbag::polygon::rectangle_data<int>]\u2019 /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/layout/cellview.cpp:137:24: required from here /usr/include/boost/geometry/strategies/relate/services.hpp:36:5: error: static assertion failed: Not implemented for this Geometry's coordinate system. 36 | BOOST_GEOMETRY_STATIC_ASSERT_FALSE( | ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ /usr/include/boost/geometry/strategies/relate/services.hpp:36:5: note: \u2018std::integral_constant<bool, false>::value\u2019 evaluates to false In file included from /usr/include/boost/geometry/index/rtree.hpp:34: /usr/include/boost/geometry/algorithms/detail/disjoint/interface.hpp: In instantiation of \u2018static bool boost::geometry::resolve_strategy::disjoint<boost::geometry::default_strategy, false>::apply(const Geometry1&, const Geometry2&, boost::geometry::default_strategy) [with Geometry1 = boost::geometry::model::box<boost::geometry::model::point<int, 2, boost::geometry::cs::cartesian> >; Geometry2 = cbag::polygon::rectangle_data<int>]\u2019: /usr/include/boost/geometry/algorithms/detail/disjoint/interface.hpp:129:21: required from \u2018static bool boost::geometry::resolve_dynamic::disjoint<Geometry1, Geometry2, IsDynamic, IsCollection>::apply(const Geometry1&, const Geometry2&, const Strategy&) [with Strategy = boost::geometry::default_strategy; Geometry1 = boost::geometry::model::box<boost::geometry::model::point<int, 2, boost::geometry::cs::cartesian> >; Geometry2 = cbag::polygon::rectangle_data<int>; bool IsDynamic = false; bool IsCollection = false]\u2019 /usr/include/boost/geometry/algorithms/detail/disjoint/interface.hpp:231:21: required from \u2018bool boost::geometry::disjoint(const Geometry1&, const Geometry2&) [with Geometry1 = model::box<model::point<int, 2, cs::cartesian> >; Geometry2 = cbag::polygon::rectangle_data<int>]\u2019 /usr/include/boost/geometry/algorithms/detail/intersects/interface.hpp:108:32: required from \u2018bool boost::geometry::intersects(const Geometry1&, const Geometry2&) [with Geometry1 = model::box<model::point<int, 2, cs::cartesian> >; Geometry2 = cbag::polygon::rectangle_data<int>]\u2019 /usr/include/boost/geometry/index/detail/predicates.hpp:218:36: required from \u2018static bool boost::geometry::index::detail::spatial_predicate_intersects<G1, G2, Tag1, Tag2>::apply(const G1&, const G2&, const S&) [with S = boost::geometry::default_strategy; G1 = boost::geometry::model::box<boost::geometry::model::point<int, 2, boost::geometry::cs::cartesian> >; G2 = cbag::polygon::rectangle_data<int>; Tag1 = boost::geometry::box_tag; Tag2 = boost::geometry::box_tag]\u2019 /usr/include/boost/geometry/index/detail/predicates.hpp:243:59: required from \u2018static bool boost::geometry::index::detail::spatial_predicate_call<boost::geometry::index::detail::predicates::intersects_tag>::apply(const G1&, const G2&, const S&) [with G1 = boost::geometry::model::box<boost::geometry::model::point<int, 2, boost::geometry::cs::cartesian> >; G2 = cbag::polygon::rectangle_data<int>; S = boost::geometry::default_strategy]\u2019 /usr/include/boost/geometry/index/detail/predicates.hpp:364:73: [ skipping 4 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ] /usr/include/boost/geometry/index/detail/rtree/visitors/spatial_query.hpp:87:21: required from \u2018boost::geometry::index::detail::rtree::visitors::spatial_query<MembersHolder, Predicates, OutIter>::size_type boost::geometry::index::detail::rtree::visitors::spatial_query<MembersHolder, Predicates, OutIter>::apply(const MembersHolder&) [with MembersHolder = boost::geometry::index::rtree<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int>, boost::geometry::index::quadratic<32, 16>, boost::geometry::index::indexable<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >, boost::geometry::index::equal_to<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >, boost::container::new_allocator<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> > >::members_holder; Predicates = boost::geometry::index::detail::predicates::spatial_predicate<cbag::polygon::rectangle_data<int>, boost::geometry::index::detail::predicates::intersects_tag, false>; OutIter = cbag::util::lambda_output_iterator<cbag::polygon::index::geo_index<int>::get_intersect<cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> > >(cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> >, const box_type&, coordinate_type, coordinate_type, bool, const cbag::polygon::transformation<int>&) const::<lambda(const cbag::polygon::index::geo_index<int>::tree_value_type&)> >; size_type = long unsigned int]\u2019 /usr/include/boost/geometry/index/rtree.hpp:1861:27: required from \u2018boost::geometry::index::rtree<Value, Options, IndexableGetter, EqualTo, Allocator>::size_type boost::geometry::index::rtree<Value, Options, IndexableGetter, EqualTo, Allocator>::query_dispatch(const Predicates&, OutIter) const [with Predicates = boost::geometry::index::detail::predicates::spatial_predicate<cbag::polygon::rectangle_data<int>, boost::geometry::index::detail::predicates::intersects_tag, false>; OutIter = cbag::util::lambda_output_iterator<cbag::polygon::index::geo_index<int>::get_intersect<cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> > >(cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> >, const box_type&, coordinate_type, coordinate_type, bool, const cbag::polygon::transformation<int>&) const::<lambda(const cbag::polygon::index::geo_index<int>::tree_value_type&)> >; typename std::enable_if<(boost::geometry::index::detail::predicates_count_distance<Predicates>::value == 0), int>::type <anonymous> = 0; Value = std::pair<cbag::polygon::rectangle_data<int>, long unsigned int>; Parameters = boost::geometry::index::quadratic<32, 16>; IndexableGetter = boost::geometry::index::indexable<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; EqualTo = boost::geometry::index::equal_to<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; Allocator = boost::container::new_allocator<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; size_type = long unsigned int]\u2019 /usr/include/boost/geometry/index/rtree.hpp:1083:30: required from \u2018boost::geometry::index::rtree<Value, Options, IndexableGetter, EqualTo, Allocator>::size_type boost::geometry::index::rtree<Value, Options, IndexableGetter, EqualTo, Allocator>::query(const Predicates&, OutIter) const [with Predicates = boost::geometry::index::detail::predicates::spatial_predicate<cbag::polygon::rectangle_data<int>, boost::geometry::index::detail::predicates::intersects_tag, false>; OutIter = cbag::util::lambda_output_iterator<cbag::polygon::index::geo_index<int>::get_intersect<cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> > >(cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> >, const box_type&, coordinate_type, coordinate_type, bool, const cbag::polygon::transformation<int>&) const::<lambda(const cbag::polygon::index::geo_index<int>::tree_value_type&)> >; Value = std::pair<cbag::polygon::rectangle_data<int>, long unsigned int>; Parameters = boost::geometry::index::quadratic<32, 16>; IndexableGetter = boost::geometry::index::indexable<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; EqualTo = boost::geometry::index::equal_to<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; Allocator = boost::container::new_allocator<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; size_type = long unsigned int]\u2019 /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/cbag_polygon/include/cbag/polygon/geo_index.h:212:21: required from \u2018void cbag::polygon::index::geo_index<T>::get_intersect(OutIter, const box_type&, coordinate_type, coordinate_type, bool, const cbag::polygon::transformation<T>&) const [with OutIter = cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> >; T = int; box_type = cbag::polygon::rectangle_data<int>; coordinate_type = int]\u2019 /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/cbag_polygon/include/cbag/polygon/geo_index.h:244:22: required from \u2018void cbag::polygon::index::apply_intersect(const geo_index<T>&, Fun, const typename geo_index<T>::box_type&, T, T, bool, const cbag::polygon::transformation<T>&) [with T = int; Fun = cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)>; typename geo_index<T>::box_type = cbag::polygon::rectangle_data<int>]\u2019 /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/layout/cellview.cpp:137:24: required from here /usr/include/boost/geometry/algorithms/detail/disjoint/interface.hpp:92:21: error: no type named \u2018type\u2019 in \u2018struct boost::geometry::strategies::relate::services::default_strategy<boost::geometry::model::box<boost::geometry::model::point<int, 2, boost::geometry::cs::cartesian> >, cbag::polygon::rectangle_data<int>, boost::geometry::cartesian_tag, boost::geometry::cartesian_tag>\u2019 92 | >::type strategy_type; | ^~~~~~~~~~~~~ compilation terminated due to -fmax-errors=2. gmake[2]: *** [CMakeFiles/cbag.dir/build.make:230: CMakeFiles/cbag.dir/src/cbag/layout/cellview.cpp.o] Error 1 gmake[1]: *** [CMakeFiles/Makefile2:124: CMakeFiles/cbag.dir/all] Error 2 Solution was to include , as discussed in this issues [kcaisley@asiclab008 cbag]$ git diff include/cbag/common/typedefs.h diff --git a/include/cbag/common/typedefs.h b/include/cbag/common/typedefs.h index 7ce06b1..86dbba1 100644 --- a/include/cbag/common/typedefs.h +++ b/include/cbag/common/typedefs.h @@ -49,6 +49,9 @@ limitations under the License. #include <cstdint> #include <tuple> +#include <limits> //https://stackoverflow.com/questions/71296302/numeric-limits-is-not-a-member-of-std +#include <boost/geometry.hpp> //https://github.com/pgRouting/pgrouting/issues/1825 NOTE: As you can see above, this is the second line we've had to add to this file. Problem #4 (.venv) [kcaisley@asiclab008 cbag]$ cmake --build build [ 1%] Building CXX object CMakeFiles/cbag.dir/src/cbag/common/box_collection.cpp.o [ 2%] Building CXX object CMakeFiles/cbag.dir/src/cbag/common/transformation_util.cpp.o [ 3%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/main.cpp.o [ 4%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/parse_map.cpp.o [ 5%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/read.cpp.o [ 7%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/read_util.cpp.o [ 8%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/write.cpp.o [ 9%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/write_util.cpp.o [ 10%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/blockage.cpp.o [ 11%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/boundary.cpp.o [ 12%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/cellview.cpp.o [ 14%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/cellview_poly.cpp.o [ 15%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/cellview_util.cpp.o [ 16%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/grid_object.cpp.o [ 17%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/instance.cpp.o [ 18%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/label.cpp.o [ 20%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/len_info.cpp.o [ 21%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/lp_lookup.cpp.o In file included from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/layout/lp_lookup.cpp:51: /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/layout/lp_lookup.h:85:10: error: \u2018optional\u2019 in namespace \u2018std\u2019 does not name a template type 85 | std::optional<lay_t> get_layer_id(const std::string &layer) const; | ^~~~~~~~ /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/layout/lp_lookup.h:53:1: note: \u2018std::optional\u2019 is defined in header \u2018<optional>\u2019; did you forget to \u2018#include <optional>\u2019? 52 | #include <cbag/common/typedefs.h> +++ |+#include <optional> 53 | /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/layout/lp_lookup.h:87:10: error: \u2018optional\u2019 in namespace \u2018std\u2019 does not name a template type 87 | std::optional<purp_t> get_purpose_id(const std::string &purpose) const; | ^~~~~~~~ /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/layout/lp_lookup.h:87:5: note: \u2018std::optional\u2019 is defined in header \u2018<optional>\u2019; did you forget to \u2018#include <optional>\u2019? 87 | std::optional<purp_t> get_purpose_id(const std::string &purpose) const; | ^~~ compilation terminated due to -fmax-errors=2. gmake[2]: *** [CMakeFiles/cbag.dir/build.make:328: CMakeFiles/cbag.dir/src/cbag/layout/lp_lookup.cpp.o] Error 1 gmake[1]: *** [CMakeFiles/Makefile2:124: CMakeFiles/cbag.dir/all] Error 2 The solution is described right there in the message, we forgot std::optional is defined in header <optional> ; yep we forget to #include <optional> in cbag/include/cbag/layout/lp_lookup.h . (.venv) [kcaisley@asiclab008 cbag]$ git diff include/cbag/layout/lp_lookup.h diff --git a/include/cbag/layout/lp_lookup.h b/include/cbag/layout/lp_lookup.h index 3f21bb9..cd6771d 100644 --- a/include/cbag/layout/lp_lookup.h +++ b/include/cbag/layout/lp_lookup.h @@ -48,7 +48,7 @@ limitations under the License. #define CBAG_COMMON_LP_LOOKUP_H #include <unordered_map> - +#include <optional> #include <cbag/common/typedefs.h> Problem #5 (.venv) [kcaisley@asiclab008 cbag]$ cmake --build build [ 1%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/main.cpp.o [ 2%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/parse_map.cpp.o [ 3%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/read.cpp.o [ 4%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/read_util.cpp.o [ 5%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/write.cpp.o [ 7%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/cellview.cpp.o [ 8%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/cellview_poly.cpp.o [ 9%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/cellview_util.cpp.o [ 10%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/grid_object.cpp.o [ 11%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/instance.cpp.o [ 12%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/lp_lookup.cpp.o [ 14%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/path.cpp.o [ 15%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/path_util.cpp.o In file included from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/layout/path_util.cpp:47: /usr/include/fmt/core.h: In instantiation of \u2018constexpr fmt::v9::detail::value<Context> fmt::v9::detail::make_value(T&&) [with Context = fmt::v9::basic_format_context<fmt::v9::appender, char>; T = cbag::layout::vector45&]\u2019: /usr/include/fmt/core.h:1777:29: required from \u2018constexpr fmt::v9::detail::value<Context> fmt::v9::detail::make_arg(T&&) [with bool IS_PACKED = true; Context = fmt::v9::basic_format_context<fmt::v9::appender, char>; type <anonymous> = fmt::v9::detail::type::custom_type; T = cbag::layout::vector45&; typename std::enable_if<IS_PACKED, int>::type <anonymous> = 0]\u2019 /usr/include/fmt/core.h:1901:77: required from \u2018constexpr fmt::v9::format_arg_store<Context, Args>::format_arg_store(T&& ...) [with T = {cbag::layout::vector45&}; Context = fmt::v9::basic_format_context<fmt::v9::appender, char>; Args = {cbag::layout::vector45}]\u2019 /usr/include/fmt/core.h:1918:31: required from \u2018constexpr fmt::v9::format_arg_store<Context, typename std::remove_cv<typename std::remove_reference<Args>::type>::type ...> fmt::v9::make_format_args(Args&& ...) [with Context = basic_format_context<appender, char>; Args = {cbag::layout::vector45&}]\u2019 /usr/include/fmt/core.h:3206:44: required from \u2018std::string fmt::v9::format(format_string<T ...>, T&& ...) [with T = {cbag::layout::vector45&}; std::string = std::__cxx11::basic_string<char>; format_string<T ...> = basic_format_string<char, cbag::layout::vector45&>]\u2019 /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/layout/path_util.cpp:123:48: required from here /usr/include/fmt/core.h:1757:7: error: static assertion failed: Cannot format an argument. To make type T formattable provide a formatter<T> specialization: https://fmt.dev/latest/api.html#udt 1757 | formattable, | ^~~~~~~~~~~ /usr/include/fmt/core.h:1757:7: note: \u2018formattable\u2019 evaluates to false gmake[2]: *** [CMakeFiles/cbag.dir/build.make:356: CMakeFiles/cbag.dir/src/cbag/layout/path_util.cpp.o] Error 1 gmake[1]: *** [CMakeFiles/Makefile2:124: CMakeFiles/cbag.dir/all] Error 2 gmake: *** [Makefile:136: all] Error 2 I think a similar problem is seen here: https://github.com/fmtlib/fmt/issues/3034 Seems I need to made a change to the function call, to meet this latest issue: https://fmt.dev/latest/api.html#udt I simply commented out the line: throw std::invalid_argument(fmt::format(\"path segment vector {} not valid\", p_norm)); And it proceeded until 52% compilation and finally an unused fix..... diff --git a/src/cbag/layout/path_util.cpp b/src/cbag/layout/path_util.cpp index b9a0802..3c62df2 100644 --- a/src/cbag/layout/path_util.cpp +++ b/src/cbag/layout/path_util.cpp @@ -46,6 +46,7 @@ limitations under the License. #include <fmt/core.h> #include <fmt/ostream.h> +//#include <fmt/format.h> // trying to add this code #include <cbag/layout/path_util.h> Problem #6 (.venv) [kcaisley@asiclab008 cbag]$ cmake --build build [ 1%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/path_util.cpp.o [ 2%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/pin.cpp.o [ 3%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/routing_grid.cpp.o [ 4%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/routing_grid_util.cpp.o [ 5%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/tech.cpp.o [ 7%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/tech_util.cpp.o [ 8%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/track_coloring.cpp.o [ 9%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/track_info.cpp.o [ 10%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/track_info_util.cpp.o [ 11%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/vector45.cpp.o [ 12%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/via.cpp.o [ 14%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/via_info.cpp.o [ 15%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/via_lookup.cpp.o [ 16%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/via_param.cpp.o [ 17%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/via_param_util.cpp.o [ 18%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/via_util.cpp.o [ 20%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/via_wrapper.cpp.o [ 21%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/wire_width.cpp.o [ 22%] Building CXX object CMakeFiles/cbag.dir/src/cbag/logging/logging.cpp.o [ 23%] Building CXX object CMakeFiles/cbag.dir/src/cbag/netlist/cdl.cpp.o [ 24%] Building CXX object CMakeFiles/cbag.dir/src/cbag/netlist/core.cpp.o [ 25%] Building CXX object CMakeFiles/cbag.dir/src/cbag/netlist/lstream.cpp.o [ 27%] Building CXX object CMakeFiles/cbag.dir/src/cbag/netlist/netlist.cpp.o [ 28%] Building CXX object CMakeFiles/cbag.dir/src/cbag/netlist/nstream_output.cpp.o [ 29%] Building CXX object CMakeFiles/cbag.dir/src/cbag/netlist/spectre.cpp.o [ 30%] Building CXX object CMakeFiles/cbag.dir/src/cbag/netlist/verilog.cpp.o [ 31%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/arc.cpp.o [ 32%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/cellview.cpp.o [ 34%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/cellview_info.cpp.o [ 35%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/donut.cpp.o [ 36%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/ellipse.cpp.o [ 37%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/eval_text.cpp.o [ 38%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/instance.cpp.o [ 40%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/line.cpp.o [ 41%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/path.cpp.o [ 42%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/pin_figure.cpp.o [ 43%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/pin_object.cpp.o [ 44%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/polygon.cpp.o [ 45%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/rectangle.cpp.o [ 47%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/shape_base.cpp.o [ 48%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/term_attr.cpp.o [ 49%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/text_base.cpp.o [ 50%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/text_t.cpp.o [ 51%] Building CXX object CMakeFiles/cbag.dir/src/cbag/spirit/ast.cpp.o [ 52%] Building CXX object CMakeFiles/cbag.dir/src/cbag/spirit/name.cpp.o In file included from /usr/include/boost/spirit/home/x3/operator/sequence.hpp:12, from /usr/include/boost/spirit/home/x3/operator.hpp:10, from /usr/include/boost/spirit/home/x3.hpp:19, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/spirit/config.h:50, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/spirit/name.cpp:47: /usr/include/boost/spirit/home/x3/operator/detail/sequence.hpp: In instantiation of \u2018struct boost::spirit::x3::detail::partition_attribute<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:133&)> >, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::sequence<boost::spirit::x3::uint_parser<unsigned int>, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > > > > > > >, cbag::spirit::ast::range, boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>, void>\u2019: /usr/include/boost/spirit/home/x3/operator/detail/sequence.hpp:243:15: required from \u2018bool boost::spirit::x3::detail::parse_sequence(const Parser&, Iterator&, const Iterator&, const Context&, RContext&, Attribute&, AttributeCategory) [with Parser = boost::spirit::x3::sequence<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:133&)> >, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::sequence<boost::spirit::x3::uint_parser<unsigned int>, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > > > > > > > >; Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; Context = boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>; RContext = cbag::spirit::ast::range; Attribute = cbag::spirit::ast::range; AttributeCategory = boost::spirit::x3::traits::tuple_attribute]\u2019 /usr/include/boost/spirit/home/x3/operator/sequence.hpp:46:42: required from \u2018bool boost::spirit::x3::sequence<Left, Right>::parse(Iterator&, const Iterator&, const Context&, RContext&, Attribute&) const [with Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; Context = boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>; RContext = cbag::spirit::ast::range; Attribute = cbag::spirit::ast::range; Left = boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:133&)> >; Right = boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::sequence<boost::spirit::x3::uint_parser<unsigned int>, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > > > > > > >]\u2019 /usr/include/boost/spirit/home/x3/directive/expect.hpp:54:41: required from \u2018bool boost::spirit::x3::expect_directive<Subject>::parse(Iterator&, const Iterator&, const Context&, RContext&, Attribute&) const [with Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; Context = boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>; RContext = cbag::spirit::ast::range; Attribute = cbag::spirit::ast::range; Subject = boost::spirit::x3::sequence<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:133&)> >, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::sequence<boost::spirit::x3::uint_parser<unsigned int>, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > > > > > > > >]\u2019 /usr/include/boost/spirit/home/x3/operator/detail/sequence.hpp:253:34: required from \u2018bool boost::spirit::x3::detail::parse_sequence(const Parser&, Iterator&, const Iterator&, const Context&, RContext&, Attribute&, AttributeCategory) [with Parser = boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::sequence<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:133&)> >, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::sequence<boost::spirit::x3::uint_parser<unsigned int>, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > > > > > > > > > >; Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; Context = boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>; RContext = cbag::spirit::ast::range; Attribute = cbag::spirit::ast::range; AttributeCategory = boost::spirit::x3::traits::tuple_attribute]\u2019 /usr/include/boost/spirit/home/x3/operator/sequence.hpp:46:42: required from \u2018bool boost::spirit::x3::sequence<Left, Right>::parse(Iterator&, const Iterator&, const Context&, RContext&, Attribute&) const [with Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; Context = boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>; RContext = cbag::spirit::ast::range; Attribute = cbag::spirit::ast::range; Left = boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>; Right = boost::spirit::x3::expect_directive<boost::spirit::x3::sequence<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:133&)> >, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::sequence<boost::spirit::x3::uint_parser<unsigned int>, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > > > > > > > > >]\u2019 /usr/include/boost/spirit/home/x3/operator/detail/sequence.hpp:252:30: [ skipping 44 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ] /usr/include/boost/spirit/home/x3/nonterminal/detail/rule.hpp:240:42: required from \u2018static bool boost::spirit::x3::detail::rule_parser<Attribute, ID, skip_definition_injection>::parse_rhs_main(const RHS&, Iterator&, const Iterator&, const Context&, RContext&, ActualAttribute&, mpl_::true_) [with RHS = boost::spirit::x3::rule_definition<cbag::spirit::parser::name_rep_class, boost::spirit::x3::alternative<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::literal_string<const char*, boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > >, boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::alternative<boost::spirit::x3::sequence<boost::spirit::x3::rule<cbag::spirit::parser::name_unit_class, cbag::spirit::ast::name_unit, true>, boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> >, boost::spirit::x3::sequence<boost::spirit::x3::rule<cbag::spirit::parser::name_class, cbag::spirit::ast::name, true>, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > > > > > >, boost::spirit::x3::sequence<boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::literal_string<const char*, boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::rule<cbag::spirit::parser::name_unit_class, cbag::spirit::ast::name_unit, true> > > >, cbag::spirit::ast::name_rep, true, true>; Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; Context = boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>; RContext = cbag::spirit::ast::name_rep; ActualAttribute = cbag::spirit::ast::name_rep; Attribute = cbag::spirit::ast::name_rep; ID = cbag::spirit::parser::name_rep_class; bool skip_definition_injection = true; mpl_::true_ = mpl_::bool_<true>]\u2019 /usr/include/boost/spirit/home/x3/nonterminal/detail/rule.hpp:267:34: required from \u2018static bool boost::spirit::x3::detail::rule_parser<Attribute, ID, skip_definition_injection>::parse_rhs_main(const RHS&, Iterator&, const Iterator&, const Context&, RContext&, ActualAttribute&) [with RHS = boost::spirit::x3::rule_definition<cbag::spirit::parser::name_rep_class, boost::spirit::x3::alternative<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::literal_string<const char*, boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > >, boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::alternative<boost::spirit::x3::sequence<boost::spirit::x3::rule<cbag::spirit::parser::name_unit_class, cbag::spirit::ast::name_unit, true>, boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> >, boost::spirit::x3::sequence<boost::spirit::x3::rule<cbag::spirit::parser::name_class, cbag::spirit::ast::name, true>, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > > > > > >, boost::spirit::x3::sequence<boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::literal_string<const char*, boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::rule<cbag::spirit::parser::name_unit_class, cbag::spirit::ast::name_unit, true> > > >, cbag::spirit::ast::name_rep, true, true>; Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; Context = boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>; RContext = cbag::spirit::ast::name_rep; ActualAttribute = cbag::spirit::ast::name_rep; Attribute = cbag::spirit::ast::name_rep; ID = cbag::spirit::parser::name_rep_class; bool skip_definition_injection = true]\u2019 /usr/include/boost/spirit/home/x3/nonterminal/detail/rule.hpp:281:34: required from \u2018static bool boost::spirit::x3::detail::rule_parser<Attribute, ID, skip_definition_injection>::parse_rhs(const RHS&, Iterator&, const Iterator&, const Context&, RContext&, ActualAttribute&, mpl_::false_) [with RHS = boost::spirit::x3::rule_definition<cbag::spirit::parser::name_rep_class, boost::spirit::x3::alternative<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::literal_string<const char*, boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > >, boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::alternative<boost::spirit::x3::sequence<boost::spirit::x3::rule<cbag::spirit::parser::name_unit_class, cbag::spirit::ast::name_unit, true>, boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> >, boost::spirit::x3::sequence<boost::spirit::x3::rule<cbag::spirit::parser::name_class, cbag::spirit::ast::name, true>, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > > > > > >, boost::spirit::x3::sequence<boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::literal_string<const char*, boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::rule<cbag::spirit::parser::name_unit_class, cbag::spirit::ast::name_unit, true> > > >, cbag::spirit::ast::name_rep, true, true>; Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; Context = boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>; RContext = cbag::spirit::ast::name_rep; ActualAttribute = cbag::spirit::ast::name_rep; Attribute = cbag::spirit::ast::name_rep; ID = cbag::spirit::parser::name_rep_class; bool skip_definition_injection = true; mpl_::false_ = mpl_::bool_<false>]\u2019 /usr/include/boost/spirit/home/x3/nonterminal/detail/rule.hpp:330:37: required from \u2018static bool boost::spirit::x3::detail::rule_parser<Attribute, ID, skip_definition_injection>::call_rule_definition(const RHS&, const char*, Iterator&, const Iterator&, const Context&, ActualAttribute&, ExplicitAttrPropagation) [with RHS = boost::spirit::x3::rule_definition<cbag::spirit::parser::name_rep_class, boost::spirit::x3::alternative<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::literal_string<const char*, boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > >, boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::alternative<boost::spirit::x3::sequence<boost::spirit::x3::rule<cbag::spirit::parser::name_unit_class, cbag::spirit::ast::name_unit, true>, boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> >, boost::spirit::x3::sequence<boost::spirit::x3::rule<cbag::spirit::parser::name_class, cbag::spirit::ast::name, true>, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > > > > > >, boost::spirit::x3::sequence<boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::literal_string<const char*, boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::rule<cbag::spirit::parser::name_unit_class, cbag::spirit::ast::name_unit, true> > > >, cbag::spirit::ast::name_rep, true, true>; Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; Context = boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>; ActualAttribute = cbag::spirit::ast::name_rep; ExplicitAttrPropagation = mpl_::bool_<true>; Attribute = cbag::spirit::ast::name_rep; ID = cbag::spirit::parser::name_rep_class; bool skip_definition_injection = true]\u2019 /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/spirit/name_def.h:112:1: required from \u2018bool cbag::spirit::parser::parse_rule(boost::spirit::x3::detail::rule_id<name_rep_class>, Iterator&, const Iterator&, const Context&, boost::spirit::x3::rule<name_rep_class, cbag::spirit::ast::name_rep, true>::attribute_type&) [with Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; Context = boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>; boost::spirit::x3::rule<name_rep_class, cbag::spirit::ast::name_rep, true>::attribute_type = cbag::spirit::ast::name_rep]\u2019 /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/spirit/name.cpp:53:1: required from here /usr/include/boost/spirit/home/x3/operator/detail/sequence.hpp:144:25: error: static assertion failed: Size of the passed attribute is bigger than expected. 144 | actual_size <= expected_size | ~~~~~~~~~~~~^~~~~~~~~~~~~~~~ /usr/include/boost/spirit/home/x3/operator/detail/sequence.hpp:144:25: note: the comparison reduces to \u2018(3 <= 2)\u2019 gmake[2]: *** [CMakeFiles/cbag.dir/build.make:972: CMakeFiles/cbag.dir/src/cbag/spirit/name.cpp.o] Error 1 gmake[1]: *** [CMakeFiles/Makefile2:124: CMakeFiles/cbag.dir/all] Error 2 How I bulit containers to test the other versions of EL: Container Creation sudo dnf install apptainer apptainer build --sandbox cbag_centos7.sif docker://centos:7 apptainer shell --fakeroot --writable cbag_centos7.sif/ For Centos7: yum -y update && yum clean all yum install centos-release-scl yum install devtoolset-8 yum install epel-release yum install which yaml-cpp yaml-cpp-devel fmt fmt-devel spdlog spdlog-devel cmake3 //note cmake3 source /opt/rh/devtoolset-8/enable cd to folder and export CC and CXX tried to build, but didn't work as spdlog wasn't new enough to include .cmake files For RockyLinux 8.7: dnf -y update && dnf clean all dnf install epel-release dnf install which yaml-cpp yaml-cpp-devel fmt fmt-devel spdlog spdlog-devel cmake gcc gcc-c++ boost boost-devel starts compiling after stting static boost to OFF, but then fails at compiling with boost... For RockyLinux 9.1: dnf -y update && dnf clean all dnf -y install epel-release dnf -y install which yaml-cpp yaml-cpp-devel fmt fmt-devel spdlog spdlog-devel cmake gcc gcc-c++ boost boost-devel same errors as fedora 37 Email to Ayan: If you have a moment, would it be possible for you to check what versions of gcc and CMake are used to compile your copy of the CBAG submodule, and what versions of the Boost, fmt, spdlog, and yaml-cpp libraries it is built against? I have tried the following combinations of packages versions across various distributions: Distro gcc/g++ CMake Boost fmt spdlog yaml-cpp CentOS 7.9 8.3.1 3.17.0 1.53 6.2.1 0.10.0 0.5.1 (devtoolset-8) RHEL 8.7 8.5.0 3.20.2 1.66 6.3.1 1.5.0 0.6.2 RHEL 9.1 11.3.1 3.20.2 1.75 8.1.1 1.10.0 0.6.3 Fedora 37 12.2.1 3.25.2 1.78 9.1.0 1.10.0 0.6.3 Fedora 37 12.2.1 3.25.2 1.81 9.1.0 1.11.0 0.6.3 In each case I have linked my copies of the OA binaies, have worked through successfully generating make/build files with CMake, and compilation begins. But in each case there are numerous compilation errors, primarily in the form of static assertion failures originating from the fmt and Boost libraries. I've made the most progress in the latter Fedora 37 configuration by editing the CBAG source to be compatible with fmt >9.0, but am still stuck on a couple of the more cryptic errors. fmt spdlog yaml-cpp 8.1.1 1.10.0 0.6.3 export OA_SRC_ROOT=/cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa/22.50p001 export OA_LINK_DIR=${OA_SRC_ROOT}/lib/linux_rhel50_gcc44x_64/opt export OA_INCLUDE_DIR=${OA_SRC_ROOT}/include I faced one last bug, which was simple to fix like Problem #1 above: [ 87%] Building CXX object CMakeFiles/cbag.dir/src/cbag/oa/read_lib.cpp.o In file included from /faust/user/kcaisley/packages/cbag/include/cbag/util/sorted_map.h:52, from /faust/user/kcaisley/packages/cbag/include/cbag/schematic/cellview_fwd.h:53, from /faust/user/kcaisley/packages/cbag/include/cbag/schematic/cellview.h:50, from /faust/user/kcaisley/packages/cbag/src/cbag/oa/read_lib.cpp:47: /faust/user/kcaisley/packages/cbag/include/cbag/util/sorted_vector.h: In member function \u2018const cbag::util::sorted_vector ::value_type& cbag::util::sorted_vector ::at_front() const\u2019: /faust/user/kcaisley/packages/cbag/include/cbag/util/sorted_vector.h:107:24: error: \u2018out_of_range\u2019 is not a member of \u2018std\u2019 107 | throw std::out_of_range(\"Cannot get front of empty vector.\"); | ^~~~~~~~~~~~ /faust/user/kcaisley/packages/cbag/include/cbag/util/sorted_vector.h: In member function \u2018const cbag::util::sorted_vector ::value_type& cbag::util::sorted_vector ::at_back() const\u2019: /faust/user/kcaisley/packages/cbag/include/cbag/util/sorted_vector.h:112:24: error: \u2018out_of_range\u2019 is not a member of \u2018std\u2019 112 | throw std::out_of_range(\"Cannot get back of empty vector.\"); | ^~~~~~~~~~~~ compilation terminated due to -fmax-errors=2. gmake[2]: *** [CMakeFiles/cbag.dir/build.make:1210: CMakeFiles/cbag.dir/src/cbag/oa/read_lib.cpp.o] Error 1 gmake[1]: *** [CMakeFiles/Makefile2:124: CMakeFiles/cbag.dir/all] Error 2 So I think I'll just: To use the std::out_of_range function in C++ code, you need to include the header file in your code. #include <stdexcept> in include/cbag/util/sorted_vector.h cmake -H. -B_build -DHUNTER_STATUS_DEBUG=ON -DCMAKE_BUILD_TYPE=Release cmake --build _build Make sure, when installing from python top level build.sh, you need to have 'sudo dnf install python3.10 python3.10-devel` 22 March I'd like to understand a number of concepts related to my Python/Pybind/CMake/C++ environment: Duck typing generics in C++ but not Python, due to it's typing in C++ done with templates avoids the need for function overloading other C++ features that were added later: garbage collector shared pointer metaprogramming detailing Searching for Pybind references: [kcaisley@asiclab008 pybag]$ grep -r --exclude-dir={pybind11, build} \"PYBIND11 \" src/pybag/core.cpp:PYBIND11_MODULE(core, m) { src/pybag/tech.cpp: PYBIND11_OVERLOAD_PURE(cbag::em_specs_t, cbag::layout::tech, get_metal_em_specs, layer, src/pybag/tech.cpp: PYBIND11_OVERLOAD_PURE(cbag::em_specs_t, cbag::layout::tech, get_via_em_specs, layer_dir, src/pybag/enum_conv.h: PYBIND11_OBJECT_DEFAULT(PyOrient2D, obj_base, true_check); src/pybag/enum_conv.h: PYBIND11_OBJECT_DEFAULT(PyOrient, obj_base, true_check); src/pybag/enum_conv.h: PYBIND11_OBJECT_DEFAULT(PyLogLevel, obj_base, true_check); src/pybag/enum_conv.h: PYBIND11_OBJECT_DEFAULT(PySigType, obj_base, true_check); src/pybag/enum_conv.h: PYBIND11_OBJECT_DEFAULT(PyDesignOutput, obj_base, true_check); pybind11_generics/src/main.cpp:PYBIND11_MODULE(pyg_test, m) { pybind11_generics/src/test_list.cpp: PYBIND11_OVERLOAD_PURE(std::string, / Return type / pybind11_generics/include/pybind11_generics/dict.h:#ifndef PYBIND11_GENERICS_DICT_H pybind11_generics/include/pybind11_generics/dict.h:#define PYBIND11_GENERICS_DICT_H pybind11_generics/include/pybind11_generics/tuple.h:#ifndef PYBIND11_GENERICS_TUPLE_H pybind11_generics/include/pybind11_generics/tuple.h:#define PYBIND11_GENERICS_TUPLE_H pybind11_generics/include/pybind11_generics/list.h:#ifndef PYBIND11_GENERICS_LIST_H pybind11_generics/include/pybind11_generics/list.h:#define PYBIND11_GENERICS_LIST_H pybind11_generics/include/pybind11_generics/any.h:#ifndef PYBIND11_GENERICS_ANY_H pybind11_generics/include/pybind11_generics/any.h:#define PYBIND11_GENERICS_ANY_H pybind11_generics/include/pybind11_generics/any.h: PYBIND11_OBJECT_DEFAULT(Any, any_base, true_check); pybind11_generics/include/pybind11_generics/type_name.h:#ifndef PYBIND11_GENERICS_TYPE_NAME_H pybind11_generics/include/pybind11_generics/type_name.h:#define PYBIND11_GENERICS_TYPE_NAME_H pybind11_generics/include/pybind11_generics/optional.h:#ifndef PYBIND11_GENERICS_OPTIONAL_H pybind11_generics/include/pybind11_generics/optional.h:#define PYBIND11_GENERICS_OPTIONAL_H pybind11_generics/include/pybind11_generics/optional.h: PYBIND11_OBJECT_DEFAULT(Optional, optional_base, optional_check); pybind11_generics/include/pybind11_generics/custom.h:#ifndef PYBIND11_GENERICS_CUSTOM_H pybind11_generics/include/pybind11_generics/custom.h:#define PYBIND11_GENERICS_CUSTOM_H pybind11_generics/include/pybind11_generics/custom.h: PYBIND11_OBJECT_DEFAULT(Custom, custom_base, true_check); pybind11_generics/include/pybind11_generics/cast_input_iterator.h:#ifndef PYBIND11_GENERICS_CAST_INPUT_ITERATOR_H pybind11_generics/include/pybind11_generics/cast_input_iterator.h:#define PYBIND11_GENERICS_CAST_INPUT_ITERATOR_H pybind11_generics/include/pybind11_generics/iterator.h:#ifndef PYBIND11_GENERICS_ITERATOR_H pybind11_generics/include/pybind11_generics/iterator.h:#define PYBIND11_GENERICS_ITERATOR_H pybind11_generics/include/pybind11_generics/iterator.h: PYBIND11_OBJECT_DEFAULT(PyIterator, iterator_base, PyIter_Check); pybind11_generics/include/pybind11_generics/iterable.h:#ifndef PYBIND11_GENERICS_ITERABLE_H pybind11_generics/include/pybind11_generics/iterable.h:#define PYBIND11_GENERICS_ITERABLE_H pybind11_generics/include/pybind11_generics/iterable.h: PYBIND11_OBJECT_DEFAULT(Iterable, iterable_base, iterable_check) pybind11_generics/include/pybind11_generics/union.h:#ifndef PYBIND11_GENERICS_UNION_H pybind11_generics/include/pybind11_generics/union.h:#define PYBIND11_GENERICS_UNION_H pybind11_generics/include/pybind11_generics/cast.h:#ifndef PYBIND11_GENERICS_CAST_H pybind11_generics/include/pybind11_generics/cast.h:#define PYBIND11_GENERICS_CAST_H pybind11_generics/CMakeLists.txt:set(PYBIND11_GENERICS_MASTER_PROJECT OFF) pybind11_generics/CMakeLists.txt: set(PYBIND11_GENERICS_MASTER_PROJECT ON) pybind11_generics/CMakeLists.txt:option(PYBIND11_GENERICS_TEST \"Build pybind11_generics test suite?\" pybind11_generics/CMakeLists.txt: ${PYBIND11_GENERICS_MASTER_PROJECT}) pybind11_generics/CMakeLists.txt:set(PYBIND11_CPP_STANDARD --std=c++1z) pybind11_generics/CMakeLists.txt:if (PYBIND11_GENERICS_TEST) Using CMake with external projects: http://www.saoe.net/blog/using-cmake-with-external-projects/ https://www.scivision.dev/cmake-fetchcontent-vs-external-project/ git hashes: b6f4ceaed0a0a24ccf575fab6c56dd50ccf6f1a9 deps/fmt (8.1.1) 76fb40d95455f249bd70824ecfcae7a8f0930fa3 deps/spdlog (v1.2.1-2055-g76fb40d9) CMake workflow: First, download and bootstrap vcpkg itself; it can be installed anywhere, but generally we recommend using vcpkg as a submodule for CMake projects. $ git clone https://github.com/microsoft/vcpkg $ ./vcpkg/bootstrap-vcpkg.sh To install the libraries for your project, run: $ ./vcpkg/vcpkg install [packages to install] You can also search for the libraries you need with the search subcommand: $ ./vcpkg/vcpkg search [search term] In order to use vcpkg with CMake, you can use the toolchain file: $ cmake -B [build directory] -S . \"-DCMAKE_TOOLCHAIN_FILE=[path to vcpkg]/scripts/buildsystems/vcpkg.cmake\" $ cmake --build [build directory] Visual Studio Code with CMake Tools Adding the following to your workspace settings.json will make CMake Tools automatically use vcpkg for libraries: { \"cmake.configureSettings\": { \"CMAKE_TOOLCHAIN_FILE\": \"[vcpkg root]/scripts/buildsystems/vcpkg.cmake\" } } To enable versioning when running vcpkg ./vcpkg/vcpkg --feature-flags=\"versions\" install I should test this from just the cbag directory: cmake -B_build -S. -DCMAKE_TOOLCHAIN_FILE=/home/kcaisley/packages/vcpkg/scripts/buildsystems/vcpkg.cmake Step 1: Clone the vcpkg repo git clone https://github.com/Microsoft/vcpkg.git Make sure you are in the directory you want the tool installed to before doing this. Step 2: Run the bootstrap script to build vcpkg ./vcpkg/bootstrap-vcpkg.sh Notes about building cbag with vcpkg the errors with not finding compilers and make program only happen when running fatal: path 'versions/baseline.json' exists on disk, but not in '664f8bb619b752430368d0f30a8289b761f5caba' You can use the current commit as a baseline, which is: \"builtin-baseline\": \"c9f906558f9bb12ee9811d6edc98ec9255c6cda5\" Adding the following before project(cbag) set(CMAKE_MAKE_PROGRAM /usr/bin/make) set(CMAKE_C_COMPILER /usr/bin/gcc) set(CMAKE_CXX_COMPILER /usr/bin/g++) Okay, that fixed it. Now the only error I'm seeing is: error: Cannot resolve a minimum constraint for dependency boost-disjoint-sets from boost:x64-linux. The dependency was not found in the baseline, indicating that the package did not exist at that time. This may be fixed by providing an explicit override version via the \"overrides\" field or by updating the baseline. See `vcpkg help versioning` for more information. These are the two links needed for me to proceed in solving this error. Also, see ../../vcpkg/vcpkg help versioning This is how a manifest file should work: { \"name\": \"example\", \"version\": \"1.0\", \"builtin-baseline\": \"a14a6bcb27287e3ec138dba1b948a0cdbc337a3a\", \"dependencies\": [ { \"name\": \"zlib\", \"version>=\": \"1.2.11#8\" }, \"rapidjson\" ], \"overrides\": [ { \"name\": \"rapidjson\", \"version\": \"2020-09-14\" } ] } The boost components I probably need are: container container-hash fusion units mpl tokenizer units spirit serialization (compiled, contains container_hash, and archive libraries) spirit (header-only) fusion units mpl tokenizer I give up for now. I can't install Boost with vcpkg. It's too frustrating. Boost 1.75 is the oldest I can install with a built-in baseline, and for some reason manually specifying the baseline with overrides requires me to write out each of the packages in Boost. Building on April 10 cp -r /cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa.aoi/22.50p001/ . find . -type d -name \"*example*\" This is the location of the binaries I want: /mnt/md127/tools/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa.aoi/22.50p001 So, assuming it is mounted on my machine at /cadence , I can do: cp -r /cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa.aoi/22.50p001/* . Multiple defenitions errors: I'm finding some possible recommendations about how to figure out this multiple definitions issue: https://stackoverflow.com/questions/69326932/multiple-definition-errors-during-gcc-linking-in-linux https://stackoverflow.com/questions/37525922/how-to-handle-gcc-link-optionslike-whole-archive-allow-multiple-definition The idea to hack it with --allow-multiple-definition in target_link_libraries doesn't work as g++ doesn't support it.","title":"BAG Analog Generator"},{"location":"bag_old_notes_delete_me/#bag-analog-generator","text":"","title":"BAG Analog Generator"},{"location":"bag_old_notes_delete_me/#crossely-iccad13","text":"produce only sized schematic, the topology is up to the designer Good for: technology characterization schematic and testbench translation simulator interfacing physical verification and extraction parameterized layout creation for common styles of layout. BAG, in it's basic form belongs to the 'knowledge-based' design automation class, although specific sub-circuits area free to be optimized with some algorithm. This is especially useful with BAG, as we already have the ability to produce a high-quality subset of design to choose between. Knowledge-based design scripts are especially useful, as they self-document the design process. They also keep the designer 'in control' of the process. a completed BAG script helps with top level design, as you can specify the top level parameters, and then recursively instantiate the subblocks \"In our approach to analog circuit automation, a designer\u2019s deliverable is not a single instance of a sized schematic and clean layout for a particular circuit, but rather a generator for a desired class of circuits that can replicate, in an automated fashion, the design procedure that would have been used for a traditional, manual design.\" Each realized circuit Generator much implement the methods of the interface ReadSpecification() DesignSchematic() DesignLayout() VerifyArchitecture() WritePerformance() Because tasks are often the same between circuit generators though, there are abstract classes which provide connections to common actions in the design process, and to connect to tools. CodeStubGeneration() RunOptimizer() LaunchSimulations() RunDRCandLVS() IO_OpenAccess()","title":"Crossely ICCAD'13"},{"location":"bag_old_notes_delete_me/#bag-install-notes","text":"BAG_prim is cadence library, with device used to build schematic template (called a schematic generator) Schematic generators are schematic templates are used as in input to design modules, which then produce new design instances as an output Drop in replacement for instances in a schematic generator","title":"BAG Install Notes"},{"location":"bag_old_notes_delete_me/#monday-june-27","text":"I'm at the point in the tutorial where I've installed the necessary python packages, and now it's instructing me on how to set up the 'configuration file. But I don't understand where the Configuration file is? Actually wait, it seems that I need to make these config files, and that they aren't BAG specific. I just need to 'point' to them from within the BAG database, so that BAG knows where they are? env_file, lvs_runset, rcx_runset, and cell_map all appear to be files that are used normally for Cadence work. Okay, so then the page with 'BAG Configuration File' listed must be listing items that I should interact with through the BAG interactive session? Okay, so do I need to import python, and then try to import the BAG library? I think I understand now; I need to load the bag2 code base from within python3, and use the setup.py and __init__.py file as my starting point for library import. To install this module, I really want to use pip, but as I have multiple python version on the remote, I should launch pip from the anaconda python environment: https://stackoverflow.com/questions/40392499/why-is-m-needed-for-python-m-pip-install https://stackoverflow.com/questions/25749621/whats-the-difference-between-pip-install-and-python-m-pip-install","title":"Monday June 27"},{"location":"bag_old_notes_delete_me/#installing-bag-environment","text":"Start by getting the latest version of Anaconda, using this page: https://linuxize.com/post/how-to-install-anaconda-on-centos-7/ This is the version of Anaconda I selected: Anaconda3-2022.05-Linux-x86_64.sh After anaconda was installed, I tried to setup the BAG3 code, but the source for the documentation didn't exist, so I also downloaded the BAG2 code, to try and generate the documentation. I got the following error trying to build the sphinx documentation: (base) bash-4.2$ make html sphinx-build -b html -d build/doctrees source build/html Running Sphinx v4.4.0 making output directory... done WARNING: sphinx_rtd_theme (< 0.3.0) found. It will not be available since Sphinx-6.0 Theme error: no theme named 'sphinx_rtd_theme' found (missing theme.conf?) make: *** [html] Error 2 I think I could try installing this with pip, but because I'm relying on Anaconda for this installation, I searched for a method using anaconda. https://anaconda.org/conda-forge/sphinx_rtd_theme To install this package with conda run one of the following: conda install -c conda-forge sphinx_rtd_theme Rerunning 'make html' I get the following: (It looks like it worked? Build finished. The HTML pages are in build/html. the documentation is working! It looks like I will be using a lot of Git, so I figured it would be best if I just updated my version of git: https://computingforgeeks.com/how-to-install-latest-version-of-git-git-2-x-on-centos-7/ Getting ready: Update git version on asiclab008 Configure easy ssh access with key authentication Building the documentation Mounting the remote drive as a SMB share, so that it's locally available on mac os (also considered StrongSync)","title":"Installing BAG Environment"},{"location":"bag_old_notes_delete_me/#cadence-crashing-notes","text":"For Friday: x Figure out if Cadence is actually crashing (yes, it is!) x test linux config, to find a way to not make it crash. (Just use IC618) The current version of my redhat/centOS distribution can be checked via: cat /etc/redhat-release To check Cadence config: /cadence/cadence/IC618/tools.lnx86/bin/checkSysConf IC6.1.8 output of this indicated that the following packages are needed: sudo yum install xorg-x11-fonts-ISO8859-1-75dpi redhat-lsb xorg-x11-server-Xvfb Now everything is passing for IC618! Let's examine our two startup scripts: /faust/user/kcaisley/cadence/tsmc65/cdr/tsmc_crn65lp_1.7a_rd53b -starts with /bin/csh -f -setenv CDSDIR /cadence/cadence/IC617 -finishes with 'virtuoso' the other script is, completely: #!/bin/bash export DMC_RUN_DIR=$(pwd) export DMC_SOS_DIR=/faust/user/kcaisley/designs/dmc65 /cadence/local/bin/tsmc_crn65lp_1.7a_rd53 & examining this other nested script tsmc_crn65lp_1.7a_rd53 We want to create a new startup script for running with cadence, we will put it in our personal directories for now. Changing the following lines: setenv CDSDIR /cadence/cadence/IC617 -> setenv CDSDIR /cadence/cadence/IC618 virtuoso -> vituoso & This works now, as long as I start it as an executable './script_name' In hind sight, it looks like we might not have some support for IC617, including no ASSURA618 version. Therefore, I should probably just use the startup script tsmc_crn65lp_1.7a_rd53, and call it from piotrs script, as this adds additional DMC-specific variables. Nope, it just freezes when it starts, so I recopied the central script, made the two changes, and will work from there.","title":"Cadence Crashing Notes:"},{"location":"bag_old_notes_delete_me/#anaconda-install","text":"Cheatsheet for using Conda: https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf Conda Environments Guide: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html open anaconda navigator: anaconda-navigator & listing conda environments: conda env list changing between conda envs: conda active bag3 create conda new env: conda create --name [myenv]","title":"Anaconda Install:"},{"location":"bag_old_notes_delete_me/#august-26","text":"to figure out current tty number w to show which tty are being used ps -e | grep tty to kill another tty pkill -9 -t tty1 to show process numbers ps -f another tool which is useful for identifying and killing processes is top/htop. i think that top can do everything htop can, but more easily perhaps i should learn to play with top more in the future","title":"August 26:"},{"location":"bag_old_notes_delete_me/#thursday-june-23","text":"BAG_prim is cadence library, with device used to build schematic template (called a schematic generator) Schematic generators are schematic templates are used as in input to design modules, which then produce new design instances as an output Drop in replacement for instances in a schematic generator","title":"Thursday June 23 \ud83c\udf4e\ud83c\udf4e"},{"location":"bag_old_notes_delete_me/#monday-june-27_1","text":"I'm at the point in the tutorial where I've installed the necessary python packages, and now it's instructing me on how to set up the 'configuration file. But I don't understand where the Configuration file is? Actually wait, it seems that I need to make these config files, and that they aren't BAG specific. I just need to 'point' to them from within the BAG database, so that BAG knows where they are? env_file , lvs_runset , rcx_runset , and cell_map all appear to be files that are used normally for Cadence work. Okay, so then the page with 'BAG Configuration File' listed must be listing items that I should interact with through the BAG interactive session? Okay, so do I need to import python, and then try to import the BAG library? Found this online: Package - A folder/directory that contains __init__.py file. Module - A valid python file with .py extension. Distribution - How one package relates to other packages and modules . I think I understand now; I need to load the BAG3 code base from within Python3, and use the setup.py and __init__.py file as my starting point for library import. To install this package, I really want to use pip , but as I have multiple python version on the remote, I should launch pip from the anaconda python environment: It still wasn't working, but it's because I had to target the directory explicitily, and not the setup.py file. The command that appears to have finally worked for me is python -m pip install /faust/user/kcaisley/bag3 It's important to remember that a 'module' in Python is the name for any file ending in a .py extension. It appears that I could have actually just ran pip install . while inside the bag3 directory. Oh well \ud83d\udc81\ud83c\udffc\u200d\u2642\ufe0f Mark helped me understand that it's better to use conda virtual environements. We built a python environement with \u200b conda create -n bag3 python=3.9.12 \u200b conda activate bag3 And then we reinstalled our BAG3 package in this virtual environement with the following command, run in our /faust/user/kcaisley/bag3 directory: python setup.py develop My next task is to figure out how to edit the configuration options of BAG, and how to start using it in an environement like JupyterNotebooks. It's obvious that the BAG environment is running as a sqlite database, and that we need to initialize that database to interact with it.","title":"Monday June 27 \ud83c\udf4e\ud83c\udf4e\ud83c\udf4e\ud83c\udf4e\ud83c\udf4e"},{"location":"bag_old_notes_delete_me/#other-bag-thoughts","text":"Make sure all packages are installed as listed in the BAG_framework documentation. The two needed, sqlitedict and openmdao are installed via pip, not anaconda, as they aren't on the conda repo (but are on Pypi repo) lauch the setup.py file with \"pip install .\", while inside the folder. As stated online, it's better to avoid directly calling the setup.py file. Create conda environment: conda create --name bag conda activate bag Install in current env: conda install ..... conda info Getting the repository: git clone -b develop https://github.com/ucb-art/bag.git --recurse-submodules Figuring out what changes I've made to local: git status git log git info git fetch --dry-run git fetch --dry-run --all git remote update git show-branch *develop git config --global user.name \"Kennedy Caisley\" git config --global user.email kcaisley@uni-bonn.de git config --global core.editor vim To copy down submodule, if forgot to do during cloning: git submodule update --init --recursive --remote ERROR: Cloning into 'pybag'... Permission denied (publickey). fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. Clone of 'git@github.com:ucb-art/pybag.git' into submodule path 'pybag' failed To reset all unstaged/uncommitted change: git checkout . git pull = git fetch + git merge https://github.com/ucb-art/pybag.git","title":"Other BAG thoughts:"},{"location":"bag_old_notes_delete_me/#august-27","text":"List of packages: apipkg appdirs attrs backcall cycler decorator distlib execnet filelock h5py importlib-metadata ipython ipython-genutils jedi Jinja2 kiwisolver MarkupSafe matplotlib more-itertools networkx numpy packaging pandocfilters parso pexpect pickleshare pluggy prompt-toolkit ptyprocess py Pygments pyparsing PyQt5 PyQt5-sip pytest pytest-forked pytest-xdist python-dateutil pyzmq ruamel.yaml ruamel.yaml.clib scipy six sortedcontainers traitlets virtualenv wcwidth zipp zmq","title":"August 27:"},{"location":"bag_old_notes_delete_me/#august-28","text":"I need to make sure that I use BAG_framework as my directory name, and not just BAG. Delete the /designs/demo_ffmpt/ file and start over, by remaking a new empty one. First, start a new project repo with these steps: https://github.com/ucb-art/cds_ff_mpt-bag3 Then start up cadence and jupyter notebook with steps at the end of: https://github.com/ucb-art/BAG2_cds_ff_mpt/ (This may not be 100% accurate.)","title":"August 28:"},{"location":"bag_old_notes_delete_me/#workspace-setup-phase","text":"Several differnt things need to be modified: Consult the bag2 ffmpt library to look at the configuration for Jupyter that is read in, before starting ipython_config.py is stored inside the .ipython\\profile_default directory. The bag3 ffmpt library was only the primitives directory on the lower level, but it does contain this file at ../../cds_ff_mpt/workspace_setup/ipython_config.py . But the bag3 ffmpt library didn't have the xbase the ipython_config.py file call bag_config.yaml bag_submodules.yaml ipython_config.py leBindKeys","title":"Workspace setup phase:"},{"location":"bag_old_notes_delete_me/#needed-changes","text":"In ./cds_ff_mpt/workspace_setup/ipython_config.py , needed to a ## List of files to run at IPython startup. c.InteractiveShellApp.exec_files = [ os.path.join(os.environ['BAG_WORK_DIR'], 'bag_startup.py'), ] This then calls ./bag_startup.py , which I had to create, as is design specific to let Jupyter Notebook/Python know where to look. It's contents was: # -*- coding: utf-8 -*- import os import sys sys.path.append(os.environ['BAG_FRAMEWORK']) sys.path.append(os.environ['BAG_TECH_CONFIG_DIR']) sys.path.append(os.path.join(os.environ['BAG_WORK_DIR'], 'BAG2_TEMPLATES_EC')) sys.path.append(os.path.join(os.environ['BAG_WORK_DIR'], 'BAG_XBase_demo')) sys.path.append(os.path.join(os.environ['BAG_WORK_DIR'], 'bag_advanced_examples')) sys.path.append(os.path.join(os.environ['BAG_WORK_DIR'], 'bag_testbenches')) This, of course, then leads me to consider if I have all the necessary project files mentioned above put in place. [x] BAG_FRAMEWORK (bag code base) [x] BAG_TECH_CONFIG_DIR (BAG PDK primitives folder, containing PDK softlink, among other things) [X] BAG2_TEMPLATES_EC ... had to grab this online, added via simple git clone: https://github.com/ucb-art/BAG2_TEMPLATES_EC.git [X] BAG_XBase_demo.... https://github.com/ucb-art/BAG_XBase_demo/tree/master [X] bag_advanced_examples... same, had to clone from https://github.com/ucb-art/bag_advanced_examples.git [X] bag_testbenches... yep, got from https://github.com/ucb-art/bag_testbenches.git This completed now, but I still have unanswered questions about several other files. Explaining everything below:","title":"Needed changes:"},{"location":"bag_old_notes_delete_me/#chronological-origin-of-files-in-top-level-directory-during-installation","text":"Following steps from: https://github.com/ucb-art/cds_ff_mpt-bag3","title":"Chronological origin of files in top level directory, during installation:"},{"location":"bag_old_notes_delete_me/#manually-making-new-git-repo-and-adding-git-modules-and-updating-them","text":"BAG_framework manually git moduled from my local copy cds_ff_mpt-bag3 manually git moduled from local copy (this is primitives tech repo) .gitmodules .git","title":"Manually making new git repo, and adding git modules, and updating them"},{"location":"bag_old_notes_delete_me/#running-installsh-in-workspace-directory-many-of-these-files-had-to-be-prepared","text":"bag_submodules.yaml copied from worksapce setup, via install.sh .cdsenv.personal copied from worksapce setup, via install.sh .cdsinit.personal copied from worksapce setup, via install.sh bag_config.yaml link from workspace setup, via install.sh, multipurpose config file, used by setup_submodules.py during installation, and during BAG runtime. .bashrc link from workspace setup, via install.sh .bashrc_bag link from workspace setup, via install.sh .cdsenv link from workspace setup, via install.sh .cdsinit link from workspace setup, via install.sh cds.lib.core link from workspace setup, via install.sh, included by cds.lib on startup .cshrc link from workspace setup, via install.sh, unused .cshrc_bag link from workspace setup, via install.sh, unused display.drf link from workspace setup, via install.sh .gitignore link from workspace setup, via install.sh models link from workspace setup, via install.sh (is a directory) pvtech.lib link from workspace setup, via install.sh leBindKeys.il link from workspace setup, via install.sh start_tutorial.sh link from workspace setup, via install.sh tutorial_files link from workspace setup, via install.sh .ipython created by install.sh (is a diretory, which contains ipython_config.py, which in turn is a link from workspace setup, via install.sh) gen_libs created by install.sh cds.lib created by install.sh (just points to cds.lib.core, which in turn points to ) run_bag.sh link from BAG framework, via install.sh, core script which actually runs BAG, called from core BAG scripts. setup_submodules.py link from BAG framework, via install.sh, executed in the next step to actually start_bag.il link from BAG framework, via install.sh, written in SKILL, seems to call virt_server.sh , which in turn starts stuff in framework. start_bag.sh link from BAG framework, via install.sh, very similar to run_bag.sh but appears unused virt_server.sh link from BAG framework, via install.sh, called by start_bag.il , contains 1-line command to start server in BAG framework.","title":"Running install.sh in workspace directory (many of these files had to be prepared)"},{"location":"bag_old_notes_delete_me/#running-setup_submodulespy-which-references-bag_modulesyaml-both-copiedlink-in-the-step-above","text":"bag3_analog automatic git module added by setup_submodules.py, from bag_modules.yaml bag3_digital automatic git module added by setup_submodules.py, from bag_modules.yaml xbase automatic git module added by setup_submodules.py, from bag_modules.yaml .bashrc_pypath created by setup_submodules.py ; contains PYTHONPATH env var python copies to sys.path on startup. Later exported by run_bag.sh and start_bag.sh . bag_libs.def created by setup_submodules.py with plaintext list of OA BAG libraries. cds.lib.bag created by setup_submodules.py , and included by cds.lib.core, which in turn is included by cds.lib.core","title":"Running setup_submodules.py, which references bag_modules.yaml, both copied/link in the step above."},{"location":"bag_old_notes_delete_me/#this-finishes-all-the-general-bag3-workspace-setup-but-for-the-tutorial-more-was-needed","text":"Following steps at at https://github.com/ucb-art/BAG2_cds_ff_mpt/ BAG2_TEMPLATES_EC manually git cloned from Github bag_advanced_examples manually git cloned from Github bag_testbenches manually git cloned from Github BAG_XBase_demo manually git cloned from Github bag_startup.py manually created, and made ipython_config.py call it, based on BAG2 example, adding libraries to pythonpath","title":"This finishes all the general BAG3 workspace setup, but for the tutorial, more was needed:"},{"location":"bag_old_notes_delete_me/#finally-starting-up-virtuso","text":"conda activate bag source .bashrc virtuoso & ./start_tutorial.sh this starts up iPython/Jupyter, which in turn calls the .ipython/ipython_config file, which then calls bag_startup.py libManager.log Log file created by Cadence Virtuoso on each startup.","title":"Finally, starting up Virtuso"},{"location":"bag_old_notes_delete_me/#pythonpath-and-syspath-understanding","text":"https://www.devdungeon.com/content/python-import-syspath-and-pythonpath-tutorial","title":"PYTHONPATH and sys.path understanding:"},{"location":"bag_old_notes_delete_me/#changes-to-accomodate","text":"renamed ~/eda/ to ~/packages/. Best to check for issues moved demo_ffmpt folder to ~/cadence/ folder, rather than /designs/. Check for issues. moved cds_ff_mpt-bag3 to ~/packages/ from ~/cadence/ moved cds_ff_mpt_v_1.1 to ~/packages/ from ~/cadence/ so had to update PDK symlink.","title":"Changes to accomodate:"},{"location":"bag_old_notes_delete_me/#potential-outstanding-problems","text":"My reference to the cds_ff_mpd-bag module is broken in git, as it's still looking for an origin master in ~/cadence/. See above, as it's moved to ~/packages/ As a solution for now, I've just deleted the remote reference. It shouldn't really matter, as this is the only project I'll need which will use this cds_ff_mpt PDK Is anything calling .bashrc_pypath ? I know run_bag.sh exports it; but I don't see the latter running anytime? Will the Jupyter notebook embedded in VScode read my .ipython/profile_default/ipython_config.py file on startup, like a standard Jupyter notebook would? I may just delete all references to git, and move on with my life in this set of modules.","title":"Potential outstanding problems:"},{"location":"bag_old_notes_delete_me/#friday-sep-9","text":"I need Jupyter to know where all my files are. Opening a Jupyter notebook in vscode will allow me to select a Anaconda environment, but it doesn't easily let me run the .ipython file, which calls the bag_startup.py to append to my Once my Jupyter notebook in started in vscode, I can't simply call bash scripts to modify the environment variables. Plus, the sys.path has already been initialized by the integrated ipykernel upon startup, and no more interaction with the environement variable ipython Jupyter comes with various components. The notebook server is language agnostic, and servers as the front end. iPython is a terminal only front end, which isn't used if I'm using the GUI jupyter notebook. The kernel is used by both, but is language specific. I need to understand the difference between the notebook server and the jupyter kernel. It seems the kernel is what needs to be associated with my anaconda env, and not the notebook server?? These are shown in two different places, Kernel in the top left, and server in the bottom right. I want to server to be started in a way that it is concious of all the environement variables I export when also starting cadence. I think part of the problem I was facing before was that the local jupyter server was starting with it's home following the jupyter notebook symlink. And so this causes problems, becuase I To start jupyter in a certain environment, simple follow basic method here, before he introduces his package: http://stuartmumford.uk/blog/jupyter-notebook-and-conda.html (as I don't need to switch my env dynamically)","title":"Friday Sep 9"},{"location":"bag_old_notes_delete_me/#monday-sep-12","text":"Figure out why does pybag.core not exist? There is a core.pyi file and core.cpp file. Maybe it's pointing at a C function! Seach 'How to import C function in An amazing note on relative imports: https://stackoverflow.com/questions/14132789/relative-imports-for-the-billionth-time","title":"Monday Sep 12"},{"location":"bag_old_notes_delete_me/#tuesday-sep-13","text":"My virtual env setup is really confusing the hell out of me. I decided to strip out Anaconda and just use Python -m venv. This can can't cross versions though, and so I need to install the latest version of python on my machine, 3.10, as i only have 3.6 as the system python.... Actually, I tried following this tutorial: https://linuxstans.com/how-to-install-python-centos/ but it didn't work A method that seemed to work istead was to create a local install of python: https://codeghar.wordpress.com/2013/09/26/install-python-3-locally-under-home-directory-in-centos-6-4/ This works. which python3.10 points in my PATH. Except now, the version of OpenSSL that I have doesn't work with Python3.10, and so I need to instead install python 3.9.14 with this method. Creating a venv: https://docs.python.org/3/library/venv.html python3 -m venv /path/to/new/virtual/environment activating a venv: source venv/bin/activate installing packages: pip install -r ./requirements.txt for some reason, numpy, scipy, matplotlib, and m5py needed to be installed manually, as they were needing compiling and crashing if included in the above file. This all works now though, in my new venv.","title":"Tuesday Sep 13"},{"location":"bag_old_notes_delete_me/#wednesday-sep-14","text":"No I'm going to copy over the bag version, and see if I can get this sorted out, with the pybag.core problem. I tried installing the packages pybind11, into pybind_generics, into pybag into bag, in that order, but I started hitting compile errors about Cmake. My Cmake version is okay, but apparently GCC was also outdated: https://stackoverflow.com/questions/47238577/target-requires-the-language-dialect-cxx17-with-compiler-extensions-but-cma However, at the bottom of this document, it points out that Centos has a package called devtoolset-7. By checking the command 'scl -l' I found that I already have this package. Running 'scl enable devtoolset-7 bash' was enough to get the gcc version updated! I think it just applies to the current terminal though. This is explained here: https://www.softwarecollections.org/en/scls/rhscl/devtoolset-7/","title":"Wednesday Sep 14"},{"location":"bag_old_notes_delete_me/#thursday-15-september","text":"How can I deal with installing Pybag? Is it a wheel package?","title":"Thursday 15 September"},{"location":"bag_old_notes_delete_me/#friday-16-sep","text":"I inspected the logs from running pip install -e . and found complaints about the packages fmt and spdlog being missing. I installed the two of these from yum, and tried to proceed. CMake has stopped complaining about fmt, but still is asking for spdlog to be located. A useful command for searching all files for some text: grep -rnw '/path/to/somewhere/' -e 'pattern' ... For example: grep -rnw '.' -e 'CMAKE_PREFIX_PATH' Cleaned out my venv folder once again, and tried to follow the instructions from yrrapt I've recloned the bag3 develop branch on two separate machines, one with CentOS 7.9 and the other with Ubuntu 20.04. On each, I built a fresh python venv, and tried compiling the bag and pybag modules. In each case, the bag modules built properly, but the sub-module pybag is problematic. I've tried to read the CMakeLists.txt and follow the error build.log, to find clues about what packages might be missing or variables not configured, but no luck so far. My machines have the following: CentOS 7.9 Python 3.9.14 (built from source) CMake 3.17.5 fmt 8.22 Boost 1.53.0 gcc 7.3.1 Ubuntu 20.04 Python 3.8.10 CMake 3.16.3 fmt 8.30 Boost 1.71.0 gcc 9.4.0 Running python -m pip install . , I'm seeing build logs that contain the error below.","title":"Friday 16 Sep"},{"location":"bag_old_notes_delete_me/#after-installed-cmake-with-apt-wheel-with-pip-sudo-apt-install-libboost171-all-dev-but-still-got","text":"ERROR: Command errored out with exit status 1: command: /home/silab/delete_me_please/bag/venv/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-allzw648/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-allzw648/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-2t7f35c2 cwd: /tmp/pip-req-build-allzw648/ CMake Error at cbag/CMakeLists.txt:94 (find_package): Could not find a package configuration file provided by \"fmt\" with any of the following names: fmtConfig.cmake fmt-config.cmake Add the installation prefix of \"fmt\" to CMAKE_PREFIX_PATH or set \"fmt_DIR\" to a directory containing one of the above files. If \"fmt\" provides a separate development package or SDK, be sure it has been installed. I think I need to point CMake toward my fmt and spdlog installs. Will investigate further.","title":"After, Installed CMake with apt, wheel with pip, sudo apt install libboost1.71-all-dev, but still got:"},{"location":"bag_old_notes_delete_me/#some-quick-notes-on-setting-env-variables","text":"To set variable only for current shell: VARNAME=\"my value\" To set it for current shell and all processes started from current shell: export VARNAME=\"my value\" # shorter, less portable version To set it permanently for all future bash sessions add such line above to your .bashrc file in your $HOME directory. Can be found here To add something to the beginning of a path variable: export CMAKE_PREFIX_PATH=____:${CMAKE_PREFIX_PATH} where ___ is the thing you want to add","title":"Some quick notes on setting ENV variables:"},{"location":"bag_old_notes_delete_me/#to-get-libfmt","text":"find_package did not find the CMake package of fmt. It comes with the -dev variant of fmt's Ubuntu package. If you check the CI code, it installs libfmt-dev via apt: sudo apt install libfmt-dev Now it's pretty clear that I need the .cmake file from this package, and to figure out where it is I can use: dpkg -L libfmt-dev Yes! Now I can just get this file: /usr/lib/x86_64-linux-gnu/cmake/fmt/fmt-config.cmake I'm not sure what 'depth' I have to point it at, but I will try all the way to /fmt. (I was right) Looks like I will also be missing spdlog, so I will installed the development flavor with: sudo apt install libspdlog-dev dpkg -L libspdlog-dev which gives: /usr/lib/x86_64-linux-gnu/cmake/spdlog/spdlogConfig.cmake Will point at this below","title":"To get libfmt:"},{"location":"bag_old_notes_delete_me/#lets-try-it-with-yrrapts-official-repo-just-making-sure-we-enable-oa","text":"git clone -b develop https://github.com/yrrapt/bag.git --recurse-submodules From inside bag dir python3.9 -m venv ~/designs/bag3_pll_verfication/venv source venv/bin/activate vim pybag/setup.cfg and deleted line openaccess-disable = True so that it could build with open access export PYBAG_PYTHON=$HOME/temp/bag/venv/bin/python python -m pip install wheel to install wheel packages python -m pip install . to build bag package export CMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu/cmake/fmt:${CMAKE_PREFIX_PATH} export CMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu/cmake/spdlog:${CMAKE_PREFIX_PATH} export CMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu/cmake/yaml-cpp:${CMAKE_PREFIX_PATH} cd inside pybag dir, and python -m pip install . again to build pybag package Nope, still errors: WARNING: building without OpenAccess support. -- Found Boost: /usr/lib/x86_64-linux-gnu/cmake/Boost-1.71.0/BoostConfig.cmake (found version \"1.71.0\") found components: serialization CMake Error at pybind11_generics/CMakeLists.txt:46: Parse error. Expected a command name, got unquoted argument with text \"<<<<<<<\". Checking this file, we find that in fact, that should probably be deleted haha. grep -rnw '.' -e 'CMAKE_PREFIX_PATH' CMake Error at pybind11_generics/pybind11/tools/pybind11Tools.cmake:17 (find_package): By not providing \"FindPythonLibsNew.cmake\" in CMAKE_MODULE_PATH this project has asked CMake to find a package configuration file provided by \"PythonLibsNew\", but CMake did not find one. Could not find a package configuration file provided by \"PythonLibsNew\" with any of the following names: PythonLibsNewConfig.cmake pythonlibsnew-config.cmake Add the installation prefix of \"PythonLibsNew\" to CMAKE_PREFIX_PATH or set \"PythonLibsNew_DIR\" to a directory containing one of the above files. If \"PythonLibsNew\" provides a separate development package or SDK, be sure it has been installed. Call Stack (most recent call first): pybind11_generics/pybind11/tools/pybind11Common.cmake:201 (include) pybind11_generics/pybind11/CMakeLists.txt:169 (include) Now it's asking me to find a .cmake file for PythonLibsNewConfig, but apt list doesn't reveal anything to install. But this already lives at ..... ./pybag/pybind11_generics/pybind11/tools/FindPythonLibsNew.cmake Can I try to hack it? export CMAKE_PREFIX_PATH=/home/silab/temp/bag/pybag/pybind11_generics/pybind11/tools:${CMAKE_PREFIX_PATH} Okay... that's enough for today. Compacting the lines from fefore: * export CMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu/cmake/fmt:/usr/lib/x86_64-linux-gnu/cmake/spdlog:CMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu/cmake/yaml-cpp:${CMAKE_PREFIX_PATH}","title":"Let's try it with yrrapt's official repo, just making sure we enable OA:"},{"location":"bag_old_notes_delete_me/#sat-17-sep","text":"Perfect. Building your modified repostory I was able to avoid the CMake issues, after I installed libspdlog-dev , fmt-config, and libyaml-cpp-dev to provide the necessary .cmake files) Simpler than manually adding them to the CMake path via, as I was doing before: export CMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu/cmake/fmt:/usr/lib/x86_64-linux-gnu/cmake/spdlog:/usr/lib/x86_64-linux-gnu/cmake/yaml-cpp:${CMAKE_PREFIX_PATH} Case study to be shared In above case sutdy did they get this working? Maybe check with Mirjana/Abassi first? Did they get the Open Access stuff? Ask yrrapt about \"<<<<<<\" HEAD\" bug, and also about the FindPythonLibsNew.cmake bug. Ask yrrap how I can contribute, and if I should move discussions into the Git log tracker.","title":"Sat 17 Sep"},{"location":"bag_old_notes_delete_me/#sunday-18-sep","text":"I really want to apply for this competition: https://github.com/sscs-ose/sscs-ose-code-a-chip.github.io Its submission deadline is November 21, which would give me a perfect outlet for completing a functioning Jupyter notebook in a reasonable amount of time. I've already been here for 133 days, and I feel as though I haven't accomplished anything. For now though, don't tell Hans about this conference. I want to learn how to use BAG, because OpenFASOC isn't really meant to work with analog chip design. The issue is that BAG3 doesn't support open source tools, and that access for the necessary OpenAccess library is apparently very difficult to get anyways. I want to use BAG3, as BAG2 is not going to be further developed. My main question then is what are the major differences between the two? I'm meeting with Thomas tomorrow, and he seems to understand the better the status of BAG. So what if I did the following.... built a generator which is able to produce a functioning VCO layout.. or even PLL, and do it in a way that uses the toolchain of BAG3? If I accomplished this, I would be the first person to successfully use BAG3 outside of Berkeley. Other papers, few of which exist, have used BAG2 only. The issue now, is that I'm not sure if I can really build a functioning tool chain with BAG3. It doesn't support open source tools yet, and getting it to even work with Cadence seems nigh impossible for those outside Berkeley/BlueCheetah. I should confirm this by reading papers, tomorrow, of course. I think I can get through a lot of material by the time I meet with Thomas Parry at 3pm. One thing I'm realizing though is that a lot of the design process doesn't really require me to have a function BAG installation. XBase seems like it is very tied to BAG, but I think LayGO2 is less BAG-centric? I should really check with JD Han to figure out how I'm expected to produce a layout in 65nm if I don't have this special Cadence OpenAccess library. Perhaps I can w Mirjana is a member of the committee which decides the winners of the code-a-chip competition, and so if I were to develop a notebook with her guidance, then there is a very good chance of being accepted. Another note, I should consider giving a group presentation on Sep 29th, because I told Hans that I would. Maybe I should break this promise though. I should explain to Hans that understanding the status of all this open source code has taken significantly more time than I anticipated. Explain that I've made connections with two important people though! If I don't use BAG, then how do I work with the schematics needed to compare against LayGO layouts during LVS? If I want to design a process portable generator, do I target the layout constraints of a small-fill factor 28nm design, and then expect it to scale to a 65nm, or even 130nm process? I think this makes more sense, as there are less degrees of freedom in 28nm. Can this tool (LayGO2) be used in isolation, without BAG? I think it can! One thing that is amazing about this, is that Laygo is written entirely in Python, and so I shouldn't have any huge problems getting it to run. No compilation required, etc. It's a 1-way path for generation. Also, much of the design work 'back of the envelope work' can be done in Python scripts, without any circuit simulation at all. But, then, once I get to circuit simulation, I wonder if I am able to run parasitic extraction with Magic on open source Tools on 65nm? If think seems to work well, then I think I can trust it enough to do large cell design without re-running a DRC/LVS ruleset in Calibrew. To be double sure, I can simply run a final Parasitic extraction and DRC and LVS test against Cadence. There's also no reason why my schematics would necessarily need to be be designed in cadence source tools, right? If I am looking for process portability though, I think that I need to use something like BAG, where I am am able to make schematic and test bench generators. So one question is: If I'm not using BAG... how do I achieve process portability in my schematics, such that I can quickly compare them against laygo? I think that the basic Laygo workflow uses hand-crafted primitives, which aren't process agnostic. On the other hand, I think that Lay I can slowly replace every part of the 65nm workflow in Cadence, with equivalents in 65nm technology.","title":"Sunday 18 Sep"},{"location":"bag_old_notes_delete_me/#monday-19-sep","text":"I think the functionality is primarily good for the Analog Chip Bottom, where the SAR ADC, the Bandgap references, the Analog Multiplexer, and monitor ADCs, the Calibration Injection Voltages, the Serdes, and the CDR and TX/RX Circuits live. This is where documentation, reusability, comprehension, and testability are much, much more important that cutting edge performance. Generators are also best suited for instances where you will need to do many different iterations, in possilbly different BAG3 is a major upgrade, as it reworks the way layouts are generated in the tool. There is no AnalogBase/DigitalBase constructs anymore, just MOSBase. As long as devices share the same 'row' information, you can tile NMOs, PMOS, and TAPs right next to each other. Don't specify the width of each wire, have 'wire classes' which have preset widths. MOSBase just places the drawn transistors, and isn't necessarily DRC clean. MOSBase Wrapper then comes back and fill in the dummy devices, extension regions, and boundaries. In the YAML file, we have to define the properties of each row. It's more complicated in BAG3. The layouts may only have one type of tile, or you may have a more complicated multi-tile type layout. In BAG2 AnalogBase, you only have access to M3 and 4, (maybe more in Digital Base). But in BAG3 Zhaokai Liu showed a BAG3 repository example on Github, but it's a private repo right now, and we can't see it. Bulk contacts are completed with TAP devices in BAG3. Different length devices are implemented with stacking devices, to increase effective channel length. To place transistors, you first specify the tile you are placing in, and then use XY coordinate (e.g.) within the tile. It defaults to position 0 for X and Y. BAG2 only supports ADEXL. BAG3 doesn't support any ADE package, Assembler, XL, or otherwise. Design variables are returned, but Stimuli and Data post processing must be done in Python, as we are just directly calling Spectre. Notice how all the BAG developers are also using BAG to build real circuits, meaning they have real design cases. BAG3 includes BAG2 Behavioral models of base circuits are completed manually, but then the hierarchical arrangement of them is done automatically, tracking whatever is done at in the real schematic implementation.","title":"Monday 19 Sep"},{"location":"bag_old_notes_delete_me/#meeting-with-thomas-parry","text":"Marco in Finland Marjana is moving to Infineon Infineon guys also putting money into Matthias is from Klayout, and he was able to implement a lot of the functions for BAG2 compatibility very quickly.","title":"Meeting with Thomas Parry"},{"location":"bag_old_notes_delete_me/#tuesday-20-sep","text":"","title":"Tuesday 20 Sep"},{"location":"bag_old_notes_delete_me/#continued-with-ayan-biswas","text":"Manually create leaf behavioural cells, and then allow BAg to generate the heirarchy. One thing is that schematic and modeling hierarchy should be identical, but layout heirarchy my be different, as for example, a large chain of CS amplifiers would generated all the resistors in one heirarchy.","title":"continued with Ayan Biswas:"},{"location":"bag_old_notes_delete_me/#recap","text":"To be honest I have not produce much at all, but it hasn't been for lack of trying or time invested. It turns out that this is way harder than I thought. I've worked on: * BAG project repository organization and setup * Python package management * Review of OOP programming in Python (To understand BAG codebase) * Jupyter Notebook Usage and Server Setup * CMake, Make, and other build related tooling (for building Python, BAG, PyBAG, XBase, etc) * Tools are 'complete' but not distributed in a complete form, so they have to be built One thing that is a bit annoying is that I have several different places where I could develop my code. I think to submit to CAC23 I need to have a GitHub repo. My personal Github account (kottenforst, or kennedycaisley, or kcaisley?) Things to understand from Hans: Can the models of TSMC65 be using in ngSPICE? Can LayGO2 be used without BAG? I think that the goal of the MOSAIC group is to modify the BAG3 codebase such that it can be used with open source tools like Xschem/Magic/Klayout. This is different from the goals of my group. My goal of my meeting with Mirjana is to figure out where my design work can overlap with the interests of MOSAIC. I don't have the time and the energy and You can use BAG3 for Measurements, without needing to have the openaccess library tooling. I think that I should maybe start here, and build a BAG3 test bench for my PLL is 65 nm. Skill is signifigantly slower than OA, but Mario Weiss also pointed out that GDS based generation is available in BAG2/BAG3. Some flows work without Virtuoso, but some functions require OA layout.","title":"Recap:"},{"location":"bag_old_notes_delete_me/#meeting-with-marjana","text":"Programatic IC design is necessary Talk to Chris Mayberry, BAG3, as he may be interested in helping Watch his 'Cascode Labs Presentation' UC Berkeley prof Vladamir S. it coming to Europe, Marjana may be able to talk with him","title":"Meeting with Marjana:"},{"location":"bag_old_notes_delete_me/#meeting-with-hans","text":"Recap what I've been working on, and what I've learned: Discovered a whole bunch of disorganized project folder for BAG2, BAG3, Xbase, Laygo, Laygo2 BAG project repository organization and setup (built my own repository) Python package management (built my own environment) Jupyter Notebook Usage and Server Setup OOP programming in Python (To understand BAG codebase) CMake, Make, and other build related tooling (for building Python, BAG, PyBAG, XBase, etc as Tools are not distributed built) Got very stuck 2 weeks ago, as I had spent many weeks now reading the code base, and trying to debug. I understand how the tool worked alot better, but I couldn't actually 'run' it full on anything. Went online (open source silicon slack channel), started connecting with people, and finding resources. I understand the lay of the land a lot better now, and the uses/limits of the tools, but I haven't made much progress on actual work (only basic prototyping of PLL specifications in Python). What I need help with is technology evaluation, and where to focus my efforts? I'm am very serious about this programmatic IC design, but it's early days. We for some of the features, we would be the first externel adopters (not Laygo1/2 or Bag2 though, but for BAG3 yes.) State of the art: BAG2 Workflow: Schematic in Cadence with generic primitives, connects with Cadence over SKILL API. Allows use of Laygo or Xbase. Status: Stable but slower, and no longer developed by UC berkeley. It does however has BAG3 Workflow: Allows use of up Status: Streamlined, actively developed, several features improved, and many aspects of the workflow have bee People: Chris Mayberry Marjana Thomas Parry In git, what is main, origin, HEAD, origin is the default name given to the remote repository that a local repository is tied to. One can change this name, and can also add two different remotes to pull changes from. This is why we need aliases for remote repositories, so that we can distinguish between two remote codebases that are often nearly identical.","title":"Meeting with Hans:"},{"location":"bag_old_notes_delete_me/#bag3-thoughts","text":"I think the functionality is primarily good for the Analog Chip Bottom, where the SAR ADC, the Bandgap references, the Analog Multiplexer, and monitor ADCs, the Calibration Injection Voltages, the Serdes, and the CDR and TX/RX Circuits live. This is where documentation, reusability, comprehension, and testability are much, much more important that cutting edge performance. BAG3 is a major upgrade, as it reworks the way layouts are generated in the tool. There is no AnalogBase/DigitalBase constructs anymore, just MOSBase. As long as devices share the same 'row' information, you can tile NMOs, PMOS, and TAPs right next to each other. Don't specify the width of each wire, have 'wire classes' which have preset widths. MOSBase just places the drawn transistors, and isn't necessarily DRC clean. MOSBase Wrapper then comes back and fill in the dummy devices, extension regions, and boundaries. In the YAML file, we have to define the properties of each row. It's more complicated in BAG3. The layouts may only have one type of tile, or you may have a more complicated multi-tile type layout. In BAG2 AnalogBase, you only have access to M3 and 4, (maybe more in Digital Base). But in BAG3 Zhaokai Liu showed a BAG3 repository example on Github, but it's a private repo right now, and we can't see it. Hi Thomas and Mirjana, After discussing with my co-workers + advisor, we've decided we want to identify and try acquiring, through official channels, the OpenAccess library used by BAG3 to interact with Virtuoso schematic and layout cellviews. If we succeed, we will document the steps we took to acquire the package, and share that knowledge with other groups. For those lacking the time or connections to acquire the dependency themselves, we would also be willing to take on the mantle of regularly compiling and distributing BAG3 with pybag/cbag binaries built against the OpenAccess component. Assuming it's legally allowed, I wonder if this would be useful for the subset of people primarily interested in just being users of BAG3 with Cadence. As the above step could take considerable time with NDAs, in parallel, I want to figure out if the component of BAG3 responsible for launching SPECTRE/SPICE simulations and returning the results is able to be used without the OpenAccess component. If so, I will compile a version of BAG with this reduced-functionality. Assuming I manage to make this work, I want to then use this for a project: My group invested significant time (4 revisions) designing this ^ 65nm clock-data recovery circuit for high radiation environments (total ionizing dose >1 Grad). The original design was done with a traditional industry workflow (Cadence Virtuoso, Calibre DRC/LVS/PEX, ADE XL testbenches, Spectre Simulation). Now that we have a silicon-verified design, though, we'd like to record that design knowledge in a BAG3 script for better documentation, modifications, and porting. Assuming BAG3's simulation component can be used as-in, I am going to start by re-implementing our verification framework for the PLL with the BAG3 API, and open source it.","title":"BAG3 Thoughts:"},{"location":"bag_old_notes_delete_me/#to-do","text":"","title":"To Do"},{"location":"bag_old_notes_delete_me/#backlog","text":"[ ] Generate pre and post-extracted netlist of 65nm VCO [ ] Create a basic simulation of inverter, using tradition GUI Virtuoso Schematic/Layout -> PEX Extraction -> ADE XL -> Spectre -> ADE XL viewer. [ ] Reproduce basic INV simulation with SPICE netlist -> SPECTRE Command Line -> Flat File Output [ ] Reproduce basic INV simulation with: SPICE netlist -> Jupyter Notebook -> BAG3 -> SPECTRE -> BAG3 -> Jupyter Notebook. [ ] Start assembling list of tests to run on VCO.","title":"Backlog"},{"location":"bag_old_notes_delete_me/#in-progress","text":"[ ] Identify what OpenAccess library is needed to properly compile and operate BAG3 (make issue of this) [ ] Move my venv over from my other venv, or just re-setup [ ] Figure out the ask to Ayan Biswas, Thomas Parry, or Marjana, to try and figure out exactly 'what' OpenAccess library is missing.","title":"In Progress"},{"location":"bag_old_notes_delete_me/#completed","text":"","title":"Completed"},{"location":"bag_old_notes_delete_me/#notes","text":"Should only have a vscode and firefox window open. Nothing more. No readme file until done. No figures or equations until done (just look in Razavi) A decorator '@' is a design pattern in Python that allows a user to add new functionality to an existing object without modifying its structure. Decorators are usually called before the definition of a function you want to decorate. They support operations such as being passed as an argument, returned from a function, modified, and assigned to a variable. This is a fundamental concept to understand before we delve into creating Python decorators. This tutorial explains it best: https://www.datacamp.com/tutorial/decorators-python The class SimProcessManager an implementation of :class: SimAccess using :class: SubProcessManager I'm not really even sure what this means. SubProcessManager looks like it batches calls in an concurrent/asyncrhonous mannager, using the asyncio Python standard lib But it looks like we would never call these latter two components, as they are lower level. Inside of the Spectre.py function, the","title":"Notes"},{"location":"bag_old_notes_delete_me/#openaccess-dependency-research","text":"A static library format for OA would require the rest of BAG be built with the the same version of build tools as it. Build tools are availabe from the CentOS software collections, in particular the devtoolset-7 through devltoolset-11 packages. These can be installed from yum, and examined with: sudo yum list devtoolset\\* As each component is also available as it's own package, we can examine package contents via: sudo yum list devtoolset-8\\* You can get started in three easy steps: Install a package with repository for your system: On CentOS, install package centos-release-scl available in CentOS repository: $ sudo yum install centos-release-scl On RHEL, enable RHSCL repository for you system: $ sudo yum-config-manager --enable rhel-server-rhscl-7-rpms Install the collection: $ sudo yum install devtoolset-8 Start using software collections: $ scl enable devtoolset-8 bash At this point you should be able to use gcc and other tools just as a normal application. See examples below: $ gcc hello.c $ sudo yum install devtoolset-8-valgrind $ valgrind ./a.out $ gdb ./a.out As Ayan points out, his .bashrc points to the locations of the various libraries. I notice that the version of OA is 22.60, which appears to be built against GCC 8.3, which is contained in devtoolset-8. It looks like OA 22.60 is maybe the most recent version? https://si2.org/tag/oa-22-60/. In either case, it's obvious that the version matters.","title":"OpenAccess Dependency Research"},{"location":"bag_old_notes_delete_me/#next-steps","text":"Contact Thomas first thing, share what I learned from Ayan, including imports in .bashrc, and what version of GCC is necessary, and version of OA. Next read the 6-7 tabs I have open about building from C++... link link link link","title":"NEXT STEPS"},{"location":"bag_old_notes_delete_me/#work-in-october","text":"oa_v22.60.063 Line 37 in this file: https://github.com/ucb-art/cds_ff_mpt-bag3/blob/master/workspace_setup/.bashrc","title":"Work in October"},{"location":"bag_old_notes_delete_me/#executables-libaries-etc","text":"Both libraries and executables are compiled non-human readable binaries, but the difference is that an executable will be intended to be used by itself and have a defined starting point, whereas code compiled as a library will have multiple extry points, and will be intended to be used as part of a larger project. When linking in a library, whether or not is static vs dynamic matters. In either case though, code described as 'libraries' are not executible on their own. A static library .a (or .lib on windows) is combined with compiled .c or .cpp machine .o in a process called linking (ld program), to produce an executible file with an .out (or .exe, on windows) extension. Static libraries increase the size of the code in your binary. They're always loaded and whatever version of the code you compiled with is the version of the code that will run. Dynamic libraries, with a .so (or .dll extension on Windows) are stored and versioned separately. It's possible for a version of the dynamic library to be loaded that wasn't the original one that shipped with your code if the update is considered binary compatible with the original version. Additionally dynamic libraries aren't necessarily loaded -- they're usually loaded when first called -- and can be shared among components that use the same library (multiple data loads, one code load). Dynamic libraries were considered to be the better approach most of the time, but originally they had a major flaw (google DLL hell), which has all but been eliminated by more recent Windows OSes (Windows XP in particular)","title":"Executables, Libaries, etc"},{"location":"bag_old_notes_delete_me/#work-in-november","text":"The OA binary libraries (.so) we have are for OA versions as recent as 22.60. In some instances we have their matching header (.h) files. But they have been compiled with GCC 4.4.x or older (probably corresponding to RHEL6). Our use case for these libraries is to link to them in from some C++17 source code. Here is a link which talks about if it's possible to link in libraries that were built with a different version of GCC/ C++ https://stackoverflow.com/questions/46746878/is-it-safe-to-link-c17-c14-and-c11-objects","title":"Work in November"},{"location":"bag_old_notes_delete_me/#response-from-ayan","text":"Hello Kennedy, The oa_v22.60.s007 is available in the Cadence Virtuoso library (IC618 or ICADVM181 or ICADVM201), as you mentioned. But BAG3 also requires the OA C++ libraries compiled for the OS (RHEL7) and C++ compiler (gcc 8) as you can see on lines 34-36 of the .bashrc that you linked. We were hoping to get the updated version of those libraries (raw or compiled). But it looks like you hit the same problems as us while trying to work out the deal with Si2. Some researchers in our group are trying to figure out ways to remove the OA dependence, or understand it a bit better by reading the available code base, in an effort to make BAG3 truly \"open source\" and accessible by people outside BWRC. I will let you know if those attempts converge to an acceptable solution.","title":"Response from Ayan:"},{"location":"bag_old_notes_delete_me/#message-from-me","text":"This does match the stance on Si2's website, which says only \"a license-keyed binary version is available for research and teaching purposes.\" What new version of the OA C++ libraries are you trying to acquire? I see a reference to oa_v22.60.s007 in your workspace setup .bashrc file, and my understanding is OpenAccess 22.60 (DM6) is the latest release. As an aside, do you know if the OA API dynamic library .so files included in the standard Cadence Virtuoso install (at ../IC618/tools.lnx86/lib/64bit/ and ../IC618/oa_v22.60.063/lib/linux_rhel60_64/opt) are essentially what we'd have by building C++ source or getting the binary library from Si2? The directories don't include associated header files or exact compiler toolchain, but the file names do match the target link library references in BAG3's CBAG CMakeLists.txt file.","title":"Message from me:"},{"location":"bag_old_notes_delete_me/#message-from-hans","text":"/cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa/22.43p006/include/oa/ /cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa/22.50p001/include/oa/ /cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa/2.2.6/include/oa/ /cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa/22.41.004/include/oa/ /cadence/mentor/ixl_cal_2016.1_14.11/shared/pkgs/icv_oa/22.43p006/include/oa/ /cadence/mentor/ixl_cal_2016.1_14.11/shared/pkgs/icv_oa/22.50p001/include/oa/ /cadence/mentor/ixl_cal_2016.1_14.11/shared/pkgs/icv_oa/2.2.6/include/oa/ /cadence/mentor/ixl_cal_2016.1_14.11/shared/pkgs/icv_oa/22.41.004/include/oa/ /cadence/mentor/ixl_cal_2012.4_25.21/shared/pkgs/icv_oa/2.2.6/include/oa/ /cadence/mentor/ixl_cal_2012.4_25.21/shared/pkgs/icv_oa/latest/include/oa/ I think the header files from the different versions (except the 2.x) are very similar. I just briefly checked a few and they only differ in adding an overloaded function here and there or changes to the include list. They might work for a wide range of lib versions.","title":"Message from Hans:"},{"location":"bag_old_notes_delete_me/#etc","text":"setup.cfg is a config file, which is read in by setup.py and contains infor that setup.py otherwise could. (Setup.py is a replacement for setuptools, as it is being deprecated) config files like these oftentime also have the .ini file type or the the .conf file type.","title":"ETC"},{"location":"bag_old_notes_delete_me/#building-pybag","text":"OA_LINK_DIR To run CMake (make sure venv is active first) python -m pip install .","title":"Building Pybag:"},{"location":"bag_old_notes_delete_me/#reading-setuppy-and-both-cmakeliststxt-files","text":"at bottom of setup.py, setup() function starts process, ext_modules and cmdclass looks to be important setup.py is a older and more featured setup script, and setup.cfg and pyproject.toml are newer versions that are simpler and recommended unless you need the features of the older project. Setuptools can build C/C++ extension modules. The keyword argument ext_modules of setup() should be a list of instances of the setuptools.Extension class. So set the ext_modules keyword and inherit this class from the setuptools.Extension class, where we set a class CMakePyBind11Extension(Extension): def __init__(self, name, sourcedir=''): Extension.__init__(self, name, sources=[]) self.sourcedir = str(Path(sourcedir).resolve()) setup( package_dir{'': 'src'}, // fed to the ext_modules command below, as a base direcotory for extensions ext_modules=[CMakePyBind11Extension('all')], // this lists the specifics of extensions what is to be built cmdclass={'build_ext': CMakePyBind11Build}, // this actually runs the build extension operation ) Inside the CmakePyBind11Build object, there is a method called build_extension , which: calculates output directory for the build init cmake command w/: build source dir, temp build dir, output dir, build type, optional compiler launcher settings build cmake command run ./gen_stubs.sh","title":"Reading setup.py and both CMakeLists.txt files"},{"location":"bag_old_notes_delete_me/#running-setuppy-in-pybag-library","text":"\"pip install .\" But build.sh appears to not follow this logic Instead, run build.sh: # this script builds the C++ extension if [ -z ${OA_LINK_DIR+x} ] then echo \"OA_LINK_DIR is unset\" exit 1 fi if [ -z ${PYBAG_PYTHON+x} ] then echo \"PYBAG_PYTHON is unset\" exit 1 fi ${PYBAG_PYTHON} setup.py build The if [ -z ${VAR} ] command evaluates to true if VAR is null, which is what the -z does. as [ ] is short for the test command in bash the ${...} construct is parameter expansion Ah shit, it looks like this +x nonsense was deprecated as of Bash v4.3, which is why this isn't working. Let's just run setup.py build directly. Therefore, it's obvious that we need OA_LINK_DIR and PYBAG_PYTHON export PYBAG_PYTHON=.... /bin/python3 CMAKE_PREFIX_PATH used by CMake, and is set to fmt,Boost,yamp-cpp, spdlog directories. It normally set at a high level by the output tool in .bashrc files of BAG project. OA_INCLUDE_DIR used in cbag CMakeLists.txt, set in .bashrc OA_LINK_DIR used in cbag CMakeLists.txt, set in .bashrc export OA_LINK_DIR=${OA_SRC_ROOT}/lib/linux_rhel70_gcc83x_64/opt `sudo dnf install cmake` `sudo dnf install fmt-devel boost-devel spdlog-devel yaml-cpp-devel` to get the packages I want: sudo yum install fmt-devel boost-devel spdlog-devel yaml-cpp-devel You must do the setup in this order: python -m venv ~/designs/dmc65v2/.venv/ source /env/bin/activate then scl enable devtoolset-8 bash otherwise, the venv will clear the scl env. export PYBAG_PYTHON=/faust/user/kcaisley/designs/dmc65v2/.venv/bin/python export OA_SRC_ROOT=/cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa/22.50p001 export OA_LINK_DIR=${OA_SRC_ROOT}/lib/linux_rhel50_gcc44x_64/opt #notice these are build for rhel5, with gcc4 export OA_INCLUDE_DIR=${OA_SRC_ROOT}/include export CMAKE_PREFIX_PATH=/faust/user/kcaisley/packages/spdlog-1.x/cmake:${CMAKE_PREFIX_PATH} export CMAKE_PREFIX_PATH=/faust/user/kcaisley/packages/yaml-cpp-yaml-cpp-0.7.0:${CMAKE_PREFIX_PATH} Fist we will need to following libraries: * Boost * fmt * spdlog > 1.x * yaml-cpp To make them accessible to the CMake tool, we first need to make sure they are on our system (and of the devel variety), and then add something to the beginning of the CMake path variable: export CMAKE_PREFIX_PATH=____:${CMAKE_PREFIX_PATH} where ___ is the thing you want to add We are looking for the files of type: /usr/lib/x86_64-linux-gnu/cmake/fmt/fmt-config.cmake In this case, we would target the parent directory: export CMAKE_PREFIX_PATH=/usr/lib/x86_64-linux-gnu/cmake/fmt:${CMAKE_PREFIX_PATH} Okay, so this strategy of trying to use a bunch of manual downloads in not going to work... Let's move to Fedora. Both the spdlog-devel and fmt-devel have their XXXConfig.cmake files right there.","title":"Running setup.py in pybag library"},{"location":"bag_old_notes_delete_me/#trying-again-in-fedora","text":"inside /pybag, make sure you delete the _build folder, as it contains a cache of settings NO, IT'S FINE, I JUST DIDN'T HAVE THE ENV VAR PROPERLY SET. build.sh run script is uses deprecated commands, so we will need to launch setup.py manually You must do the setup in this order: sudo dnf install cmake sudo dnf install fmt-devel boost-devel spdlog-devel yaml-cpp-devel sudo dnf install gcc_c++ sudo dnf install python3.10 as h5py doesn't work on python 3.11 yet python3.10 -m venv ~/designs/dmc65v2/.venv as h5py doesn't work on python 3.11 yet python -m pip install wheel source .venv/bin/activate to activate environment export PYBAG_PYTHON=/faust/user/kcaisley/designs/dmc65v2/.venv/bin/python export OA_SRC_ROOT=/cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa/22.50p001 export OA_LINK_DIR=${OA_SRC_ROOT}/lib/linux_rhel50_gcc44x_64/opt #notice these are build for rhel5, with gcc4 export OA_INCLUDE_DIR=${OA_SRC_ROOT}/include export CXX=g++ from here","title":"Trying again in Fedora:"},{"location":"bag_old_notes_delete_me/#bag","text":"To install the top level BAG python lib simply navigate to this directory and execute: python -m pip install .","title":"BAG"},{"location":"bag_old_notes_delete_me/#pybag-cbag-pybind11","text":"To install the lower level pybag package tools navigate to the pybag directory and execute: python -m pip install . --log build.log","title":"PyBAG + CBAG + pybind11"},{"location":"bag_old_notes_delete_me/#diff-of-stuff-changed-in-cbag","text":"diff --git a/CMakeLists.txt b/CMakeLists.txt index 6ea5233..67896a4 100644 --- a/CMakeLists.txt +++ b/CMakeLists.txt @@ -202,7 +202,7 @@ set(SRC_FILES_LIB_CBAG_OA ${CMAKE_CURRENT_SOURCE_DIR}/src/cbag/oa/write.cpp ) - + if(DEFINED ENV{OA_LINK_DIR}) message(\"OA include directory: \" $ENV{OA_INCLUDE_DIR}) message(\"OA link directory: \" $ENV{OA_LINK_DIR}) @@ -226,6 +226,7 @@ target_link_libraries(cbag oaCommon oaPlugIn PRIVATE + spdlog::spdlog #https://bugzilla.redhat.com/show_bug.cgi?id=1851497 stdc++fs ${Boost_LIBRARIES} yaml-cpp diff --git a/include/cbag/common/typedefs.h b/include/cbag/common/typedefs.h index 7ce06b1..86dbba1 100644 --- a/include/cbag/common/typedefs.h +++ b/include/cbag/common/typedefs.h @@ -49,6 +49,9 @@ limitations under the License. #include <cstdint> #include <tuple> +#include <limits> //https://stackoverflow.com/questions/71296302/numeric-limits-is-not-a-member-of-std +#include <optional> //based on above comment +#include <boost/geometry.hpp> //https://github.com/pgRouting/pgrouting/issues/1825 namespace cbag { diff --git a/src/cbag/layout/path_util.cpp b/src/cbag/layout/path_util.cpp index b9a0802..def66aa 100644 --- a/src/cbag/layout/path_util.cpp +++ b/src/cbag/layout/path_util.cpp @@ -46,6 +46,7 @@ limitations under the License. #include <fmt/core.h> #include <fmt/ostream.h> +#include <fmt/format.h> // trying to add this code #include <cbag/layout/path_util.h> you make use git clean -dfx , that will remove all files not under source control. Be careful with this though, as it can delete newly created files if they haven't been added to source control yet. this assumes an in-source build, of course, which is a bad habit","title":"Diff of stuff changed in cbag"},{"location":"bag_old_notes_delete_me/#downgrading-packages-on-fedora","text":"","title":"Downgrading packages on Fedora:"},{"location":"bag_old_notes_delete_me/#error-with-fmt-package-during-build","text":"/usr/include/fmt/core.h:1757:7: error: static assertion failed: Cannot format an argument. To make type T formattable provide a formatter<T> specialization: https://fmt.dev/latest/api.html#udt Solution: It looks like other people have have problems with fmt To downgrade to fmt 8.1, we can use the koji build system: https://koji.fedoraproject.org/koji/buildinfo?buildID=1927503 looks like the newest version of spdlog (1.11) supports fmt 9.1, and so we need to downgrade to spdlog 1.10 to support fmt 8.1.1. here's the fmt/fmt-devel packages: https://koji.fedoraproject.org/koji/buildinfo?buildID=1927503 here's the spdlog/spdlog-devel packages: If a package is still in the repos, here's how you find and target it: https://unix.stackexchange.com/questions/266888/can-i-force-dnf-to-install-an-old-version-of-a-package dnf --showduplicates list <package> List packages that depend on the package of choice: dnf repoquery --installed --whatrequires qemu-kvm manually downgrade to a different package version downloaded from this link sudo dnf downgrade ~/Downloads/fmt-8.1.1-5.fc37.x86_64.rpm Had to remove gnome boxes, as it depends on fmt lib: sudo dnf remove gnome-boxes sudo dnf remove fmt fmt-devel spdlog spdlog-devel (march 02) I knew I needed older version of spdlog to be compatible with my older fmt, based on this wget https://kojipkgs.fedoraproject.org//packages/fmt/8.1.1/4.fc37/x86_64/fmt-8.1.1-4.fc37.x86_64.rpm https://kojipkgs.fedoraproject.org//packages/fmt/8.1.1/4.fc37/x86_64/fmt-devel-8.1.1-4.fc37.x86_64.rpm https://kojipkgs.fedoraproject.org//packages/spdlog/1.10.0/1.fc37/x86_64/spdlog-1.10.0-1.fc37.x86_64.rpm https://kojipkgs.fedoraproject.org//packages/spdlog/1.10.0/1.fc37/x86_64/spdlog-devel-1.10.0-1.fc37.x86_64.rpm -P ~/Downloads/ then simply manually install all of these rpms, assuming nothing else is in the download path sudo dnf install ~/Downloads/*.rpm","title":"Error with {fmt} package during build"},{"location":"bag_old_notes_delete_me/#error-with-boost-packages-during-build","text":"/faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/spirit/range.cpp:53:1: required from here /usr/include/boost/spirit/home/x3/operator/detail/sequence.hpp:144:25: error: static assertion failed: Size of the passed attribute is bigger than expected. 144 | actual_size <= expected_size | ~~~~~~~~~~~~^~~~~~~~~~~~~~~~ Let's try downgrading Boost and Boost devel: > sudo dnf remove boost boost-devel > wget https://kojipkgs.fedoraproject.org//packages/boost/1.76.0/10.fc37/x86_64/boost-1.76.0-10.fc37.x86_64.rpm https://kojipkgs.fedoraproject.org//packages/boost/1.76.0/10.fc37/x86_64/boost-devel-1.76.0-10.fc37.x86_64.rpm -P ~/Downloads/ Now I can't just manually install these now, because a couple core system components, namely gnome-shell require other boost libraries, like boost-system which an older version of boost wouldn't be compatible with.","title":"Error with Boost packages during build"},{"location":"bag_old_notes_delete_me/#fmt-and-spdlog","text":"fmt is a formatting library which provides a standardized, safe, fast way to construct strings. This is useful for output messages and dynamically generating filepaths in C++ code. spdlog, pronounced 'speed log', is a library built on top of {fmt} which allows for the","title":"fmt and spdlog"},{"location":"bag_old_notes_delete_me/#putting-more-effor-into-understanding-bag","text":"the implementation of cbag is as a header only library. It uses","title":"putting more effor into understanding BAG:"},{"location":"bag_old_notes_delete_me/#january-12-picking-back-up-work","text":"My current status ilss that I need to compile and install pybag, which depends on a bunch of different packages. Building the cbag subrepo though requires fmt , spdlog , and a collection of boost libs. The specific versions needed can't be accessed easily via the Fedora fedora dnf repos, and so I fear I'll just need to manually download and build the projects from source. First, let's examine what we currently have installed, and specify exactly what we need to download.","title":"January 12: picking back up work"},{"location":"bag_old_notes_delete_me/#january-20-continuing-with-development","text":"I think looking at how cbag and cbag_polygon are built first might be helpful in this case. One must understand that CMake is a build system, in that it simple generates build files which are then compiles with a system's native build environment. Running just the CMake build command gives me several clues: In the CMakeLists.txt file, I see a control flow statement for: if(DEFINED ENV{OA_LINK_DIR}) I've check it, and it's not evaluating, the else() statment below is instead.","title":"January 20: continuing with development"},{"location":"bag_old_notes_delete_me/#workflow","text":"[kcaisley@asiclab008 ~]$ cd designs/dmc65v2/ [kcaisley@asiclab008 dmc65v2]$ source setup.sh (.venv) [kcaisley@asiclab008 dmc65v2]$ cd bag/pybag/cbag/ (.venv) [kcaisley@asiclab008 cbag]$ rm -r _build (.venv) [kcaisley@asiclab008 cbag]$ export BUILD_TYPE=${1:-Debug} (.venv) [kcaisley@asiclab008 cbag]$ cmake -H. -Bbuild -DCMAKE_BUILD_TYPE=${BUILD_TYPE} -DCMAKE_CXX_COMPILER_LAUNCHER=ccache Where -H This internal option is not documented but widely used by community and Has been replaced in 3.13 with the official source directory flag of -S. there is no space place after for the directory, so the . is the local location -B Starting with CMake 3.13, -B is an officially supported flag, can handle spaces correctly and can be used independently of the -S or -H options. again, there should be nospace, so _build is the build directory name -D -D : = Create a cmake cache entry. When cmake is first run in an empty build tree, it creates a CMakeCache.txt file and populates it with customizable settings for the project. This option may be used to specify a setting that takes priority over the project's default value. The option may be repeated for as many cache entries as desired. Again, no space! There are hundreds of options in this file, but this comman above is specifically setting: CMAKE_BUILD_TYPE which is being set to 'Debug' CMAKE_CXX_COMPILER_LAUNCHER which passes the 'ccache' option to the makefiles generator, no idea relaly that this does || exit suffix should be removed, as this is being run on the command line, and this will cause the terminal to exit if the command fails Now I just need to understand why the WARNING: building without OpenAccess support. path is running..... I think the issue is that there is some complexity to CMake variable/environment variable scope system. Yes, this is the issues. Those unix bash environment variables are not visible inside the CMake scope. Let's figure out how this can be set. It's not being passed down from higher level CMake script, so it must be in the way that the command to build is called. cmake -H. -B_build -DCMAKE_BUILD_TYPE=${BUILD_TYPE} -DCMAKE_CXX_COMPILER_LAUNCHER=ccache -DOA_LINK_DIR=/cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa/22.50p001/lib/linux_rhel50_gcc44x_64/opt The CMakeCache.txt File After the first configuration of a project, CMake persists variable information in a text file called CMakeCache.txt. Caches are used to improve. When CMake is re-run on a project the cache is read before starting so that some re-parsing time can be saved on CMakeLists.txt. Here is where passing parameters to CMake befuddles a beginner. If a variable is passed via the command line that variable is stored in the cache. Accessing that variable on future runs of CMake will always get the value stored inside the cache and new value passed through the command line are ignored. To make CMake take the new value passed through the command line the first value have to be explicitly undefined. Like so: cmake -U <previously defined variable> -D <previously defined variable>[=new value] (A better approach is to use CMake internal variables. For more information refer the CMake manual here.) The syntax if(DEFINED <name>|CACHE{<name>}|ENV{<name>}) is true if a variable, cache variable or environment variable with given is defined. The value of the variable does not matter. Environment Variables are like ordinary Variables, with the following differences: * Environment variables have global scope in a CMake process. They are never cached. * Variable References have the form $ENV{ }, using the ENV operator. * Initial values of the CMake environment variables are those of the calling process. Values can be changed using the set() and unset() commands. These commands only affect the running CMake process, not the system environment at large. Changed values are not written back to the calling process, and they are not seen by subsequent build or test processes. * See the cmake -E env command-line tool to run a command in a modified environment. * Source: https://cmake.org/cmake/help/latest/manual/cmake-language.7.html#cmake-language-environment-variables This isn't working because I'm setting regular variables, but the check is for an environment variable. Ohhhhh. If you set an vairable, just by writing it, it will only be valid in that shell. It's called a variable. If you write 'export' first though, it's now and 'environment variable' and will be active in all child processes. Now, if it's inside a script (which is run as a child process), and if you want to access it inside another child process, you need to make sure that first you 'source' the script, so that the contents of the script re available in the parent bash session, but then you also need to add the 'export' command, if you want it to be available in a child pocess like CMake. So in short the 'source' command makes bash script which would otherwise be child process, instead run in the parent bash session. And 'export' makes all variables defined in the parent process also defined in the child processes.","title":"Workflow:"},{"location":"bag_old_notes_delete_me/#fully-documented-build-problems","text":"[kcaisley@asiclab008 ~]$ cd designs/dmc65v2/ [kcaisley@asiclab008 dmc65v2]$ source setup.sh (.venv) [kcaisley@asiclab008 dmc65v2]$ cd bag/pybag/cbag/ (.venv) [kcaisley@asiclab008 cbag]$ mkdir build (.venv) [kcaisley@asiclab008 cbag]$ rm -r build/* (.venv) [kcaisley@asiclab008 cbag]$ cd build (.venv) [kcaisley@asiclab008 cbag]$ cmake ../ (.venv) [kcaisley@asiclab008 cbag]$ cd .. (.venv) [kcaisley@asiclab008 cbag]$ cmake --build build","title":"Fully documented build problems:"},{"location":"bag_old_notes_delete_me/#problem-1","text":"(.venv) [kcaisley@asiclab008 build]$ cmake --build . [ 1%] Building CXX object CMakeFiles/cbag.dir/src/cbag/common/box_collection.cpp.o In file included from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/common/box_t.h:50, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/common/box_array.h:50, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/common/box_collection.h:23, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/common/box_collection.cpp:18: /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/common/typedefs.h:73:33: error: \u2018numeric_limits\u2019 is not a member of \u2018std\u2019 73 | constexpr auto COORD_MIN = std::numeric_limits<coord_t>::min(); | ^~~~~~~~~~~~~~ /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/common/typedefs.h:73:55: error: expected primary-expression before \u2018>\u2019 token 73 | constexpr auto COORD_MIN = std::numeric_limits<coord_t>::min(); | ^ compilation terminated due to -fmax-errors=2. gmake[2]: *** [CMakeFiles/cbag.dir/build.make:76: CMakeFiles/cbag.dir/src/cbag/common/box_collection.cpp.o] Error 1 gmake[1]: *** [CMakeFiles/Makefile2:124: CMakeFiles/cbag.dir/all] Error 2 gmake: *** [Makefile:136: all] Error 2 When including files, if it's just a simple world like #include <tuple> , then this is probably a C++ standard lib component. If it is formatted like #include <boost/geometry.hpp> , then this is a good clue that the library isn't standard due to the .hpp suffix, and also that the libary is written in C++. If you have one like #include <limits.h> , we know this is written in C, and there is a good chance this is actually part of the C standard library, which is included in the C++ std lib but is deprecated and not advisable to use. The fix was including the <limits> standard library header in the offending file, so that std::numeric_limits is defined. One thing I learned from the above error log is that we can see the trace of imports that lead to the offending file. We tried to compile src/cbag/common/box_collection.cpp but the error was in cbag/include/cbag/common/typedefs.h . We we read the \"build line first\", then the \"In file included from\" bottom to top\", then finally the lowest section with the \"error\" to find our problem. In this case, our solution was adding a single line: kcaisley@asiclab008 cbag]$ git diff include/cbag/common/typedefs.h diff --git a/include/cbag/common/typedefs.h b/include/cbag/common/typedefs.h index 7ce06b1..ed526cb 100644 --- a/include/cbag/common/typedefs.h +++ b/include/cbag/common/typedefs.h @@ -49,6 +49,7 @@ limitations under the License. #include <cstdint> #include <tuple> +#include <limits> //needed for std::numeric_limits NOTE: I believe that <cstdint> defines primitive types like int32_t . Like the other std lib headers, gcc stores them at /usr/include/c++/12","title":"Problem 1:"},{"location":"bag_old_notes_delete_me/#problem-2","text":"(.venv) [kcaisley@asiclab008 cbag]$ cmake --build build [ 1%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/main.cpp.o In file included from /usr/include/spdlog/common.h:45, from /usr/include/spdlog/spdlog.h:12, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/logging/spdlog.h:53, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/logging/logging.h:55, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/gdsii/read.h:58, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/gdsii/main.cpp:25: /usr/include/spdlog/fmt/fmt.h:27:14: fatal error: spdlog/fmt/bundled/core.h: No such file or directory 27 | # include <spdlog/fmt/bundled/core.h> | ^~~~~~~~~~~~~~~~~~~~~~~~~~~ compilation terminated. gmake[2]: *** [CMakeFiles/cbag.dir/build.make:104: CMakeFiles/cbag.dir/src/cbag/gdsii/main.cpp.o] Error 1 gmake[1]: *** [CMakeFiles/Makefile2:124: CMakeFiles/cbag.dir/all] Error 2 As mentioned at this link this error crops up if the CMakeLists.txt for a project doesn't correct set all the flags for spdlog and fmt. In this case, spdlog was erroneously omitted from the target_link_libraries section. A simple fix: diff --git a/CMakeLists.txt b/CMakeLists.txt index 6ea5233..e8f1408 100644 --- a/CMakeLists.txt +++ b/CMakeLists.txt @@ -201,8 +201,7 @@ set(SRC_FILES_LIB_CBAG_OA ${CMAKE_CURRENT_SOURCE_DIR}/src/cbag/oa/util.cpp ${CMAKE_CURRENT_SOURCE_DIR}/src/cbag/oa/write.cpp ) - - + if(DEFINED ENV{OA_LINK_DIR}) message(\"OA include directory: \" $ENV{OA_INCLUDE_DIR}) message(\"OA link directory: \" $ENV{OA_LINK_DIR}) @@ -226,6 +225,7 @@ target_link_libraries(cbag oaCommon oaPlugIn PRIVATE + spdlog::spdlog #https://bugzilla.redhat.com/show_bug.cgi?id=1851497 stdc++fs ${Boost_LIBRARIES}","title":"Problem #2"},{"location":"bag_old_notes_delete_me/#problem-3","text":"(.venv) [kcaisley@asiclab008 cbag]$ cmake --build build Interprocedural optimization disabled -- Found Boost: /usr/lib64/cmake/Boost-1.78.0/BoostConfig.cmake (found version \"1.78.0\") Interprocedural optimization disabled install prefix: /usr/local install rpath: OA include directory: /cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa/22.50p001/include OA link directory: /cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa/22.50p001/lib/linux_rhel50_gcc44x_64/opt -- Found Boost: /usr/lib64/cmake/Boost-1.78.0/BoostConfig.cmake (found version \"1.78.0\") found components: serialization -- Configuring done -- Generating done -- Build files have been written to: /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/build [ 1%] Building CXX object CMakeFiles/cbag.dir/src/cbag/common/box_collection.cpp.o [ 2%] Building CXX object CMakeFiles/cbag.dir/src/cbag/common/transformation_util.cpp.o [ 3%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/main.cpp.o [ 4%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/math.cpp.o [ 5%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/parse_map.cpp.o [ 7%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/read.cpp.o [ 8%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/read_util.cpp.o [ 9%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/write.cpp.o [ 10%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/write_util.cpp.o [ 11%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/blockage.cpp.o [ 12%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/boundary.cpp.o [ 14%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/cellview.cpp.o In file included from /usr/include/boost/geometry/index/rtree.hpp:30, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/cbag_polygon/include/cbag/polygon/geo_index.h:54, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/layout/cellview_fwd.h:59, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/layout/cellview.h:52, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/layout/cellview.cpp:55: /usr/include/boost/geometry/strategies/relate/services.hpp: In instantiation of \u2018struct boost::geometry::strategies::relate::services::default_strategy<boost::geometry::model::box<boost::geometry::model::point<int, 2, boost::geometry::cs::cartesian> >, cbag::polygon::rectangle_data<int>, boost::geometry::cartesian_tag, boost::geometry::cartesian_tag>\u2019: /usr/include/boost/geometry/algorithms/detail/disjoint/interface.hpp:92:21: required from \u2018static bool boost::geometry::resolve_strategy::disjoint<boost::geometry::default_strategy, false>::apply(const Geometry1&, const Geometry2&, boost::geometry::default_strategy) [with Geometry1 = boost::geometry::model::box<boost::geometry::model::point<int, 2, boost::geometry::cs::cartesian> >; Geometry2 = cbag::polygon::rectangle_data<int>]\u2019 /usr/include/boost/geometry/algorithms/detail/disjoint/interface.hpp:129:21: required from \u2018static bool boost::geometry::resolve_dynamic::disjoint<Geometry1, Geometry2, IsDynamic, IsCollection>::apply(const Geometry1&, const Geometry2&, const Strategy&) [with Strategy = boost::geometry::default_strategy; Geometry1 = boost::geometry::model::box<boost::geometry::model::point<int, 2, boost::geometry::cs::cartesian> >; Geometry2 = cbag::polygon::rectangle_data<int>; bool IsDynamic = false; bool IsCollection = false]\u2019 /usr/include/boost/geometry/algorithms/detail/disjoint/interface.hpp:231:21: required from \u2018bool boost::geometry::disjoint(const Geometry1&, const Geometry2&) [with Geometry1 = model::box<model::point<int, 2, cs::cartesian> >; Geometry2 = cbag::polygon::rectangle_data<int>]\u2019 /usr/include/boost/geometry/algorithms/detail/intersects/interface.hpp:108:32: required from \u2018bool boost::geometry::intersects(const Geometry1&, const Geometry2&) [with Geometry1 = model::box<model::point<int, 2, cs::cartesian> >; Geometry2 = cbag::polygon::rectangle_data<int>]\u2019 /usr/include/boost/geometry/index/detail/predicates.hpp:218:36: required from \u2018static bool boost::geometry::index::detail::spatial_predicate_intersects<G1, G2, Tag1, Tag2>::apply(const G1&, const G2&, const S&) [with S = boost::geometry::default_strategy; G1 = boost::geometry::model::box<boost::geometry::model::point<int, 2, boost::geometry::cs::cartesian> >; G2 = cbag::polygon::rectangle_data<int>; Tag1 = boost::geometry::box_tag; Tag2 = boost::geometry::box_tag]\u2019 /usr/include/boost/geometry/index/detail/predicates.hpp:243:59: [ skipping 5 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ] /usr/include/boost/geometry/index/detail/rtree/visitors/spatial_query.hpp:87:21: required from \u2018boost::geometry::index::detail::rtree::visitors::spatial_query<MembersHolder, Predicates, OutIter>::size_type boost::geometry::index::detail::rtree::visitors::spatial_query<MembersHolder, Predicates, OutIter>::apply(const MembersHolder&) [with MembersHolder = boost::geometry::index::rtree<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int>, boost::geometry::index::quadratic<32, 16>, boost::geometry::index::indexable<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >, boost::geometry::index::equal_to<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >, boost::container::new_allocator<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> > >::members_holder; Predicates = boost::geometry::index::detail::predicates::spatial_predicate<cbag::polygon::rectangle_data<int>, boost::geometry::index::detail::predicates::intersects_tag, false>; OutIter = cbag::util::lambda_output_iterator<cbag::polygon::index::geo_index<int>::get_intersect<cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> > >(cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> >, const box_type&, coordinate_type, coordinate_type, bool, const cbag::polygon::transformation<int>&) const::<lambda(const cbag::polygon::index::geo_index<int>::tree_value_type&)> >; size_type = long unsigned int]\u2019 /usr/include/boost/geometry/index/rtree.hpp:1861:27: required from \u2018boost::geometry::index::rtree<Value, Options, IndexableGetter, EqualTo, Allocator>::size_type boost::geometry::index::rtree<Value, Options, IndexableGetter, EqualTo, Allocator>::query_dispatch(const Predicates&, OutIter) const [with Predicates = boost::geometry::index::detail::predicates::spatial_predicate<cbag::polygon::rectangle_data<int>, boost::geometry::index::detail::predicates::intersects_tag, false>; OutIter = cbag::util::lambda_output_iterator<cbag::polygon::index::geo_index<int>::get_intersect<cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> > >(cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> >, const box_type&, coordinate_type, coordinate_type, bool, const cbag::polygon::transformation<int>&) const::<lambda(const cbag::polygon::index::geo_index<int>::tree_value_type&)> >; typename std::enable_if<(boost::geometry::index::detail::predicates_count_distance<Predicates>::value == 0), int>::type <anonymous> = 0; Value = std::pair<cbag::polygon::rectangle_data<int>, long unsigned int>; Parameters = boost::geometry::index::quadratic<32, 16>; IndexableGetter = boost::geometry::index::indexable<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; EqualTo = boost::geometry::index::equal_to<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; Allocator = boost::container::new_allocator<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; size_type = long unsigned int]\u2019 /usr/include/boost/geometry/index/rtree.hpp:1083:30: required from \u2018boost::geometry::index::rtree<Value, Options, IndexableGetter, EqualTo, Allocator>::size_type boost::geometry::index::rtree<Value, Options, IndexableGetter, EqualTo, Allocator>::query(const Predicates&, OutIter) const [with Predicates = boost::geometry::index::detail::predicates::spatial_predicate<cbag::polygon::rectangle_data<int>, boost::geometry::index::detail::predicates::intersects_tag, false>; OutIter = cbag::util::lambda_output_iterator<cbag::polygon::index::geo_index<int>::get_intersect<cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> > >(cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> >, const box_type&, coordinate_type, coordinate_type, bool, const cbag::polygon::transformation<int>&) const::<lambda(const cbag::polygon::index::geo_index<int>::tree_value_type&)> >; Value = std::pair<cbag::polygon::rectangle_data<int>, long unsigned int>; Parameters = boost::geometry::index::quadratic<32, 16>; IndexableGetter = boost::geometry::index::indexable<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; EqualTo = boost::geometry::index::equal_to<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; Allocator = boost::container::new_allocator<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; size_type = long unsigned int]\u2019 /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/cbag_polygon/include/cbag/polygon/geo_index.h:212:21: required from \u2018void cbag::polygon::index::geo_index<T>::get_intersect(OutIter, const box_type&, coordinate_type, coordinate_type, bool, const cbag::polygon::transformation<T>&) const [with OutIter = cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> >; T = int; box_type = cbag::polygon::rectangle_data<int>; coordinate_type = int]\u2019 /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/cbag_polygon/include/cbag/polygon/geo_index.h:244:22: required from \u2018void cbag::polygon::index::apply_intersect(const geo_index<T>&, Fun, const typename geo_index<T>::box_type&, T, T, bool, const cbag::polygon::transformation<T>&) [with T = int; Fun = cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)>; typename geo_index<T>::box_type = cbag::polygon::rectangle_data<int>]\u2019 /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/layout/cellview.cpp:137:24: required from here /usr/include/boost/geometry/strategies/relate/services.hpp:36:5: error: static assertion failed: Not implemented for this Geometry's coordinate system. 36 | BOOST_GEOMETRY_STATIC_ASSERT_FALSE( | ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ /usr/include/boost/geometry/strategies/relate/services.hpp:36:5: note: \u2018std::integral_constant<bool, false>::value\u2019 evaluates to false In file included from /usr/include/boost/geometry/index/rtree.hpp:34: /usr/include/boost/geometry/algorithms/detail/disjoint/interface.hpp: In instantiation of \u2018static bool boost::geometry::resolve_strategy::disjoint<boost::geometry::default_strategy, false>::apply(const Geometry1&, const Geometry2&, boost::geometry::default_strategy) [with Geometry1 = boost::geometry::model::box<boost::geometry::model::point<int, 2, boost::geometry::cs::cartesian> >; Geometry2 = cbag::polygon::rectangle_data<int>]\u2019: /usr/include/boost/geometry/algorithms/detail/disjoint/interface.hpp:129:21: required from \u2018static bool boost::geometry::resolve_dynamic::disjoint<Geometry1, Geometry2, IsDynamic, IsCollection>::apply(const Geometry1&, const Geometry2&, const Strategy&) [with Strategy = boost::geometry::default_strategy; Geometry1 = boost::geometry::model::box<boost::geometry::model::point<int, 2, boost::geometry::cs::cartesian> >; Geometry2 = cbag::polygon::rectangle_data<int>; bool IsDynamic = false; bool IsCollection = false]\u2019 /usr/include/boost/geometry/algorithms/detail/disjoint/interface.hpp:231:21: required from \u2018bool boost::geometry::disjoint(const Geometry1&, const Geometry2&) [with Geometry1 = model::box<model::point<int, 2, cs::cartesian> >; Geometry2 = cbag::polygon::rectangle_data<int>]\u2019 /usr/include/boost/geometry/algorithms/detail/intersects/interface.hpp:108:32: required from \u2018bool boost::geometry::intersects(const Geometry1&, const Geometry2&) [with Geometry1 = model::box<model::point<int, 2, cs::cartesian> >; Geometry2 = cbag::polygon::rectangle_data<int>]\u2019 /usr/include/boost/geometry/index/detail/predicates.hpp:218:36: required from \u2018static bool boost::geometry::index::detail::spatial_predicate_intersects<G1, G2, Tag1, Tag2>::apply(const G1&, const G2&, const S&) [with S = boost::geometry::default_strategy; G1 = boost::geometry::model::box<boost::geometry::model::point<int, 2, boost::geometry::cs::cartesian> >; G2 = cbag::polygon::rectangle_data<int>; Tag1 = boost::geometry::box_tag; Tag2 = boost::geometry::box_tag]\u2019 /usr/include/boost/geometry/index/detail/predicates.hpp:243:59: required from \u2018static bool boost::geometry::index::detail::spatial_predicate_call<boost::geometry::index::detail::predicates::intersects_tag>::apply(const G1&, const G2&, const S&) [with G1 = boost::geometry::model::box<boost::geometry::model::point<int, 2, boost::geometry::cs::cartesian> >; G2 = cbag::polygon::rectangle_data<int>; S = boost::geometry::default_strategy]\u2019 /usr/include/boost/geometry/index/detail/predicates.hpp:364:73: [ skipping 4 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ] /usr/include/boost/geometry/index/detail/rtree/visitors/spatial_query.hpp:87:21: required from \u2018boost::geometry::index::detail::rtree::visitors::spatial_query<MembersHolder, Predicates, OutIter>::size_type boost::geometry::index::detail::rtree::visitors::spatial_query<MembersHolder, Predicates, OutIter>::apply(const MembersHolder&) [with MembersHolder = boost::geometry::index::rtree<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int>, boost::geometry::index::quadratic<32, 16>, boost::geometry::index::indexable<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >, boost::geometry::index::equal_to<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >, boost::container::new_allocator<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> > >::members_holder; Predicates = boost::geometry::index::detail::predicates::spatial_predicate<cbag::polygon::rectangle_data<int>, boost::geometry::index::detail::predicates::intersects_tag, false>; OutIter = cbag::util::lambda_output_iterator<cbag::polygon::index::geo_index<int>::get_intersect<cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> > >(cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> >, const box_type&, coordinate_type, coordinate_type, bool, const cbag::polygon::transformation<int>&) const::<lambda(const cbag::polygon::index::geo_index<int>::tree_value_type&)> >; size_type = long unsigned int]\u2019 /usr/include/boost/geometry/index/rtree.hpp:1861:27: required from \u2018boost::geometry::index::rtree<Value, Options, IndexableGetter, EqualTo, Allocator>::size_type boost::geometry::index::rtree<Value, Options, IndexableGetter, EqualTo, Allocator>::query_dispatch(const Predicates&, OutIter) const [with Predicates = boost::geometry::index::detail::predicates::spatial_predicate<cbag::polygon::rectangle_data<int>, boost::geometry::index::detail::predicates::intersects_tag, false>; OutIter = cbag::util::lambda_output_iterator<cbag::polygon::index::geo_index<int>::get_intersect<cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> > >(cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> >, const box_type&, coordinate_type, coordinate_type, bool, const cbag::polygon::transformation<int>&) const::<lambda(const cbag::polygon::index::geo_index<int>::tree_value_type&)> >; typename std::enable_if<(boost::geometry::index::detail::predicates_count_distance<Predicates>::value == 0), int>::type <anonymous> = 0; Value = std::pair<cbag::polygon::rectangle_data<int>, long unsigned int>; Parameters = boost::geometry::index::quadratic<32, 16>; IndexableGetter = boost::geometry::index::indexable<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; EqualTo = boost::geometry::index::equal_to<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; Allocator = boost::container::new_allocator<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; size_type = long unsigned int]\u2019 /usr/include/boost/geometry/index/rtree.hpp:1083:30: required from \u2018boost::geometry::index::rtree<Value, Options, IndexableGetter, EqualTo, Allocator>::size_type boost::geometry::index::rtree<Value, Options, IndexableGetter, EqualTo, Allocator>::query(const Predicates&, OutIter) const [with Predicates = boost::geometry::index::detail::predicates::spatial_predicate<cbag::polygon::rectangle_data<int>, boost::geometry::index::detail::predicates::intersects_tag, false>; OutIter = cbag::util::lambda_output_iterator<cbag::polygon::index::geo_index<int>::get_intersect<cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> > >(cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> >, const box_type&, coordinate_type, coordinate_type, bool, const cbag::polygon::transformation<int>&) const::<lambda(const cbag::polygon::index::geo_index<int>::tree_value_type&)> >; Value = std::pair<cbag::polygon::rectangle_data<int>, long unsigned int>; Parameters = boost::geometry::index::quadratic<32, 16>; IndexableGetter = boost::geometry::index::indexable<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; EqualTo = boost::geometry::index::equal_to<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; Allocator = boost::container::new_allocator<std::pair<cbag::polygon::rectangle_data<int>, long unsigned int> >; size_type = long unsigned int]\u2019 /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/cbag_polygon/include/cbag/polygon/geo_index.h:212:21: required from \u2018void cbag::polygon::index::geo_index<T>::get_intersect(OutIter, const box_type&, coordinate_type, coordinate_type, bool, const cbag::polygon::transformation<T>&) const [with OutIter = cbag::util::lambda_output_iterator<cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)> >; T = int; box_type = cbag::polygon::rectangle_data<int>; coordinate_type = int]\u2019 /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/cbag_polygon/include/cbag/polygon/geo_index.h:244:22: required from \u2018void cbag::polygon::index::apply_intersect(const geo_index<T>&, Fun, const typename geo_index<T>::box_type&, T, T, bool, const cbag::polygon::transformation<T>&) [with T = int; Fun = cbag::layout::cellview::helper::get_ip_fill_intvs(const cbag::layout::geo_index_t&, const cbag::box_t&, cbag::coord_t, cbag::coord_t, int, cbag::offset_t)::<lambda(const auto:80&)>; typename geo_index<T>::box_type = cbag::polygon::rectangle_data<int>]\u2019 /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/layout/cellview.cpp:137:24: required from here /usr/include/boost/geometry/algorithms/detail/disjoint/interface.hpp:92:21: error: no type named \u2018type\u2019 in \u2018struct boost::geometry::strategies::relate::services::default_strategy<boost::geometry::model::box<boost::geometry::model::point<int, 2, boost::geometry::cs::cartesian> >, cbag::polygon::rectangle_data<int>, boost::geometry::cartesian_tag, boost::geometry::cartesian_tag>\u2019 92 | >::type strategy_type; | ^~~~~~~~~~~~~ compilation terminated due to -fmax-errors=2. gmake[2]: *** [CMakeFiles/cbag.dir/build.make:230: CMakeFiles/cbag.dir/src/cbag/layout/cellview.cpp.o] Error 1 gmake[1]: *** [CMakeFiles/Makefile2:124: CMakeFiles/cbag.dir/all] Error 2 Solution was to include , as discussed in this issues [kcaisley@asiclab008 cbag]$ git diff include/cbag/common/typedefs.h diff --git a/include/cbag/common/typedefs.h b/include/cbag/common/typedefs.h index 7ce06b1..86dbba1 100644 --- a/include/cbag/common/typedefs.h +++ b/include/cbag/common/typedefs.h @@ -49,6 +49,9 @@ limitations under the License. #include <cstdint> #include <tuple> +#include <limits> //https://stackoverflow.com/questions/71296302/numeric-limits-is-not-a-member-of-std +#include <boost/geometry.hpp> //https://github.com/pgRouting/pgrouting/issues/1825 NOTE: As you can see above, this is the second line we've had to add to this file.","title":"Problem #3"},{"location":"bag_old_notes_delete_me/#problem-4","text":"(.venv) [kcaisley@asiclab008 cbag]$ cmake --build build [ 1%] Building CXX object CMakeFiles/cbag.dir/src/cbag/common/box_collection.cpp.o [ 2%] Building CXX object CMakeFiles/cbag.dir/src/cbag/common/transformation_util.cpp.o [ 3%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/main.cpp.o [ 4%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/parse_map.cpp.o [ 5%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/read.cpp.o [ 7%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/read_util.cpp.o [ 8%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/write.cpp.o [ 9%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/write_util.cpp.o [ 10%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/blockage.cpp.o [ 11%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/boundary.cpp.o [ 12%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/cellview.cpp.o [ 14%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/cellview_poly.cpp.o [ 15%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/cellview_util.cpp.o [ 16%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/grid_object.cpp.o [ 17%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/instance.cpp.o [ 18%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/label.cpp.o [ 20%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/len_info.cpp.o [ 21%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/lp_lookup.cpp.o In file included from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/layout/lp_lookup.cpp:51: /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/layout/lp_lookup.h:85:10: error: \u2018optional\u2019 in namespace \u2018std\u2019 does not name a template type 85 | std::optional<lay_t> get_layer_id(const std::string &layer) const; | ^~~~~~~~ /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/layout/lp_lookup.h:53:1: note: \u2018std::optional\u2019 is defined in header \u2018<optional>\u2019; did you forget to \u2018#include <optional>\u2019? 52 | #include <cbag/common/typedefs.h> +++ |+#include <optional> 53 | /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/layout/lp_lookup.h:87:10: error: \u2018optional\u2019 in namespace \u2018std\u2019 does not name a template type 87 | std::optional<purp_t> get_purpose_id(const std::string &purpose) const; | ^~~~~~~~ /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/layout/lp_lookup.h:87:5: note: \u2018std::optional\u2019 is defined in header \u2018<optional>\u2019; did you forget to \u2018#include <optional>\u2019? 87 | std::optional<purp_t> get_purpose_id(const std::string &purpose) const; | ^~~ compilation terminated due to -fmax-errors=2. gmake[2]: *** [CMakeFiles/cbag.dir/build.make:328: CMakeFiles/cbag.dir/src/cbag/layout/lp_lookup.cpp.o] Error 1 gmake[1]: *** [CMakeFiles/Makefile2:124: CMakeFiles/cbag.dir/all] Error 2 The solution is described right there in the message, we forgot std::optional is defined in header <optional> ; yep we forget to #include <optional> in cbag/include/cbag/layout/lp_lookup.h . (.venv) [kcaisley@asiclab008 cbag]$ git diff include/cbag/layout/lp_lookup.h diff --git a/include/cbag/layout/lp_lookup.h b/include/cbag/layout/lp_lookup.h index 3f21bb9..cd6771d 100644 --- a/include/cbag/layout/lp_lookup.h +++ b/include/cbag/layout/lp_lookup.h @@ -48,7 +48,7 @@ limitations under the License. #define CBAG_COMMON_LP_LOOKUP_H #include <unordered_map> - +#include <optional> #include <cbag/common/typedefs.h>","title":"Problem #4"},{"location":"bag_old_notes_delete_me/#problem-5","text":"(.venv) [kcaisley@asiclab008 cbag]$ cmake --build build [ 1%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/main.cpp.o [ 2%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/parse_map.cpp.o [ 3%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/read.cpp.o [ 4%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/read_util.cpp.o [ 5%] Building CXX object CMakeFiles/cbag.dir/src/cbag/gdsii/write.cpp.o [ 7%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/cellview.cpp.o [ 8%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/cellview_poly.cpp.o [ 9%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/cellview_util.cpp.o [ 10%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/grid_object.cpp.o [ 11%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/instance.cpp.o [ 12%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/lp_lookup.cpp.o [ 14%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/path.cpp.o [ 15%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/path_util.cpp.o In file included from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/layout/path_util.cpp:47: /usr/include/fmt/core.h: In instantiation of \u2018constexpr fmt::v9::detail::value<Context> fmt::v9::detail::make_value(T&&) [with Context = fmt::v9::basic_format_context<fmt::v9::appender, char>; T = cbag::layout::vector45&]\u2019: /usr/include/fmt/core.h:1777:29: required from \u2018constexpr fmt::v9::detail::value<Context> fmt::v9::detail::make_arg(T&&) [with bool IS_PACKED = true; Context = fmt::v9::basic_format_context<fmt::v9::appender, char>; type <anonymous> = fmt::v9::detail::type::custom_type; T = cbag::layout::vector45&; typename std::enable_if<IS_PACKED, int>::type <anonymous> = 0]\u2019 /usr/include/fmt/core.h:1901:77: required from \u2018constexpr fmt::v9::format_arg_store<Context, Args>::format_arg_store(T&& ...) [with T = {cbag::layout::vector45&}; Context = fmt::v9::basic_format_context<fmt::v9::appender, char>; Args = {cbag::layout::vector45}]\u2019 /usr/include/fmt/core.h:1918:31: required from \u2018constexpr fmt::v9::format_arg_store<Context, typename std::remove_cv<typename std::remove_reference<Args>::type>::type ...> fmt::v9::make_format_args(Args&& ...) [with Context = basic_format_context<appender, char>; Args = {cbag::layout::vector45&}]\u2019 /usr/include/fmt/core.h:3206:44: required from \u2018std::string fmt::v9::format(format_string<T ...>, T&& ...) [with T = {cbag::layout::vector45&}; std::string = std::__cxx11::basic_string<char>; format_string<T ...> = basic_format_string<char, cbag::layout::vector45&>]\u2019 /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/layout/path_util.cpp:123:48: required from here /usr/include/fmt/core.h:1757:7: error: static assertion failed: Cannot format an argument. To make type T formattable provide a formatter<T> specialization: https://fmt.dev/latest/api.html#udt 1757 | formattable, | ^~~~~~~~~~~ /usr/include/fmt/core.h:1757:7: note: \u2018formattable\u2019 evaluates to false gmake[2]: *** [CMakeFiles/cbag.dir/build.make:356: CMakeFiles/cbag.dir/src/cbag/layout/path_util.cpp.o] Error 1 gmake[1]: *** [CMakeFiles/Makefile2:124: CMakeFiles/cbag.dir/all] Error 2 gmake: *** [Makefile:136: all] Error 2 I think a similar problem is seen here: https://github.com/fmtlib/fmt/issues/3034 Seems I need to made a change to the function call, to meet this latest issue: https://fmt.dev/latest/api.html#udt I simply commented out the line: throw std::invalid_argument(fmt::format(\"path segment vector {} not valid\", p_norm)); And it proceeded until 52% compilation and finally an unused fix..... diff --git a/src/cbag/layout/path_util.cpp b/src/cbag/layout/path_util.cpp index b9a0802..3c62df2 100644 --- a/src/cbag/layout/path_util.cpp +++ b/src/cbag/layout/path_util.cpp @@ -46,6 +46,7 @@ limitations under the License. #include <fmt/core.h> #include <fmt/ostream.h> +//#include <fmt/format.h> // trying to add this code #include <cbag/layout/path_util.h>","title":"Problem #5"},{"location":"bag_old_notes_delete_me/#problem-6","text":"(.venv) [kcaisley@asiclab008 cbag]$ cmake --build build [ 1%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/path_util.cpp.o [ 2%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/pin.cpp.o [ 3%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/routing_grid.cpp.o [ 4%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/routing_grid_util.cpp.o [ 5%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/tech.cpp.o [ 7%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/tech_util.cpp.o [ 8%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/track_coloring.cpp.o [ 9%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/track_info.cpp.o [ 10%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/track_info_util.cpp.o [ 11%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/vector45.cpp.o [ 12%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/via.cpp.o [ 14%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/via_info.cpp.o [ 15%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/via_lookup.cpp.o [ 16%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/via_param.cpp.o [ 17%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/via_param_util.cpp.o [ 18%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/via_util.cpp.o [ 20%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/via_wrapper.cpp.o [ 21%] Building CXX object CMakeFiles/cbag.dir/src/cbag/layout/wire_width.cpp.o [ 22%] Building CXX object CMakeFiles/cbag.dir/src/cbag/logging/logging.cpp.o [ 23%] Building CXX object CMakeFiles/cbag.dir/src/cbag/netlist/cdl.cpp.o [ 24%] Building CXX object CMakeFiles/cbag.dir/src/cbag/netlist/core.cpp.o [ 25%] Building CXX object CMakeFiles/cbag.dir/src/cbag/netlist/lstream.cpp.o [ 27%] Building CXX object CMakeFiles/cbag.dir/src/cbag/netlist/netlist.cpp.o [ 28%] Building CXX object CMakeFiles/cbag.dir/src/cbag/netlist/nstream_output.cpp.o [ 29%] Building CXX object CMakeFiles/cbag.dir/src/cbag/netlist/spectre.cpp.o [ 30%] Building CXX object CMakeFiles/cbag.dir/src/cbag/netlist/verilog.cpp.o [ 31%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/arc.cpp.o [ 32%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/cellview.cpp.o [ 34%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/cellview_info.cpp.o [ 35%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/donut.cpp.o [ 36%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/ellipse.cpp.o [ 37%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/eval_text.cpp.o [ 38%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/instance.cpp.o [ 40%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/line.cpp.o [ 41%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/path.cpp.o [ 42%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/pin_figure.cpp.o [ 43%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/pin_object.cpp.o [ 44%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/polygon.cpp.o [ 45%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/rectangle.cpp.o [ 47%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/shape_base.cpp.o [ 48%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/term_attr.cpp.o [ 49%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/text_base.cpp.o [ 50%] Building CXX object CMakeFiles/cbag.dir/src/cbag/schematic/text_t.cpp.o [ 51%] Building CXX object CMakeFiles/cbag.dir/src/cbag/spirit/ast.cpp.o [ 52%] Building CXX object CMakeFiles/cbag.dir/src/cbag/spirit/name.cpp.o In file included from /usr/include/boost/spirit/home/x3/operator/sequence.hpp:12, from /usr/include/boost/spirit/home/x3/operator.hpp:10, from /usr/include/boost/spirit/home/x3.hpp:19, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/spirit/config.h:50, from /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/spirit/name.cpp:47: /usr/include/boost/spirit/home/x3/operator/detail/sequence.hpp: In instantiation of \u2018struct boost::spirit::x3::detail::partition_attribute<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:133&)> >, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::sequence<boost::spirit::x3::uint_parser<unsigned int>, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > > > > > > >, cbag::spirit::ast::range, boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>, void>\u2019: /usr/include/boost/spirit/home/x3/operator/detail/sequence.hpp:243:15: required from \u2018bool boost::spirit::x3::detail::parse_sequence(const Parser&, Iterator&, const Iterator&, const Context&, RContext&, Attribute&, AttributeCategory) [with Parser = boost::spirit::x3::sequence<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:133&)> >, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::sequence<boost::spirit::x3::uint_parser<unsigned int>, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > > > > > > > >; Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; Context = boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>; RContext = cbag::spirit::ast::range; Attribute = cbag::spirit::ast::range; AttributeCategory = boost::spirit::x3::traits::tuple_attribute]\u2019 /usr/include/boost/spirit/home/x3/operator/sequence.hpp:46:42: required from \u2018bool boost::spirit::x3::sequence<Left, Right>::parse(Iterator&, const Iterator&, const Context&, RContext&, Attribute&) const [with Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; Context = boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>; RContext = cbag::spirit::ast::range; Attribute = cbag::spirit::ast::range; Left = boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:133&)> >; Right = boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::sequence<boost::spirit::x3::uint_parser<unsigned int>, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > > > > > > >]\u2019 /usr/include/boost/spirit/home/x3/directive/expect.hpp:54:41: required from \u2018bool boost::spirit::x3::expect_directive<Subject>::parse(Iterator&, const Iterator&, const Context&, RContext&, Attribute&) const [with Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; Context = boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>; RContext = cbag::spirit::ast::range; Attribute = cbag::spirit::ast::range; Subject = boost::spirit::x3::sequence<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:133&)> >, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::sequence<boost::spirit::x3::uint_parser<unsigned int>, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > > > > > > > >]\u2019 /usr/include/boost/spirit/home/x3/operator/detail/sequence.hpp:253:34: required from \u2018bool boost::spirit::x3::detail::parse_sequence(const Parser&, Iterator&, const Iterator&, const Context&, RContext&, Attribute&, AttributeCategory) [with Parser = boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::sequence<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:133&)> >, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::sequence<boost::spirit::x3::uint_parser<unsigned int>, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > > > > > > > > > >; Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; Context = boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>; RContext = cbag::spirit::ast::range; Attribute = cbag::spirit::ast::range; AttributeCategory = boost::spirit::x3::traits::tuple_attribute]\u2019 /usr/include/boost/spirit/home/x3/operator/sequence.hpp:46:42: required from \u2018bool boost::spirit::x3::sequence<Left, Right>::parse(Iterator&, const Iterator&, const Context&, RContext&, Attribute&) const [with Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; Context = boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>; RContext = cbag::spirit::ast::range; Attribute = cbag::spirit::ast::range; Left = boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>; Right = boost::spirit::x3::expect_directive<boost::spirit::x3::sequence<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:133&)> >, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::sequence<boost::spirit::x3::uint_parser<unsigned int>, boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > > > > > > > > >]\u2019 /usr/include/boost/spirit/home/x3/operator/detail/sequence.hpp:252:30: [ skipping 44 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ] /usr/include/boost/spirit/home/x3/nonterminal/detail/rule.hpp:240:42: required from \u2018static bool boost::spirit::x3::detail::rule_parser<Attribute, ID, skip_definition_injection>::parse_rhs_main(const RHS&, Iterator&, const Iterator&, const Context&, RContext&, ActualAttribute&, mpl_::true_) [with RHS = boost::spirit::x3::rule_definition<cbag::spirit::parser::name_rep_class, boost::spirit::x3::alternative<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::literal_string<const char*, boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > >, boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::alternative<boost::spirit::x3::sequence<boost::spirit::x3::rule<cbag::spirit::parser::name_unit_class, cbag::spirit::ast::name_unit, true>, boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> >, boost::spirit::x3::sequence<boost::spirit::x3::rule<cbag::spirit::parser::name_class, cbag::spirit::ast::name, true>, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > > > > > >, boost::spirit::x3::sequence<boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::literal_string<const char*, boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::rule<cbag::spirit::parser::name_unit_class, cbag::spirit::ast::name_unit, true> > > >, cbag::spirit::ast::name_rep, true, true>; Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; Context = boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>; RContext = cbag::spirit::ast::name_rep; ActualAttribute = cbag::spirit::ast::name_rep; Attribute = cbag::spirit::ast::name_rep; ID = cbag::spirit::parser::name_rep_class; bool skip_definition_injection = true; mpl_::true_ = mpl_::bool_<true>]\u2019 /usr/include/boost/spirit/home/x3/nonterminal/detail/rule.hpp:267:34: required from \u2018static bool boost::spirit::x3::detail::rule_parser<Attribute, ID, skip_definition_injection>::parse_rhs_main(const RHS&, Iterator&, const Iterator&, const Context&, RContext&, ActualAttribute&) [with RHS = boost::spirit::x3::rule_definition<cbag::spirit::parser::name_rep_class, boost::spirit::x3::alternative<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::literal_string<const char*, boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > >, boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::alternative<boost::spirit::x3::sequence<boost::spirit::x3::rule<cbag::spirit::parser::name_unit_class, cbag::spirit::ast::name_unit, true>, boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> >, boost::spirit::x3::sequence<boost::spirit::x3::rule<cbag::spirit::parser::name_class, cbag::spirit::ast::name, true>, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > > > > > >, boost::spirit::x3::sequence<boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::literal_string<const char*, boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::rule<cbag::spirit::parser::name_unit_class, cbag::spirit::ast::name_unit, true> > > >, cbag::spirit::ast::name_rep, true, true>; Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; Context = boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>; RContext = cbag::spirit::ast::name_rep; ActualAttribute = cbag::spirit::ast::name_rep; Attribute = cbag::spirit::ast::name_rep; ID = cbag::spirit::parser::name_rep_class; bool skip_definition_injection = true]\u2019 /usr/include/boost/spirit/home/x3/nonterminal/detail/rule.hpp:281:34: required from \u2018static bool boost::spirit::x3::detail::rule_parser<Attribute, ID, skip_definition_injection>::parse_rhs(const RHS&, Iterator&, const Iterator&, const Context&, RContext&, ActualAttribute&, mpl_::false_) [with RHS = boost::spirit::x3::rule_definition<cbag::spirit::parser::name_rep_class, boost::spirit::x3::alternative<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::literal_string<const char*, boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > >, boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::alternative<boost::spirit::x3::sequence<boost::spirit::x3::rule<cbag::spirit::parser::name_unit_class, cbag::spirit::ast::name_unit, true>, boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> >, boost::spirit::x3::sequence<boost::spirit::x3::rule<cbag::spirit::parser::name_class, cbag::spirit::ast::name, true>, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > > > > > >, boost::spirit::x3::sequence<boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::literal_string<const char*, boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::rule<cbag::spirit::parser::name_unit_class, cbag::spirit::ast::name_unit, true> > > >, cbag::spirit::ast::name_rep, true, true>; Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; Context = boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>; RContext = cbag::spirit::ast::name_rep; ActualAttribute = cbag::spirit::ast::name_rep; Attribute = cbag::spirit::ast::name_rep; ID = cbag::spirit::parser::name_rep_class; bool skip_definition_injection = true; mpl_::false_ = mpl_::bool_<false>]\u2019 /usr/include/boost/spirit/home/x3/nonterminal/detail/rule.hpp:330:37: required from \u2018static bool boost::spirit::x3::detail::rule_parser<Attribute, ID, skip_definition_injection>::call_rule_definition(const RHS&, const char*, Iterator&, const Iterator&, const Context&, ActualAttribute&, ExplicitAttrPropagation) [with RHS = boost::spirit::x3::rule_definition<cbag::spirit::parser::name_rep_class, boost::spirit::x3::alternative<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::literal_string<const char*, boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > >, boost::spirit::x3::sequence<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::alternative<boost::spirit::x3::sequence<boost::spirit::x3::rule<cbag::spirit::parser::name_unit_class, cbag::spirit::ast::name_unit, true>, boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> >, boost::spirit::x3::sequence<boost::spirit::x3::rule<cbag::spirit::parser::name_class, cbag::spirit::ast::name, true>, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > > > > > >, boost::spirit::x3::sequence<boost::spirit::x3::optional<boost::spirit::x3::sequence<boost::spirit::x3::sequence<boost::spirit::x3::literal_string<const char*, boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type>, boost::spirit::x3::expect_directive<boost::spirit::x3::action<boost::spirit::x3::uint_parser<unsigned int>, cbag::spirit::parser::<lambda(auto:134&)> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::literal_char<boost::spirit::char_encoding::standard, boost::spirit::x3::unused_type> > > >, boost::spirit::x3::expect_directive<boost::spirit::x3::rule<cbag::spirit::parser::name_unit_class, cbag::spirit::ast::name_unit, true> > > >, cbag::spirit::ast::name_rep, true, true>; Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; Context = boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>; ActualAttribute = cbag::spirit::ast::name_rep; ExplicitAttrPropagation = mpl_::bool_<true>; Attribute = cbag::spirit::ast::name_rep; ID = cbag::spirit::parser::name_rep_class; bool skip_definition_injection = true]\u2019 /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/include/cbag/spirit/name_def.h:112:1: required from \u2018bool cbag::spirit::parser::parse_rule(boost::spirit::x3::detail::rule_id<name_rep_class>, Iterator&, const Iterator&, const Context&, boost::spirit::x3::rule<name_rep_class, cbag::spirit::ast::name_rep, true>::attribute_type&) [with Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; Context = boost::spirit::x3::context<boost::spirit::x3::error_handler_tag, const std::reference_wrapper<boost::spirit::x3::error_handler<__gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> > > >, boost::spirit::x3::unused_type>; boost::spirit::x3::rule<name_rep_class, cbag::spirit::ast::name_rep, true>::attribute_type = cbag::spirit::ast::name_rep]\u2019 /faust/user/kcaisley/designs/dmc65v2/bag/pybag/cbag/src/cbag/spirit/name.cpp:53:1: required from here /usr/include/boost/spirit/home/x3/operator/detail/sequence.hpp:144:25: error: static assertion failed: Size of the passed attribute is bigger than expected. 144 | actual_size <= expected_size | ~~~~~~~~~~~~^~~~~~~~~~~~~~~~ /usr/include/boost/spirit/home/x3/operator/detail/sequence.hpp:144:25: note: the comparison reduces to \u2018(3 <= 2)\u2019 gmake[2]: *** [CMakeFiles/cbag.dir/build.make:972: CMakeFiles/cbag.dir/src/cbag/spirit/name.cpp.o] Error 1 gmake[1]: *** [CMakeFiles/Makefile2:124: CMakeFiles/cbag.dir/all] Error 2 How I bulit containers to test the other versions of EL:","title":"Problem #6"},{"location":"bag_old_notes_delete_me/#container-creation","text":"sudo dnf install apptainer apptainer build --sandbox cbag_centos7.sif docker://centos:7 apptainer shell --fakeroot --writable cbag_centos7.sif/","title":"Container Creation"},{"location":"bag_old_notes_delete_me/#for-centos7","text":"yum -y update && yum clean all yum install centos-release-scl yum install devtoolset-8 yum install epel-release yum install which yaml-cpp yaml-cpp-devel fmt fmt-devel spdlog spdlog-devel cmake3 //note cmake3 source /opt/rh/devtoolset-8/enable cd to folder and export CC and CXX tried to build, but didn't work as spdlog wasn't new enough to include .cmake files","title":"For Centos7:"},{"location":"bag_old_notes_delete_me/#for-rockylinux-87","text":"dnf -y update && dnf clean all dnf install epel-release dnf install which yaml-cpp yaml-cpp-devel fmt fmt-devel spdlog spdlog-devel cmake gcc gcc-c++ boost boost-devel starts compiling after stting static boost to OFF, but then fails at compiling with boost...","title":"For RockyLinux 8.7:"},{"location":"bag_old_notes_delete_me/#for-rockylinux-91","text":"dnf -y update && dnf clean all dnf -y install epel-release dnf -y install which yaml-cpp yaml-cpp-devel fmt fmt-devel spdlog spdlog-devel cmake gcc gcc-c++ boost boost-devel same errors as fedora 37","title":"For RockyLinux 9.1:"},{"location":"bag_old_notes_delete_me/#email-to-ayan","text":"If you have a moment, would it be possible for you to check what versions of gcc and CMake are used to compile your copy of the CBAG submodule, and what versions of the Boost, fmt, spdlog, and yaml-cpp libraries it is built against? I have tried the following combinations of packages versions across various distributions: Distro gcc/g++ CMake Boost fmt spdlog yaml-cpp CentOS 7.9 8.3.1 3.17.0 1.53 6.2.1 0.10.0 0.5.1 (devtoolset-8) RHEL 8.7 8.5.0 3.20.2 1.66 6.3.1 1.5.0 0.6.2 RHEL 9.1 11.3.1 3.20.2 1.75 8.1.1 1.10.0 0.6.3 Fedora 37 12.2.1 3.25.2 1.78 9.1.0 1.10.0 0.6.3 Fedora 37 12.2.1 3.25.2 1.81 9.1.0 1.11.0 0.6.3 In each case I have linked my copies of the OA binaies, have worked through successfully generating make/build files with CMake, and compilation begins. But in each case there are numerous compilation errors, primarily in the form of static assertion failures originating from the fmt and Boost libraries. I've made the most progress in the latter Fedora 37 configuration by editing the CBAG source to be compatible with fmt >9.0, but am still stuck on a couple of the more cryptic errors. fmt spdlog yaml-cpp 8.1.1 1.10.0 0.6.3 export OA_SRC_ROOT=/cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa/22.50p001 export OA_LINK_DIR=${OA_SRC_ROOT}/lib/linux_rhel50_gcc44x_64/opt export OA_INCLUDE_DIR=${OA_SRC_ROOT}/include I faced one last bug, which was simple to fix like Problem #1 above: [ 87%] Building CXX object CMakeFiles/cbag.dir/src/cbag/oa/read_lib.cpp.o In file included from /faust/user/kcaisley/packages/cbag/include/cbag/util/sorted_map.h:52, from /faust/user/kcaisley/packages/cbag/include/cbag/schematic/cellview_fwd.h:53, from /faust/user/kcaisley/packages/cbag/include/cbag/schematic/cellview.h:50, from /faust/user/kcaisley/packages/cbag/src/cbag/oa/read_lib.cpp:47: /faust/user/kcaisley/packages/cbag/include/cbag/util/sorted_vector.h: In member function \u2018const cbag::util::sorted_vector ::value_type& cbag::util::sorted_vector ::at_front() const\u2019: /faust/user/kcaisley/packages/cbag/include/cbag/util/sorted_vector.h:107:24: error: \u2018out_of_range\u2019 is not a member of \u2018std\u2019 107 | throw std::out_of_range(\"Cannot get front of empty vector.\"); | ^~~~~~~~~~~~ /faust/user/kcaisley/packages/cbag/include/cbag/util/sorted_vector.h: In member function \u2018const cbag::util::sorted_vector ::value_type& cbag::util::sorted_vector ::at_back() const\u2019: /faust/user/kcaisley/packages/cbag/include/cbag/util/sorted_vector.h:112:24: error: \u2018out_of_range\u2019 is not a member of \u2018std\u2019 112 | throw std::out_of_range(\"Cannot get back of empty vector.\"); | ^~~~~~~~~~~~ compilation terminated due to -fmax-errors=2. gmake[2]: *** [CMakeFiles/cbag.dir/build.make:1210: CMakeFiles/cbag.dir/src/cbag/oa/read_lib.cpp.o] Error 1 gmake[1]: *** [CMakeFiles/Makefile2:124: CMakeFiles/cbag.dir/all] Error 2 So I think I'll just: To use the std::out_of_range function in C++ code, you need to include the header file in your code. #include <stdexcept> in include/cbag/util/sorted_vector.h cmake -H. -B_build -DHUNTER_STATUS_DEBUG=ON -DCMAKE_BUILD_TYPE=Release cmake --build _build Make sure, when installing from python top level build.sh, you need to have 'sudo dnf install python3.10 python3.10-devel`","title":"Email to Ayan:"},{"location":"bag_old_notes_delete_me/#22-march","text":"I'd like to understand a number of concepts related to my Python/Pybind/CMake/C++ environment: Duck typing generics in C++ but not Python, due to it's typing in C++ done with templates avoids the need for function overloading other C++ features that were added later: garbage collector shared pointer metaprogramming detailing Searching for Pybind references: [kcaisley@asiclab008 pybag]$ grep -r --exclude-dir={pybind11, build} \"PYBIND11 \" src/pybag/core.cpp:PYBIND11_MODULE(core, m) { src/pybag/tech.cpp: PYBIND11_OVERLOAD_PURE(cbag::em_specs_t, cbag::layout::tech, get_metal_em_specs, layer, src/pybag/tech.cpp: PYBIND11_OVERLOAD_PURE(cbag::em_specs_t, cbag::layout::tech, get_via_em_specs, layer_dir, src/pybag/enum_conv.h: PYBIND11_OBJECT_DEFAULT(PyOrient2D, obj_base, true_check); src/pybag/enum_conv.h: PYBIND11_OBJECT_DEFAULT(PyOrient, obj_base, true_check); src/pybag/enum_conv.h: PYBIND11_OBJECT_DEFAULT(PyLogLevel, obj_base, true_check); src/pybag/enum_conv.h: PYBIND11_OBJECT_DEFAULT(PySigType, obj_base, true_check); src/pybag/enum_conv.h: PYBIND11_OBJECT_DEFAULT(PyDesignOutput, obj_base, true_check); pybind11_generics/src/main.cpp:PYBIND11_MODULE(pyg_test, m) { pybind11_generics/src/test_list.cpp: PYBIND11_OVERLOAD_PURE(std::string, / Return type / pybind11_generics/include/pybind11_generics/dict.h:#ifndef PYBIND11_GENERICS_DICT_H pybind11_generics/include/pybind11_generics/dict.h:#define PYBIND11_GENERICS_DICT_H pybind11_generics/include/pybind11_generics/tuple.h:#ifndef PYBIND11_GENERICS_TUPLE_H pybind11_generics/include/pybind11_generics/tuple.h:#define PYBIND11_GENERICS_TUPLE_H pybind11_generics/include/pybind11_generics/list.h:#ifndef PYBIND11_GENERICS_LIST_H pybind11_generics/include/pybind11_generics/list.h:#define PYBIND11_GENERICS_LIST_H pybind11_generics/include/pybind11_generics/any.h:#ifndef PYBIND11_GENERICS_ANY_H pybind11_generics/include/pybind11_generics/any.h:#define PYBIND11_GENERICS_ANY_H pybind11_generics/include/pybind11_generics/any.h: PYBIND11_OBJECT_DEFAULT(Any, any_base, true_check); pybind11_generics/include/pybind11_generics/type_name.h:#ifndef PYBIND11_GENERICS_TYPE_NAME_H pybind11_generics/include/pybind11_generics/type_name.h:#define PYBIND11_GENERICS_TYPE_NAME_H pybind11_generics/include/pybind11_generics/optional.h:#ifndef PYBIND11_GENERICS_OPTIONAL_H pybind11_generics/include/pybind11_generics/optional.h:#define PYBIND11_GENERICS_OPTIONAL_H pybind11_generics/include/pybind11_generics/optional.h: PYBIND11_OBJECT_DEFAULT(Optional, optional_base, optional_check); pybind11_generics/include/pybind11_generics/custom.h:#ifndef PYBIND11_GENERICS_CUSTOM_H pybind11_generics/include/pybind11_generics/custom.h:#define PYBIND11_GENERICS_CUSTOM_H pybind11_generics/include/pybind11_generics/custom.h: PYBIND11_OBJECT_DEFAULT(Custom, custom_base, true_check); pybind11_generics/include/pybind11_generics/cast_input_iterator.h:#ifndef PYBIND11_GENERICS_CAST_INPUT_ITERATOR_H pybind11_generics/include/pybind11_generics/cast_input_iterator.h:#define PYBIND11_GENERICS_CAST_INPUT_ITERATOR_H pybind11_generics/include/pybind11_generics/iterator.h:#ifndef PYBIND11_GENERICS_ITERATOR_H pybind11_generics/include/pybind11_generics/iterator.h:#define PYBIND11_GENERICS_ITERATOR_H pybind11_generics/include/pybind11_generics/iterator.h: PYBIND11_OBJECT_DEFAULT(PyIterator, iterator_base, PyIter_Check); pybind11_generics/include/pybind11_generics/iterable.h:#ifndef PYBIND11_GENERICS_ITERABLE_H pybind11_generics/include/pybind11_generics/iterable.h:#define PYBIND11_GENERICS_ITERABLE_H pybind11_generics/include/pybind11_generics/iterable.h: PYBIND11_OBJECT_DEFAULT(Iterable, iterable_base, iterable_check) pybind11_generics/include/pybind11_generics/union.h:#ifndef PYBIND11_GENERICS_UNION_H pybind11_generics/include/pybind11_generics/union.h:#define PYBIND11_GENERICS_UNION_H pybind11_generics/include/pybind11_generics/cast.h:#ifndef PYBIND11_GENERICS_CAST_H pybind11_generics/include/pybind11_generics/cast.h:#define PYBIND11_GENERICS_CAST_H pybind11_generics/CMakeLists.txt:set(PYBIND11_GENERICS_MASTER_PROJECT OFF) pybind11_generics/CMakeLists.txt: set(PYBIND11_GENERICS_MASTER_PROJECT ON) pybind11_generics/CMakeLists.txt:option(PYBIND11_GENERICS_TEST \"Build pybind11_generics test suite?\" pybind11_generics/CMakeLists.txt: ${PYBIND11_GENERICS_MASTER_PROJECT}) pybind11_generics/CMakeLists.txt:set(PYBIND11_CPP_STANDARD --std=c++1z) pybind11_generics/CMakeLists.txt:if (PYBIND11_GENERICS_TEST) Using CMake with external projects: http://www.saoe.net/blog/using-cmake-with-external-projects/ https://www.scivision.dev/cmake-fetchcontent-vs-external-project/ git hashes: b6f4ceaed0a0a24ccf575fab6c56dd50ccf6f1a9 deps/fmt (8.1.1) 76fb40d95455f249bd70824ecfcae7a8f0930fa3 deps/spdlog (v1.2.1-2055-g76fb40d9)","title":"22 March"},{"location":"bag_old_notes_delete_me/#cmake-workflow","text":"First, download and bootstrap vcpkg itself; it can be installed anywhere, but generally we recommend using vcpkg as a submodule for CMake projects. $ git clone https://github.com/microsoft/vcpkg $ ./vcpkg/bootstrap-vcpkg.sh To install the libraries for your project, run: $ ./vcpkg/vcpkg install [packages to install] You can also search for the libraries you need with the search subcommand: $ ./vcpkg/vcpkg search [search term] In order to use vcpkg with CMake, you can use the toolchain file: $ cmake -B [build directory] -S . \"-DCMAKE_TOOLCHAIN_FILE=[path to vcpkg]/scripts/buildsystems/vcpkg.cmake\" $ cmake --build [build directory]","title":"CMake workflow:"},{"location":"bag_old_notes_delete_me/#visual-studio-code-with-cmake-tools","text":"Adding the following to your workspace settings.json will make CMake Tools automatically use vcpkg for libraries: { \"cmake.configureSettings\": { \"CMAKE_TOOLCHAIN_FILE\": \"[vcpkg root]/scripts/buildsystems/vcpkg.cmake\" } }","title":"Visual Studio Code with CMake Tools"},{"location":"bag_old_notes_delete_me/#to-enable-versioning-when-running-vcpkg","text":"./vcpkg/vcpkg --feature-flags=\"versions\" install I should test this from just the cbag directory: cmake -B_build -S. -DCMAKE_TOOLCHAIN_FILE=/home/kcaisley/packages/vcpkg/scripts/buildsystems/vcpkg.cmake Step 1: Clone the vcpkg repo git clone https://github.com/Microsoft/vcpkg.git Make sure you are in the directory you want the tool installed to before doing this. Step 2: Run the bootstrap script to build vcpkg ./vcpkg/bootstrap-vcpkg.sh","title":"To enable versioning when running vcpkg"},{"location":"bag_old_notes_delete_me/#notes-about-building-cbag-with-vcpkg","text":"the errors with not finding compilers and make program only happen when running fatal: path 'versions/baseline.json' exists on disk, but not in '664f8bb619b752430368d0f30a8289b761f5caba' You can use the current commit as a baseline, which is: \"builtin-baseline\": \"c9f906558f9bb12ee9811d6edc98ec9255c6cda5\" Adding the following before project(cbag) set(CMAKE_MAKE_PROGRAM /usr/bin/make) set(CMAKE_C_COMPILER /usr/bin/gcc) set(CMAKE_CXX_COMPILER /usr/bin/g++) Okay, that fixed it. Now the only error I'm seeing is: error: Cannot resolve a minimum constraint for dependency boost-disjoint-sets from boost:x64-linux. The dependency was not found in the baseline, indicating that the package did not exist at that time. This may be fixed by providing an explicit override version via the \"overrides\" field or by updating the baseline. See `vcpkg help versioning` for more information. These are the two links needed for me to proceed in solving this error. Also, see ../../vcpkg/vcpkg help versioning This is how a manifest file should work: { \"name\": \"example\", \"version\": \"1.0\", \"builtin-baseline\": \"a14a6bcb27287e3ec138dba1b948a0cdbc337a3a\", \"dependencies\": [ { \"name\": \"zlib\", \"version>=\": \"1.2.11#8\" }, \"rapidjson\" ], \"overrides\": [ { \"name\": \"rapidjson\", \"version\": \"2020-09-14\" } ] } The boost components I probably need are: container container-hash fusion units mpl tokenizer units spirit serialization (compiled, contains container_hash, and archive libraries) spirit (header-only) fusion units mpl tokenizer I give up for now. I can't install Boost with vcpkg. It's too frustrating. Boost 1.75 is the oldest I can install with a built-in baseline, and for some reason manually specifying the baseline with overrides requires me to write out each of the packages in Boost.","title":"Notes about building cbag with vcpkg"},{"location":"bag_old_notes_delete_me/#building-on-april-10","text":"cp -r /cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa.aoi/22.50p001/ . find . -type d -name \"*example*\" This is the location of the binaries I want: /mnt/md127/tools/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa.aoi/22.50p001 So, assuming it is mounted on my machine at /cadence , I can do: cp -r /cadence/mentor/aoi_cal_2016.4_38.25/shared/pkgs/icv_oa.aoi/22.50p001/* .","title":"Building on April 10"},{"location":"bag_old_notes_delete_me/#multiple-defenitions-errors","text":"I'm finding some possible recommendations about how to figure out this multiple definitions issue: https://stackoverflow.com/questions/69326932/multiple-definition-errors-during-gcc-linking-in-linux https://stackoverflow.com/questions/37525922/how-to-handle-gcc-link-optionslike-whole-archive-allow-multiple-definition The idea to hack it with --allow-multiple-definition in target_link_libraries doesn't work as g++ doesn't support it.","title":"Multiple defenitions errors:"},{"location":"bash/","text":"Bash: You can set env variables in a bash script by just writing X='text', where X is the env variable name. You don't have to export. To make sure that these are available then in the terminal session that launched it, you need to make sure you source the script, or write a '.' before the bash script name. . is the POSIX compliant way of sourcing, and source is a Bash-exclusive synonym. Building specs table for each machine: lscpu gives cpu info free -hm gives ram info Find all files with certain extension: find . -type f -name \"*.txt\" Find all files containing a certain string (you can add -i flag to IGNORE case) grep -R \"text to find\" . note, when running ls -l, the second column is the number of hardlinks (which is equal to the number of directories, sorta?) anyways, i can just think of it as the approximate number of directories inside this one. Using Git to restore a deleted, uncommitted file: If your changes have not been staged or committed: The command you should use in this scenario is git restore FILENAME to figure out current tty number w to show which tty are being used ps -e | grep tty to kill another tty pkill -9 -t tty1 to show process numbers ps -f another tool which is useful for identifying and killing processes is top/htop. i think that top can do everything htop can, but more easily perhaps i should learn to play with top more in the future who whoami whereis for i in *.pdf do mv \"$i\" \"`echo $i | sed 's/ Circuit Intuitions//'`\" done Navigation ls - list directory contents pwd - print name of current/working directory cd - change working directory pushd/popd - put working directory on a stack file - determine file type find - file search by name, size, location, etc locate - find files by name updatedb - update database for locate which - locate a command history - display bash command history GETTING HELP whatis - display the on-line manual descriptions apropos - search the manual page names and descriptions man - an interface to the on-line reference manuals WORKING W/ FILES mkdir - create a directory/make directories touch - change file timestamps/create empty files cp - copy files and directories mv - move (rename) files rm - remove files or directories rmdir - remove empty directories TEXT FILES cat - concatenate files and print on the standard output more/less - file perusal filter for crt viewing nano - command line text editor USERS sudo - execute a command as superuser su - change user ID or become another user users - print the user names of users currently logged in id - print real and effective user and group IDs CHANGING FILE PERMISSIONS chmod - change permissions of a file KILLING PROGRAMS AND LOGGING OUT Ctrl+C - kill a running command killall - kill processes by name exit - log out of bash USEFUL SHORTCUTS Ctrl+D - signal bash that there is no more input Ctrl+L - redraw the screen Ctrl++ - make text bigger in terminal emulator Ctrl+- - make text smaller in terminal emulator MORE pidof top ps acx df du pgrep gotop htop uname -a or -s -o ip addr ip address show ip address pinging website gives ip address date cal df du grep find lsblk dd lspci file UTF-8 vs ASCII cat less locate updatedb grok alias cp to cp -iv ldd nohup (prevents shell exit from 'hang up' the processes aka killing the processes it started)","title":"Bash:"},{"location":"bash/#bash","text":"You can set env variables in a bash script by just writing X='text', where X is the env variable name. You don't have to export. To make sure that these are available then in the terminal session that launched it, you need to make sure you source the script, or write a '.' before the bash script name. . is the POSIX compliant way of sourcing, and source is a Bash-exclusive synonym.","title":"Bash:"},{"location":"bash/#building-specs-table-for-each-machine","text":"lscpu gives cpu info free -hm gives ram info","title":"Building specs table for each machine:"},{"location":"bash/#find-all-files-with-certain-extension","text":"find . -type f -name \"*.txt\"","title":"Find all files with certain extension:"},{"location":"bash/#find-all-files-containing-a-certain-string-you-can-add-i-flag-to-ignore-case","text":"grep -R \"text to find\" . note, when running ls -l, the second column is the number of hardlinks (which is equal to the number of directories, sorta?) anyways, i can just think of it as the approximate number of directories inside this one.","title":"Find all files containing a certain string (you can add -i flag to IGNORE case)"},{"location":"bash/#using-git-to-restore-a-deleted-uncommitted-file","text":"If your changes have not been staged or committed: The command you should use in this scenario is git restore FILENAME","title":"Using Git to restore a deleted, uncommitted file:"},{"location":"bash/#to-figure-out-current-tty-number","text":"w","title":"to figure out current tty number"},{"location":"bash/#to-show-which-tty-are-being-used","text":"ps -e | grep tty to kill another tty pkill -9 -t tty1 to show process numbers ps -f another tool which is useful for identifying and killing processes is top/htop. i think that top can do everything htop can, but more easily perhaps i should learn to play with top more in the future who whoami whereis for i in *.pdf do mv \"$i\" \"`echo $i | sed 's/ Circuit Intuitions//'`\" done","title":"to show which tty are being used"},{"location":"bash/#navigation","text":"ls - list directory contents pwd - print name of current/working directory cd - change working directory pushd/popd - put working directory on a stack file - determine file type find - file search by name, size, location, etc locate - find files by name updatedb - update database for locate which - locate a command history - display bash command history","title":"Navigation"},{"location":"bash/#getting-help","text":"whatis - display the on-line manual descriptions apropos - search the manual page names and descriptions man - an interface to the on-line reference manuals","title":"GETTING HELP"},{"location":"bash/#working-w-files","text":"mkdir - create a directory/make directories touch - change file timestamps/create empty files cp - copy files and directories mv - move (rename) files rm - remove files or directories rmdir - remove empty directories","title":"WORKING W/ FILES"},{"location":"bash/#text-files","text":"cat - concatenate files and print on the standard output more/less - file perusal filter for crt viewing nano - command line text editor","title":"TEXT FILES"},{"location":"bash/#users","text":"sudo - execute a command as superuser su - change user ID or become another user users - print the user names of users currently logged in id - print real and effective user and group IDs","title":"USERS"},{"location":"bash/#changing-file-permissions","text":"chmod - change permissions of a file","title":"CHANGING FILE PERMISSIONS"},{"location":"bash/#killing-programs-and-logging-out","text":"Ctrl+C - kill a running command killall - kill processes by name exit - log out of bash","title":"KILLING PROGRAMS AND LOGGING OUT"},{"location":"bash/#useful-shortcuts","text":"Ctrl+D - signal bash that there is no more input Ctrl+L - redraw the screen Ctrl++ - make text bigger in terminal emulator Ctrl+- - make text smaller in terminal emulator","title":"USEFUL SHORTCUTS"},{"location":"bash/#more","text":"pidof top ps acx df du pgrep gotop htop uname -a or -s -o ip addr ip address show ip address pinging website gives ip address date cal df du grep find lsblk dd lspci file UTF-8 vs ASCII cat less locate updatedb grok alias cp to cp -iv ldd nohup (prevents shell exit from 'hang up' the processes aka killing the processes it started)","title":"MORE"},{"location":"boot_sequence/","text":"BIOS MBR - loads an executes GRUB boot loader, stored in first sector of /dev/sda/ GRUB - loads kernel versions. grub.conf file is stored at /boot/grub/ or /etc/ Kernel - mounts root file system listed in grub.conf, executes /sbin/init/ init - right before run level programs, detemines run level with /etc/inittab, should pick either RL3 for multi-user.target or RL5 for graphical.target. run level programs in GRUB, press 'e' to edit kernel parameters Kernel command line parameters either have the format 'parameter' or 'parameter=value' Ctrl + x to boot Remove the arguments rhgb quiet and add the arguments loglevel=7 and systemd.log_level=debug instead to change the verbosity to highest level. Press CTRL+x to accept the changes and boot the system. You should see a lot of logs on you screen now. Instructions: https://www.thegeekdiary.com/centos-rhel-7-how-to-change-the-verbosity-of-debug-logs-during-booting/ To kill user sessions https://askubuntu.com/questions/974718/how-do-i-get-the-list-of-the-active-login-sessions to show the active users and process number who -u to kill active session sudo kill -9 <session-process-number>","title":"Boot sequence"},{"location":"boot_sequence/#to-kill-user-sessions","text":"https://askubuntu.com/questions/974718/how-do-i-get-the-list-of-the-active-login-sessions to show the active users and process number who -u to kill active session sudo kill -9 <session-process-number>","title":"To kill user sessions"},{"location":"checksum_notes/","text":"Checksum notes https://getfedora.org/en/security/ RPM files are signed by 'GPG signatures'; be sure to 'verify signatures' Fedora provides 'current keys', for example: Fedora 32 id: 4096R/12C944D0 2019-08-12 Fingerprint: 97A1 AE57 C3A2 372C CA3A 4ABA 6C13 026D 12C9 44D0 There a fedora GPG keys, which you import to GPG: $ curl https://getfedora.org/static/fedora.gpg | gpg --import This does: gpg: key 6C13026D12C944D0: public key \"Fedora (32) fedora-32-primary@fedoraproject.org \" imported Then there are also checksum files, which you can get on the page above, or here: $ curl https://getfedora.org/static/fedora.gpg | gpg --import This contains: -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA256 Fedora-Workstation-Live-x86_64-32-1.6.iso: 1966178304 bytes SHA256 (Fedora-Workstation-Live-x86_64-32-1.6.iso) = 4d0f6653e2e0860c99ffe0ef274a46d875fb85bd2a40cb896dce1ed013566924 -----BEGIN PGP SIGNATURE----- iQIcBAEBCAAGBQJeoxq7AAoJEGwTAm0SyUTQ9U0P/0JsYUr02Z7A1nxs+lBTt78b w6yxl95vl4NXtwnWH+jfWC9XZ7d8WKoeH6cXglcs3ecXbnpI5PV8xQ/w871bG/JH 1hK5fIEbx/Q80Dyoh5EdiuV+iYhKqI/qNSy2uNiFASgFR0SPTTs7aeuP3Exv4/vc P4KK/bdBE3xmI7XXc5yqPJE8gn/YYcEdS7uQbgkQA3Vi5KjCwffnxtKc5Q+OzCAW 3QvMkr6GZNaIyH+hQdmtVEQVV8LUTxHCyy37kGAG29nLtZjwIX6tPjfSr8SbyRkn 7Ma7brm+uQ5zbP0VUhZA72GWuvC52OBCmoUyz1PNcKZgSp2KyEzbUtbJTljqfI9i 2c9Xgb/8tF4zU3D77B5SecCvKVbZW8KC1sOA3LA87mDQnyCIi4DgjU45NPutuS61 JrGuu19n5PAGzMJ3Ti6hmEviavd2QirLboGlrqVCvqH7JX+qSX4YuPcscaLCtAy5 aSzyro1EGYZFIdE6avV9KXoXCMn8br4YgzuEw4/Phvk6+KmEDZGMV4HILkKkrxvO 0ICShZr7wUA/NTslzyW2Bj9Nv2hQ5svcEpuZkci/Hf4XtxQg/UZj2UlRg8DyDcyY fH754xkduzkzdX5PjPR5nmEs4p+ByGJfV/chnZuoIiVCXu7EHvjkfo9pyMPv4v8d QkKm3TxTBBg+qg+rSkqC =vWPF -----END PGP SIGNATURE----- You must 'verify the CHECKSUM file is valid\" using: $ gpg --verify-files *-CHECKSUM This does: gpg: Signature made Fri 24 Apr 2020 12:58:35 PM EDT gpg: using RSA key 6C13026D12C944D0 gpg: Good signature from \"Fedora (32) fedora-32-primary@fedoraproject.org \" [unknown] gpg: WARNING: This key is not certified with a trusted signature! gpg: There is no indication that the signature belongs to the owner. Primary key fingerprint: 97A1 AE57 C3A2 372C CA3A 4ABA 6C13 026D 12C9 44D0 The \"CHECKSUM file should have a good signature\" from the fedora 32 KEY list above Finally you check the trusted checksum matches the download: $ sha256sum -c *-CHECKSUM This does: Fedora-Workstation-Live-x86_64-32-1.6.iso: OK sha256sum: WARNING: 19 lines are improperly formatted ^This means the 'file is unmodified'","title":"Checksum notes"},{"location":"checksum_notes/#checksum-notes","text":"https://getfedora.org/en/security/ RPM files are signed by 'GPG signatures'; be sure to 'verify signatures' Fedora provides 'current keys', for example: Fedora 32 id: 4096R/12C944D0 2019-08-12 Fingerprint: 97A1 AE57 C3A2 372C CA3A 4ABA 6C13 026D 12C9 44D0 There a fedora GPG keys, which you import to GPG: $ curl https://getfedora.org/static/fedora.gpg | gpg --import This does: gpg: key 6C13026D12C944D0: public key \"Fedora (32) fedora-32-primary@fedoraproject.org \" imported Then there are also checksum files, which you can get on the page above, or here: $ curl https://getfedora.org/static/fedora.gpg | gpg --import This contains: -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA256","title":"Checksum notes"},{"location":"checksum_notes/#fedora-workstation-live-x86_64-32-16iso-1966178304-bytes","text":"SHA256 (Fedora-Workstation-Live-x86_64-32-1.6.iso) = 4d0f6653e2e0860c99ffe0ef274a46d875fb85bd2a40cb896dce1ed013566924 -----BEGIN PGP SIGNATURE----- iQIcBAEBCAAGBQJeoxq7AAoJEGwTAm0SyUTQ9U0P/0JsYUr02Z7A1nxs+lBTt78b w6yxl95vl4NXtwnWH+jfWC9XZ7d8WKoeH6cXglcs3ecXbnpI5PV8xQ/w871bG/JH 1hK5fIEbx/Q80Dyoh5EdiuV+iYhKqI/qNSy2uNiFASgFR0SPTTs7aeuP3Exv4/vc P4KK/bdBE3xmI7XXc5yqPJE8gn/YYcEdS7uQbgkQA3Vi5KjCwffnxtKc5Q+OzCAW 3QvMkr6GZNaIyH+hQdmtVEQVV8LUTxHCyy37kGAG29nLtZjwIX6tPjfSr8SbyRkn 7Ma7brm+uQ5zbP0VUhZA72GWuvC52OBCmoUyz1PNcKZgSp2KyEzbUtbJTljqfI9i 2c9Xgb/8tF4zU3D77B5SecCvKVbZW8KC1sOA3LA87mDQnyCIi4DgjU45NPutuS61 JrGuu19n5PAGzMJ3Ti6hmEviavd2QirLboGlrqVCvqH7JX+qSX4YuPcscaLCtAy5 aSzyro1EGYZFIdE6avV9KXoXCMn8br4YgzuEw4/Phvk6+KmEDZGMV4HILkKkrxvO 0ICShZr7wUA/NTslzyW2Bj9Nv2hQ5svcEpuZkci/Hf4XtxQg/UZj2UlRg8DyDcyY fH754xkduzkzdX5PjPR5nmEs4p+ByGJfV/chnZuoIiVCXu7EHvjkfo9pyMPv4v8d QkKm3TxTBBg+qg+rSkqC =vWPF -----END PGP SIGNATURE----- You must 'verify the CHECKSUM file is valid\" using: $ gpg --verify-files *-CHECKSUM This does: gpg: Signature made Fri 24 Apr 2020 12:58:35 PM EDT gpg: using RSA key 6C13026D12C944D0 gpg: Good signature from \"Fedora (32) fedora-32-primary@fedoraproject.org \" [unknown] gpg: WARNING: This key is not certified with a trusted signature! gpg: There is no indication that the signature belongs to the owner. Primary key fingerprint: 97A1 AE57 C3A2 372C CA3A 4ABA 6C13 026D 12C9 44D0 The \"CHECKSUM file should have a good signature\" from the fedora 32 KEY list above Finally you check the trusted checksum matches the download: $ sha256sum -c *-CHECKSUM This does: Fedora-Workstation-Live-x86_64-32-1.6.iso: OK sha256sum: WARNING: 19 lines are improperly formatted ^This means the 'file is unmodified'","title":"Fedora-Workstation-Live-x86_64-32-1.6.iso: 1966178304 bytes"},{"location":"clonezilla/","text":"Instructions for operating Clonezilla to restore CentOS7 Image in BIOS change boot to legacy boot from usb deafult clonezilla mode (sometimes gets stuck, displays orange stripes - maybe because of too much RAM. Needs power cycling) default language keep keyboard layout start terminal, 'sudo nano /etc/ssh/ssh_config', and add line 'HostKeyAlgorithms +ssh-rsa,ssh-dss'. Save and close. Exit terminal. start clonezilla mode: device-image ssh_server choose active network port config network server name: penelope.physik.uni-bonn.de port 22 (default) account: root (default) dir: /cadence/os/ confirm login, give password begginer mode savedisk or restore disk, depending on need for restore choose image select both hdd and ssd check before restoring choose end action after check is done confirm overwrite for all drives","title":"Clonezilla"},{"location":"clonezilla/#instructions-for-operating-clonezilla-to-restore-centos7-image","text":"in BIOS change boot to legacy boot from usb deafult clonezilla mode (sometimes gets stuck, displays orange stripes - maybe because of too much RAM. Needs power cycling) default language keep keyboard layout start terminal, 'sudo nano /etc/ssh/ssh_config', and add line 'HostKeyAlgorithms +ssh-rsa,ssh-dss'. Save and close. Exit terminal. start clonezilla mode: device-image ssh_server choose active network port config network server name: penelope.physik.uni-bonn.de port 22 (default) account: root (default) dir: /cadence/os/ confirm login, give password begginer mode savedisk or restore disk, depending on need for restore choose image select both hdd and ssd check before restoring choose end action after check is done confirm overwrite for all drives","title":"Instructions for operating Clonezilla to restore CentOS7 Image"},{"location":"containers/","text":"Motivation Proprietary EDA tools like Cadence Virtuoso are typically only supported on a handful of operating systems . As machines for ASIC design are typically dedicated for that purpose, this can therefore lead to Containers Container provide for operating system virtualization, in which the kernel allows the existence of multiple isolated user space instances. Intro to Containers OS-level virtualization is an operating system (OS) paradigm in which the kernel allows the existence of multiple isolated user space instances, called containers (LXC, Solaris containers, Docker, Podman), zones (Solaris containers), virtual private servers (OpenVZ), partitions, virtual environments (VEs), virtual kernels (DragonFly BSD), or jails (FreeBSD jail or chroot jail).[1] Such instances may look like real computers from the point of view of programs running in them. A computer program running on an ordinary operating system can see all resources (connected devices, files and folders, network shares, CPU power, quantifiable hardware capabilities) of that computer. However, programs running inside of a container can only see the container's contents and devices assigned to the container. A short list of tech to consider: Docker, Fedora CoreOS, Silverblue, Toolbx, Singularity/Apptainer, Podman, LXC, Flatpak In short, unlike a virtual machine, containers provide abstraction at the operating system level. This can be useful for running proprietary GUI-based applications like Cadence Virtuoso, which e We can use a CentOS7 image for setting up and running our cadence development. But how do we then access data? Docker is a single-purpose application virtualization and LXC is a multi-purpose operating system virtualization. If one looks for a portability and scalability of the application / micro-service, Docker and Kubernetes is a good choice. If one would like to have several (or even thousands of) portable systems on a single computer, LXC is a good choice. Docker was not designed out of the box for GUI apps, so you need to have a video server with X11 typically for that. Linux containers are all based on the virtualization, isolation, and resource management mechanisms provided by the Linux kernel , notably Linux namespaces and cgroups . OS-Level Virtualization (Containers) and Application Virtualization Technologies: With this tech, different distributions are fine, but other operating systems or kernels are not. Full application virtualization requires a virtualization layer.[ 2] Application virtualization layers replace part of the runtime environment normally provided by the operating system. The layer intercepts all disk operations of virtualized applications and transparently redirects them to a virtualized location, often a single file.[ 3] The application remains unaware that it accesses a virtual resource instead of a physical one. Since the application is now working with one file instead of many files spread throughout the system, it becomes easy to run the application on a different computer and previously incompatible applications can be run side by side. A container image is simply a file (or collection of files) saved on disk that stores everything you need to run a target application or applications: code runtime system tools libraries etc A container process is simply a standard (Linux) process running on top of the underlying host's operating system and kernel, but whose software environment is defined by the contents of the container image. How do I develop in a container, when I need to run a GUI as part of my workflow? What are the limits of containerization? Limitations of Containers Architecture dependent; always limited by CPU architecture (x86 vs ARM) and binary format (ELF) Portability: Requires glibc and kernel compatibility between host and container; also requires any other kernel-user space API compatibility (e.g., OFED/IB, NVIDIA/GPUs) Like something built on Ubuntu 22.04 wouldn't work on CentOS 6 Filesystem isolation: Filesystem paths are (mostly) different when viewed inside and outside container By default containers can't see the contents of the host file system. To access host filesystem from inside the file system requires a bit of extra work to 'bind' it. Most Common Use Cases Building and running applications that require older/newer system libraries than are available on the host system Most modern tools, like PyTorch, assume the latest packages, and expect a debian-based environment like Ubuntu, which most HPC system aren't doing. Containers can solve these incompatibilities. Running commercial applications binaries that have specific OS requirements not met by the host system. Your license agreement may only give you a compiled binary, which you're Oh! Wow this is exactly my use case. How do I build a repoducible environment when commercial binaries (maybe even with GUI?) are part of the workflow? Converting prexisting Docker containers, which won't work for HPC clusters, to Singularity containers Workflow Build your Singularity containers on you local system where you have root or sudo access; e.g., a personal computer where you have installed Singularity You can't work on native MacOS, as you need to use a Virtual machine with a Ubuntu or Fedora-based OS. This is doubly true, if your Mac has an M1 ARM chipset. Transfer your Singularity containers to the HPC system where you want to run them Run your Singularity containers on that HPC system Why Apptainer? (open-source Singularity fork) Docker Really designed for network centric services like web servers and databases, but not really meant for HPC systems where many users are sharing a space. Docker assumes you have trust for all users running on systems. Whereas in HPC, the admins don't even trust the users. Docker also wasn't designed to support batch-based workflows Docker not designed to support tightly-coupled, highly distributed parallel applications (MPI) Singularity Designed at Berkeley Lab as the equivalent for HPC. Each container is a single (read-only) image file (unlike the layered arch. in Docker). If you want to change it, you have to to rebuild it. No root owned daemon processes Support shared/multi-tenant resource environments Support HPC Hardware: Infiniband, GPUs Supports HPC applications: MPI Conclusion: Use Apptainer Overview of why not Docker, and the better options: https://blog.jwf.io/2019/08/hpc-workloads-containers/ Using Singularity/Apptainer with GUI commercial applications: https://learningpatterns.me/posts/2018-01-30-abaqus-singularity/ https://apptainer.org Starting Cadence Virtuoso (IC617 and IC618) from Fedora Linux Client Install apptainer sudo dnf install apptainer Create a .def file with the name virtuoso_centos7.def : Bootstrap: docker From: centos:7 %setup #run on the host system, after the base container OS is installed. Filepaths are relative to host (fedora) mkdir ${APPTAINER_ROOTFS}/users mkdir ${APPTAINER_ROOTFS}/tools %environment # IC617 doesn't recognize the OS environment properly, need to specify it manually export OA_UNSUPPORTED_PLAT=linux_rhel50_gcc48x %post #section to download and install packages before container is immutable #CentOS 7 image on dockerhub isn't updated, so run this first yum -y update && yum clean all #list of packages required by Cadence Virtusoso IC618 yum install -y csh tcsh glibc elfutils-libelf ksh mesa-libGL mesa-libGLU motif libXp libpng libjpeg-turbo expat glibc-devel gdb xorg-x11-fonts-misc xorg-x11-fonts-ISO8859-1-75dpi redhat-lsb libXScrnSaver apr apr-util compat-db47 xorg-x11-server-Xvfb mesa-dri-drivers openssl-devel #For IC617 and below, i686 package versions are required to support 32-bit binaries yum install -y glibc.i686 elfutils-libelf.i686 mesa-libGL.i686 mesa-libGLU.i686 motif.i686 libXp.i686 libpng.i686 libjpeg-turbo.i686 expat.i686 glibc-devel.i686 redhat-lsb.i686 # additional packages required by Cliosoft SOS yum install -y libXpm # additional package required for X11 forwarding yum install -y xorg-x11-utils Build the container, in an immutable mode: apptainer build virtuoso_centos7.sif virtuoso_centos7.def On the host machine, before running the startup script, we must start a xhost + , as discussed in this solution found here : xhost + Run the immutable container, and start a shell inside: apptainer shell -B /tools,/users virtuoso_centos7.sif Inside the container, check the system configuration with the Cadence provided tool checkSysConf : Apptainer> /cadence/cadence/IC618/tools.lnx86/bin/checkSysConf IC6.1.8 or Apptainer> /tools/cadence/IC618/tools.lnx86/bin/checkSysConf IC6.1.8 Finally, run the virtuoso start-up script, using the full path to tcsh . Make sure the start up script contains the line virtuoso & . Apptainer> /bin/tcsh ./startup.sh Else you need to source the script instead, and run the command virtuoso & manually. Troubleshooting If you are not able to start Cadence, ensure your environment has both tcsh and ksh installed. Check to make sure the cadence executable is known: which virutoso To check the current shell interpreter: # Display your current shell name reliably. ps -p $$ # Print the shell for the current user but not necessarily the shell that is running at the movement. echo \"$SHELL\" # Another reliable and simple method to get the current shell interpreter name on Linux or Unix-like systems. echo $0 Changing shell Need the tcsh and chsh commands to be To check which shell is active: cat /etc/passwd | grep cd; pwd To change shell, by running this and then restarting termina: chsh -s /usr/bin/tcsh This works now, but I needed ksh installed. okay, still complaining that it's not a supported OS. Tried running the checkSysConf tool: /cadence/cadence/IC618/tools.lnx86/bin/checkSysConf Okay, I'm still having issues, and cadence isn't giving me any useful messages. I've ruled the issue with wayland and Xorg, and I've switched to Xorg. My next step is to figure out how to get more verbose information on why Virtuoso isn't starting. All it says right now is: 2022/11/30 13:13:40 System is not a supported distribution 2022/11/30 13:13:40 An error occurred. We don't recognize OS 2022/11/30 13:13:40 WARNING This OS does not appear to be a Cadence supported Linux configuration. 2022/11/30 13:13:40 For more info, please run CheckSysConf in <cdsRoot/tools.lnx86/bin/checkSysConf <productId> Before I start hacking away and changing packages like openssl-devel to be more inline with the CentOS7 system, perhaps I can figure out better exactly what may need to change. Let's look at the CheckSysConf script and try to figure out what the latest supported version of the packages is. What version of Ubuntu is supported? Maybe there is a log message being written somewhere, and the people here might know where to find that log message. Which package provides file on RHEL Based System dnf provides ./filename Then check why a package was explici The run , exec , and shell commands are the three primary ways in which to interact with a container image. The run command will start the container, run the scripts marked for execution inside the container (if any), and then exit. The exec commands allows a one time command to be run inside the shell, which then exits. The shell command allows an interactive shell to be spun up, which can be exited at will. The first option is good for image usage once it is finalized. The latter two are best for prototyping when building in sandbox mode. WARNING: The sandbox contain files/dirs that cannot be removed with 'rm'. WARNING: Use 'chmod -R u+rwX' to set permissions that allow removal with 'rm -rf' WARNING: Use the '--fix-perms' option to 'apptainer build' to modify permissions at build time. I need to install a couple packages within this container. The right process is not to enter a shell within the container and run sudo yum install. Tomorrow: bind mount the cadence folder install the lsb-release package run the cadence compatibility script, and manually specify all the packages to install, until this check is passed figure out how to start the gui of the container write define file to make the above reproducible docs commerical GUI NIH tutorail Starting the container with Cadence folder available: apptainer shell -B /cadence:/cadence virtuoso_centos7.sif Okay, so I can't use the --writable option and have stuff automatically bind-mounted. Therefor in sandbox mode, I at least need to create the mountpoints manually. No the better $ apptainer build virtuoso_centos7_immut.sif virtuoso_centos7.def $ xhost + $ apptainer shell -B /cadence:/cadence virtuoso_centos7_immut.sif Apptainer> /cadence/cadence/IC618/tools.lnx86/bin/checkSysConf IC6.1.8 Apptainer> /bin/tcsh /faust/user/kcaisley/cadence/tsmc65/tsmc_crn65lp_1.7a On the host machine, before running the startup script, we must start a xhost + Solution found here for the whole bash vs tcsh problem: ps -p $$ \u2013 Display your current shell name reliably. echo \"$SHELL\" \u2013 Print the shell for the current user but not necessarily the shell that is running at the movement. echo $0 \u2013 Another reliable and simple method to get the current shell interpreter name on Linux or Unix-like systems. I may want to look at passing the --nv flag when trying to do heavy layout work flows. I think that the reason why xdpyinfo didn't work is because I hadn't made the xhost visible to the container. To find which package a utility is provided by (that isn't downloaded yet) yum provides /usr/bin/xdpyinfo If the package were downloaded, and if which were also on the system, we could do this instead: yum provides `which free` Anyways, we find the right package to install for it is yum info xorg-x11-utils Yes, it does appear that the package is no longer missing. However, now there is no $DISPLAY env variable, so I can't test it. This is because I'm command line only for now. check where container is running Further more, some notes from stuff I did in BAG: Container Creation sudo dnf install apptainer apptainer build --sandbox cbag_centos7.sif docker://centos:7 apptainer shell --fakeroot --writable cbag_centos7.sif/ For Centos7: yum -y update && yum clean all yum install centos-release-scl yum install devtoolset-8 yum install epel-release yum install which yaml-cpp yaml-cpp-devel fmt fmt-devel spdlog spdlog-devel cmake3 //note cmake3 source /opt/rh/devtoolset-8/enable cd to folder and export CC and CXX tried to build, but didn't work as spdlog wasn't new enough to include .cmake files For RockyLinux 8.7: dnf -y update && dnf clean all dnf install epel-release dnf install which yaml-cpp yaml-cpp-devel fmt fmt-devel spdlog spdlog-devel cmake gcc gcc-c++ boost boost-devel starts compiling after stting static boost to OFF, but then fails at compiling with boost... For RockyLinux 9.1: dnf -y update && dnf clean all dnf -y install epel-release dnf -y install which yaml-cpp yaml-cpp-devel fmt fmt-devel spdlog spdlog-devel cmake gcc gcc-c++ boost boost-devel Final Container Design for Cadence: (This ist he .def file, used to generate an immutable container) Bootstrap: docker From: centos:7 %setup #run on the host system, after the base container OS is installed. Filepaths are relative to host (fedora) mkdir ${APPTAINER_ROOTFS}/cadence %post #section to download and install packages before container is immutable #CentOS 7 image on dockerhub isn't updated, so run this first yum -y update && yum clean all #list of packages required by Cadence yum install -y csh tcsh glibc elfutils-libelf ksh mesa-libGL mesa-libGLU motif libXp libpng libjpeg-turbo expat glibc-devel gdb xorg-x11-fonts-misc xorg-x11-fonts-ISO8859-1-75dpi redhat-lsb libXScrnSaver apr apr-util compat-db47 xorg-x11-server-Xvfb mesa-dri-drivers openssl-devel yum install -y xorg-x11-utils There appears to be some permissions issue with writing sandbox containers directly to the NFS directories The fix is to just write on the host system. If I can't delete a file, just do this first: sudo chmod -R +rw cbag_centos7_sandbox.sif/ rm -rf cbag_centos7_sandbox.sif/ Determining dependencies on Linux: show library dependencies (pretty simple, actually): ldd \"executable name\" including nested dependencies ldd -v \"executable name\" One has to target the direct (and full) path to the binary: /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso Interestingly, the are a lot of libraries which are not found, even from within the apptainer: Apptainer> ldd /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso) /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso) /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.22' not found (required by /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso) /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso: /lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso) /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso) linux-vdso.so.1 => (0x00007ffd18935000) libap_sh.so => not found libapAssist_sh.so => not found libapInfra_sh.so => not found libpl_sh.so => not found libplt_sh.so => not found libadeStateXML_sh.so => not found libeadSpectreEmir.so => not found libvsacpp.so => not found libpthread.so.0 => /lib64/libpthread.so.0 (0x00007fa524600000) libpython3.6m.so.1.0 => not found libvpsl.so => not found libcthDRM.so => not found libcrypt.so.1 => /lib64/libcrypt.so.1 (0x00007fa524200000) libxerces-c-3.1.so => not found libcrypto.so.10 => /lib64/libcrypto.so.10 (0x00007fa523c00000) libPolyBool.so => not found libprotobuf.so.3.12.2.0 => not found libgrpc++.so.1 => not found libgrpc.so.11 => not found libaddress_sorting.so.11 => not found libre2.so => not found libupb.so.11 => not found libz.so.1 => /lib64/libz.so.1 (0x00007fa523800000) .....","title":"Motivation"},{"location":"containers/#motivation","text":"Proprietary EDA tools like Cadence Virtuoso are typically only supported on a handful of operating systems . As machines for ASIC design are typically dedicated for that purpose, this can therefore lead to","title":"Motivation"},{"location":"containers/#containers","text":"Container provide for operating system virtualization, in which the kernel allows the existence of multiple isolated user space instances.","title":"Containers"},{"location":"containers/#intro-to-containers","text":"OS-level virtualization is an operating system (OS) paradigm in which the kernel allows the existence of multiple isolated user space instances, called containers (LXC, Solaris containers, Docker, Podman), zones (Solaris containers), virtual private servers (OpenVZ), partitions, virtual environments (VEs), virtual kernels (DragonFly BSD), or jails (FreeBSD jail or chroot jail).[1] Such instances may look like real computers from the point of view of programs running in them. A computer program running on an ordinary operating system can see all resources (connected devices, files and folders, network shares, CPU power, quantifiable hardware capabilities) of that computer. However, programs running inside of a container can only see the container's contents and devices assigned to the container. A short list of tech to consider: Docker, Fedora CoreOS, Silverblue, Toolbx, Singularity/Apptainer, Podman, LXC, Flatpak In short, unlike a virtual machine, containers provide abstraction at the operating system level. This can be useful for running proprietary GUI-based applications like Cadence Virtuoso, which e We can use a CentOS7 image for setting up and running our cadence development. But how do we then access data? Docker is a single-purpose application virtualization and LXC is a multi-purpose operating system virtualization. If one looks for a portability and scalability of the application / micro-service, Docker and Kubernetes is a good choice. If one would like to have several (or even thousands of) portable systems on a single computer, LXC is a good choice. Docker was not designed out of the box for GUI apps, so you need to have a video server with X11 typically for that. Linux containers are all based on the virtualization, isolation, and resource management mechanisms provided by the Linux kernel , notably Linux namespaces and cgroups .","title":"Intro to Containers"},{"location":"containers/#os-level-virtualization-containers-and-application-virtualization-technologies","text":"With this tech, different distributions are fine, but other operating systems or kernels are not. Full application virtualization requires a virtualization layer.[ 2] Application virtualization layers replace part of the runtime environment normally provided by the operating system. The layer intercepts all disk operations of virtualized applications and transparently redirects them to a virtualized location, often a single file.[ 3] The application remains unaware that it accesses a virtual resource instead of a physical one. Since the application is now working with one file instead of many files spread throughout the system, it becomes easy to run the application on a different computer and previously incompatible applications can be run side by side. A container image is simply a file (or collection of files) saved on disk that stores everything you need to run a target application or applications: code runtime system tools libraries etc A container process is simply a standard (Linux) process running on top of the underlying host's operating system and kernel, but whose software environment is defined by the contents of the container image. How do I develop in a container, when I need to run a GUI as part of my workflow? What are the limits of containerization?","title":"OS-Level Virtualization (Containers) and Application Virtualization Technologies:"},{"location":"containers/#limitations-of-containers","text":"Architecture dependent; always limited by CPU architecture (x86 vs ARM) and binary format (ELF) Portability: Requires glibc and kernel compatibility between host and container; also requires any other kernel-user space API compatibility (e.g., OFED/IB, NVIDIA/GPUs) Like something built on Ubuntu 22.04 wouldn't work on CentOS 6 Filesystem isolation: Filesystem paths are (mostly) different when viewed inside and outside container By default containers can't see the contents of the host file system. To access host filesystem from inside the file system requires a bit of extra work to 'bind' it.","title":"Limitations of Containers"},{"location":"containers/#most-common-use-cases","text":"Building and running applications that require older/newer system libraries than are available on the host system Most modern tools, like PyTorch, assume the latest packages, and expect a debian-based environment like Ubuntu, which most HPC system aren't doing. Containers can solve these incompatibilities. Running commercial applications binaries that have specific OS requirements not met by the host system. Your license agreement may only give you a compiled binary, which you're Oh! Wow this is exactly my use case. How do I build a repoducible environment when commercial binaries (maybe even with GUI?) are part of the workflow? Converting prexisting Docker containers, which won't work for HPC clusters, to Singularity containers","title":"Most Common Use Cases"},{"location":"containers/#workflow","text":"Build your Singularity containers on you local system where you have root or sudo access; e.g., a personal computer where you have installed Singularity You can't work on native MacOS, as you need to use a Virtual machine with a Ubuntu or Fedora-based OS. This is doubly true, if your Mac has an M1 ARM chipset. Transfer your Singularity containers to the HPC system where you want to run them Run your Singularity containers on that HPC system","title":"Workflow"},{"location":"containers/#why-apptainer-open-source-singularity-fork","text":"","title":"Why Apptainer? (open-source Singularity fork)"},{"location":"containers/#docker","text":"Really designed for network centric services like web servers and databases, but not really meant for HPC systems where many users are sharing a space. Docker assumes you have trust for all users running on systems. Whereas in HPC, the admins don't even trust the users. Docker also wasn't designed to support batch-based workflows Docker not designed to support tightly-coupled, highly distributed parallel applications (MPI)","title":"Docker"},{"location":"containers/#singularity","text":"Designed at Berkeley Lab as the equivalent for HPC. Each container is a single (read-only) image file (unlike the layered arch. in Docker). If you want to change it, you have to to rebuild it. No root owned daemon processes Support shared/multi-tenant resource environments Support HPC Hardware: Infiniband, GPUs Supports HPC applications: MPI","title":"Singularity"},{"location":"containers/#conclusion-use-apptainer","text":"Overview of why not Docker, and the better options: https://blog.jwf.io/2019/08/hpc-workloads-containers/ Using Singularity/Apptainer with GUI commercial applications: https://learningpatterns.me/posts/2018-01-30-abaqus-singularity/ https://apptainer.org","title":"Conclusion: Use Apptainer"},{"location":"containers/#starting-cadence-virtuoso-ic617-and-ic618-from-fedora-linux-client","text":"Install apptainer sudo dnf install apptainer Create a .def file with the name virtuoso_centos7.def : Bootstrap: docker From: centos:7 %setup #run on the host system, after the base container OS is installed. Filepaths are relative to host (fedora) mkdir ${APPTAINER_ROOTFS}/users mkdir ${APPTAINER_ROOTFS}/tools %environment # IC617 doesn't recognize the OS environment properly, need to specify it manually export OA_UNSUPPORTED_PLAT=linux_rhel50_gcc48x %post #section to download and install packages before container is immutable #CentOS 7 image on dockerhub isn't updated, so run this first yum -y update && yum clean all #list of packages required by Cadence Virtusoso IC618 yum install -y csh tcsh glibc elfutils-libelf ksh mesa-libGL mesa-libGLU motif libXp libpng libjpeg-turbo expat glibc-devel gdb xorg-x11-fonts-misc xorg-x11-fonts-ISO8859-1-75dpi redhat-lsb libXScrnSaver apr apr-util compat-db47 xorg-x11-server-Xvfb mesa-dri-drivers openssl-devel #For IC617 and below, i686 package versions are required to support 32-bit binaries yum install -y glibc.i686 elfutils-libelf.i686 mesa-libGL.i686 mesa-libGLU.i686 motif.i686 libXp.i686 libpng.i686 libjpeg-turbo.i686 expat.i686 glibc-devel.i686 redhat-lsb.i686 # additional packages required by Cliosoft SOS yum install -y libXpm # additional package required for X11 forwarding yum install -y xorg-x11-utils Build the container, in an immutable mode: apptainer build virtuoso_centos7.sif virtuoso_centos7.def On the host machine, before running the startup script, we must start a xhost + , as discussed in this solution found here : xhost + Run the immutable container, and start a shell inside: apptainer shell -B /tools,/users virtuoso_centos7.sif Inside the container, check the system configuration with the Cadence provided tool checkSysConf : Apptainer> /cadence/cadence/IC618/tools.lnx86/bin/checkSysConf IC6.1.8 or Apptainer> /tools/cadence/IC618/tools.lnx86/bin/checkSysConf IC6.1.8 Finally, run the virtuoso start-up script, using the full path to tcsh . Make sure the start up script contains the line virtuoso & . Apptainer> /bin/tcsh ./startup.sh Else you need to source the script instead, and run the command virtuoso & manually.","title":"Starting Cadence Virtuoso (IC617 and IC618) from Fedora Linux Client"},{"location":"containers/#troubleshooting","text":"If you are not able to start Cadence, ensure your environment has both tcsh and ksh installed. Check to make sure the cadence executable is known: which virutoso To check the current shell interpreter: # Display your current shell name reliably. ps -p $$ # Print the shell for the current user but not necessarily the shell that is running at the movement. echo \"$SHELL\" # Another reliable and simple method to get the current shell interpreter name on Linux or Unix-like systems. echo $0","title":"Troubleshooting"},{"location":"containers/#changing-shell","text":"Need the tcsh and chsh commands to be To check which shell is active: cat /etc/passwd | grep cd; pwd To change shell, by running this and then restarting termina: chsh -s /usr/bin/tcsh This works now, but I needed ksh installed. okay, still complaining that it's not a supported OS. Tried running the checkSysConf tool: /cadence/cadence/IC618/tools.lnx86/bin/checkSysConf Okay, I'm still having issues, and cadence isn't giving me any useful messages. I've ruled the issue with wayland and Xorg, and I've switched to Xorg. My next step is to figure out how to get more verbose information on why Virtuoso isn't starting. All it says right now is: 2022/11/30 13:13:40 System is not a supported distribution 2022/11/30 13:13:40 An error occurred. We don't recognize OS 2022/11/30 13:13:40 WARNING This OS does not appear to be a Cadence supported Linux configuration. 2022/11/30 13:13:40 For more info, please run CheckSysConf in <cdsRoot/tools.lnx86/bin/checkSysConf <productId> Before I start hacking away and changing packages like openssl-devel to be more inline with the CentOS7 system, perhaps I can figure out better exactly what may need to change. Let's look at the CheckSysConf script and try to figure out what the latest supported version of the packages is. What version of Ubuntu is supported? Maybe there is a log message being written somewhere, and the people here might know where to find that log message.","title":"Changing shell"},{"location":"containers/#which-package-provides-file-on-rhel-based-system","text":"dnf provides ./filename Then check why a package was explici The run , exec , and shell commands are the three primary ways in which to interact with a container image. The run command will start the container, run the scripts marked for execution inside the container (if any), and then exit. The exec commands allows a one time command to be run inside the shell, which then exits. The shell command allows an interactive shell to be spun up, which can be exited at will. The first option is good for image usage once it is finalized. The latter two are best for prototyping when building in sandbox mode. WARNING: The sandbox contain files/dirs that cannot be removed with 'rm'. WARNING: Use 'chmod -R u+rwX' to set permissions that allow removal with 'rm -rf' WARNING: Use the '--fix-perms' option to 'apptainer build' to modify permissions at build time. I need to install a couple packages within this container. The right process is not to enter a shell within the container and run sudo yum install. Tomorrow: bind mount the cadence folder install the lsb-release package run the cadence compatibility script, and manually specify all the packages to install, until this check is passed figure out how to start the gui of the container write define file to make the above reproducible docs commerical GUI NIH tutorail Starting the container with Cadence folder available: apptainer shell -B /cadence:/cadence virtuoso_centos7.sif Okay, so I can't use the --writable option and have stuff automatically bind-mounted. Therefor in sandbox mode, I at least need to create the mountpoints manually. No the better $ apptainer build virtuoso_centos7_immut.sif virtuoso_centos7.def $ xhost + $ apptainer shell -B /cadence:/cadence virtuoso_centos7_immut.sif Apptainer> /cadence/cadence/IC618/tools.lnx86/bin/checkSysConf IC6.1.8 Apptainer> /bin/tcsh /faust/user/kcaisley/cadence/tsmc65/tsmc_crn65lp_1.7a On the host machine, before running the startup script, we must start a xhost + Solution found here for the whole bash vs tcsh problem: ps -p $$ \u2013 Display your current shell name reliably. echo \"$SHELL\" \u2013 Print the shell for the current user but not necessarily the shell that is running at the movement. echo $0 \u2013 Another reliable and simple method to get the current shell interpreter name on Linux or Unix-like systems. I may want to look at passing the --nv flag when trying to do heavy layout work flows. I think that the reason why xdpyinfo didn't work is because I hadn't made the xhost visible to the container.","title":"Which package provides file on RHEL Based System"},{"location":"containers/#to-find-which-package-a-utility-is-provided-by-that-isnt-downloaded-yet","text":"yum provides /usr/bin/xdpyinfo If the package were downloaded, and if which were also on the system, we could do this instead: yum provides `which free` Anyways, we find the right package to install for it is yum info xorg-x11-utils Yes, it does appear that the package is no longer missing. However, now there is no $DISPLAY env variable, so I can't test it. This is because I'm command line only for now. check where container is running Further more, some notes from stuff I did in BAG:","title":"To find which package a utility is provided by (that isn't downloaded yet)"},{"location":"containers/#container-creation","text":"sudo dnf install apptainer apptainer build --sandbox cbag_centos7.sif docker://centos:7 apptainer shell --fakeroot --writable cbag_centos7.sif/","title":"Container Creation"},{"location":"containers/#for-centos7","text":"yum -y update && yum clean all yum install centos-release-scl yum install devtoolset-8 yum install epel-release yum install which yaml-cpp yaml-cpp-devel fmt fmt-devel spdlog spdlog-devel cmake3 //note cmake3 source /opt/rh/devtoolset-8/enable cd to folder and export CC and CXX tried to build, but didn't work as spdlog wasn't new enough to include .cmake files","title":"For Centos7:"},{"location":"containers/#for-rockylinux-87","text":"dnf -y update && dnf clean all dnf install epel-release dnf install which yaml-cpp yaml-cpp-devel fmt fmt-devel spdlog spdlog-devel cmake gcc gcc-c++ boost boost-devel starts compiling after stting static boost to OFF, but then fails at compiling with boost...","title":"For RockyLinux 8.7:"},{"location":"containers/#for-rockylinux-91","text":"dnf -y update && dnf clean all dnf -y install epel-release dnf -y install which yaml-cpp yaml-cpp-devel fmt fmt-devel spdlog spdlog-devel cmake gcc gcc-c++ boost boost-devel","title":"For RockyLinux 9.1:"},{"location":"containers/#final-container-design-for-cadence-this-ist-he-def-file-used-to-generate-an-immutable-container","text":"Bootstrap: docker From: centos:7 %setup #run on the host system, after the base container OS is installed. Filepaths are relative to host (fedora) mkdir ${APPTAINER_ROOTFS}/cadence %post #section to download and install packages before container is immutable #CentOS 7 image on dockerhub isn't updated, so run this first yum -y update && yum clean all #list of packages required by Cadence yum install -y csh tcsh glibc elfutils-libelf ksh mesa-libGL mesa-libGLU motif libXp libpng libjpeg-turbo expat glibc-devel gdb xorg-x11-fonts-misc xorg-x11-fonts-ISO8859-1-75dpi redhat-lsb libXScrnSaver apr apr-util compat-db47 xorg-x11-server-Xvfb mesa-dri-drivers openssl-devel yum install -y xorg-x11-utils","title":"Final Container Design for Cadence: (This ist he .def file, used to generate an immutable container)"},{"location":"containers/#there-appears-to-be-some-permissions-issue-with-writing-sandbox-containers-directly-to-the-nfs-directories","text":"The fix is to just write on the host system. If I can't delete a file, just do this first: sudo chmod -R +rw cbag_centos7_sandbox.sif/ rm -rf cbag_centos7_sandbox.sif/","title":"There appears to be some permissions issue with writing sandbox containers directly to the NFS directories"},{"location":"containers/#determining-dependencies-on-linux","text":"show library dependencies (pretty simple, actually): ldd \"executable name\" including nested dependencies ldd -v \"executable name\" One has to target the direct (and full) path to the binary: /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso Interestingly, the are a lot of libraries which are not found, even from within the apptainer: Apptainer> ldd /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso) /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso) /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.22' not found (required by /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso) /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso: /lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso) /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /cadence/cadence/IC618/tools.lnx86/dfII/bin/64bit/virtuoso) linux-vdso.so.1 => (0x00007ffd18935000) libap_sh.so => not found libapAssist_sh.so => not found libapInfra_sh.so => not found libpl_sh.so => not found libplt_sh.so => not found libadeStateXML_sh.so => not found libeadSpectreEmir.so => not found libvsacpp.so => not found libpthread.so.0 => /lib64/libpthread.so.0 (0x00007fa524600000) libpython3.6m.so.1.0 => not found libvpsl.so => not found libcthDRM.so => not found libcrypt.so.1 => /lib64/libcrypt.so.1 (0x00007fa524200000) libxerces-c-3.1.so => not found libcrypto.so.10 => /lib64/libcrypto.so.10 (0x00007fa523c00000) libPolyBool.so => not found libprotobuf.so.3.12.2.0 => not found libgrpc++.so.1 => not found libgrpc.so.11 => not found libaddress_sorting.so.11 => not found libre2.so => not found libupb.so.11 => not found libz.so.1 => /lib64/libz.so.1 (0x00007fa523800000) .....","title":"Determining dependencies on Linux:"},{"location":"file_server/","text":"Client NFS Mounting (For Fedora) $ cd / $ sudo mkdir users $ sudo mkdir tools $ sudo vim /etc/fstab penelope.physik.uni-bonn.de:/export/disk/users /users nfs4 defaults 0 0 penelope.physik.uni-bonn.de:/export/disk/tools /tools nfs4 ro 0 0 # To remount, but only works sudo mount -a Base Drive Configurtion Could have used BTRFS as it has built-in volume management, but EXT4 - LVM combination is simple and good enough for our uses mount mkfs.ext4 some basic justification as to why RAID6 was chosen Should I use a raid configuration? If you buy in batches, they fail in batches. Rebuild time is large. The sensible approach moving forward is raid6, raidz2, raiddp. (two disk partiy) Never choose raid5. Use Raid6, raid10, raidz2, or raidz3. 'raid' is used commonly for different configs, where you combine multiple drives. the parameters are mirroring, parity, striping, and if so, how fast? Instant or minutes after ZFS and BTRFS are two options, which aren't 'RAID' in the traditional sense, even though people say it. ZFS and BTRFS is a combination of a file system (based on copy-on-write COW principle) with a logical volume manager. Btrfs is intended to address the lack of pooling, snapshots, checksums, and integral multi-device spanning in Linux file systems. For performance, EXT4 is very fast. In (4 drive) RAID 10, if one drive is failed, there\u2019s a 1-in-3 chance that a second drive failure will take out the whole array. RAID 6 doesn\u2019t have this risk. Also, when an array has more than 4 drives RAID 6 starts to make even more sense, as you can get more useable space out of the drives while still having two-point-failure redundancy. Hardware support for RAID has been deprecated, and so EXT4 on LVM is similar to BTRFS, but is two components so it is more clunky. BTRFS has better snapshotting, and better data integrity. Caching and COW on Raid Array Tutorial for creating RAID6 array: https://www.digitalocean.com/community/tutorials/how-to-create-raid-arrays-with-mdadm-on-ubuntu-22-04 lsblk sudo mdadm --create --verbose /dev/md/0 --level=6 --raid-devices=5 /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf sudo mdadm --query /dev/md/0 array will take time to mirror, but in the mean time can be used. Monitor with: cat /proc/mdstat sudo mkfs.ext4 -F /dev/md/0 Mounting Integrity check The RAID array should be checked on a regular basis to detect emerging issues early. This can be handled by a cron job, but the Fedora default method is systemd. sudo systemctl list-timers --all lists the active timers. In our case, we are interested in raid-check.timer which we can edit with sudo systemctl edit raid-check.timer For example, the entry OnCalendar=*-*-* Fri 19:00:00/4w schedules a check on Friday at 19:00 every 4 weeks. The change is applied with sudo systemctl daemon-reload sudo systemctl restart raid-check.timer To turn the automatic check off, run the following commands: sudo systemctl stop raid-check.timer sudo systemctl disable raid-check.timer sudo systemctl daemon-reload sudo systemctl restart raid-check.timer NFS Server Remirror to Export mount Mounting points /cadence and /faust/user Backup System for File Server We use IBM Spectrum Protect (SP), provided by the HRZ to backup our Penelope file server. The services comes with an annual fee, based on the used data volume. Email notifications will be sent to the contacts after each scheduled activity. Installation The setup procedure for the IBM Tivoli Sorage Manager software (old name for IBM SP) is described on the HRZ Confluence page: https://confluence.team.uni-bonn.de/display/HRZDOK/Einrichtung#. Here's a short summary: Download latest TSM client from http://www-01.ibm.com/support/docview.wss?rs=663&uid=swg21239415 or wget ftp://ftp.software.ibm.com/storage/tivoli-storage-management/patches/client/v8r1/Linux/ Unpack installation archive tar -xvf [filename.tar] Installation (in this order) rpm -ivh gskcrypt* rpm -ivh gskssl64* yum localinstall TIVsm-API64.x86_64.rpm rpm -ivh TIVsm-BA.x86_64.rpm rpm -ivh TIVsm-APIcit.x86_64.rpm rpm -ivh TIVsm-BAcit.x86_64.rpm Configuration Navigation in installation directory cd /opt/tivoli/tsm/client/ba/bin Create the file \"dsm.opt\" and add the configurations sudo vi dsm.opt Insert the following lines (Ctrl + Shift + V) Servername tsm3.rhrz.uni-bonn.de Domain all-local Subdir yes Create the file \"dsm.sys\" and insert the following configuration sudo vi dsm.sys Insert the following lines (Str + Shift + V) but replace [nodename] with the name assigned by HRZ Servername tsm3.rhrz.uni-bonn.de CommMethod tcpip TCPPort 1500 TCPClientPort 1501 WEBPorts 1501,0 NODEname [nodename] TCPServeraddress tsm2.rhrz.uni-bonn.de PASSWORDAccess generate INCLEXCL /opt/tivoli/tsm/client/ba/bin/dsm.excl_incl SCHEDLOGNAME /var/log/tsm/dsmsched.log ERRORLOGNAME /var/log/tsm/dsmerror.log SCHEDLOGRETENTION 7 S ERRORLOGRETENTION 7 S schedmode prompted managedservices schedule 4. Create Include/Exclude file (leave empty, or read more here) sudo touch /opt/tivoli/tsm/client/ba/bin/dsm.excl_incl Running the command line programme sudo dsmc Change password sudo dsmc set passsword [old passwort] [new passwort] Include/exclude list Next we have to define which files and folder to be backed up. This is managed with the file \u00b4/opt/tivoli/tsm/client/ba/bin/dsm.excl_incl\u00b4 as defined in \u00b4dsm.sys\u00b4. Allowed statements are Include/Exclude Statement Options include Includes files in backup exclude Exclude files from backup. Exclude a directory including all subdirectories and files it contains. exclude.dir Subdirectories and files. It is not possible to include options to override exclude.dir exclude.fs Exclude a file system Example: If we want to backup the /home directory but exclude the asiclab user, we would define include /home/* exclude /home/asiclab/* Commands for data store and restore Start by \u00b4$ sudo dsmc\u00b4. Enter in command line after prompt Protect>. This is only a subset. Partitions can also be saved and restored, but this seems to be less relevant for us. Data backup Save the whole folder bash incr /[path]/* Back up individual files bash incr /[path]/[file] Example of a backup procedure: ``` Protect> incr /mnt/md127/vm/* -su=yes Incremental backup of volume '/mnt/md127/vm/ ' Successful incremental backup of '/mnt/md127/vm/ ' Total number of objects inspected: 5 Total number of objects backed up: 0 Total number of objects updated: 0 Total number of objects rebound: 0 Total number of objects deleted: 0 Total number of objects expired: 0 Total number of objects failed: 0 Total number of objects encrypted: 0 Total number of objects grew: 0 Total number of retries: 0 Total number of bytes inspected: 11.69 GB Total number of bytes transferred: 0 B Data transfer time: 0.00 sec Network data transfer rate: 0.00 KB/sec Aggregate data transfer rate: 0.00 KB/sec Objects compressed by: 0% Total data reduction ratio: 100.00% Elapsed processing time: 00:00:01 ``` Query for data backup Includes/Excludes bash q inclexcl Files bash q ba /[path]/* -subdir=yes Restore data When restoring data, make a concious decision about the destination path. Most of the times, it is better to restore data to a temporary folder instead of the original location. 1. Individual files bash rest /[source-path]/[soruce-file] Multiple files, folders and partitions bash rest /[source-path]/* /[destination-path]/ -su=yes Display what is backed up on TSM nodes (and subsequent restore to second folder path when files are selected) bash restore -subdir=yes -pick \"/*\" \"/[destination-path]/\" In this mode, you can interactivly browse the backup and select folders and files. Example: ``` restore -subdir=yes -pick \"/mnt/md127/vm/*\" \"/tmp/restore-test/\" Scrollable PICK Window - Restore # Backup Date/Time File Size A/I File ---------------------------------------------------------------------------------------------------------------- 1. | 03/03/2023 13:29:47 4.00 KB A /mnt/md127/vm 2. | 03/03/2023 13:29:47 5.00 GB A /mnt/md127/vm/noyce.physik.uni-bonn.de-disk1-2022-11-09.img 3. | 03/03/2023 13:29:47 4.00 KB A /mnt/md127/vm/tests 4. | 03/03/2023 13:31:16 5.00 GB A /mnt/md127/vm/tests/noyce.physik.uni-bonn.de-disk1.img 5. | 03/03/2023 13:32:42 1.69 GB A /mnt/md127/vm/tests/noyce.physik.uni-bonn.de-disk1.qcow2 | | | | | | | | | | | | | | | | 0---------10--------20--------30--------40--------50--------60--------70--------80--------90--------100-------11 =Up =Down =Top =Bottom =Right =Left =Goto Line # <#>=Toggle Entry <+>=Select All <->=Deselect All <#:#+>=Select A Range <#:#->=Deselect A Range =Ok =Cancel pick> ``` If the files already exist in the destination, you will be prompted to descide whether you want to replace or skip the object in questions. ``` --- User Action is Required --- File '/tmp/restore-test/vm/tests/noyce.physik.uni-bonn.de-disk1.img' exists Select an appropriate action 1. Replace this object 2. Replace all objects that already exist 3. Skip this object 4. Skip all objects that already exist A. Abort this operation Action [1,2,3,4,A] : ``` Raid Server management lsblk command is useful foor checking drive names mdadm --query option Free space df -h Used space du (??options?) List Hardware devices: lsblk To check the status of current NFS or SMB shares, use the following (TYPE = nfs4, smb, autofs,cifs,etc) mount -l -t TYPE Software Raid Re-Setup: https://serverfault.com/questions/32709/how-do-i-move-a-linux-software-raid-to-a-new-machine mdadm --assemble --scan --verbose /dev/md{number} /dev/{disk1} /dev/{disk2} /dev/{disk3} /dev/{disk4} it may be automatically detected and rebuilt, and so you'll just need to: sudo mkdir mnt/raid/ sudo mount /dev/md127/ /mnt/raid/ NFS List PIDs of NFS shares on a host/server machine: service nfs status (CentOS6) or systemctl status nfs (CentOS7) Show status of nfs-mountd service on NFS clients (doesn't exist on CentOS 6) systemctl status nfs-mountd On host, show info on what clients are mounting an NFS server (should show nothing on client machines) showmount --all Resolve DNS names Reverse resolve a IP address to a name, via the local DNS: nslookup 131.220.166.32 (etc) Resolve a name to an IP: nslookup faust02.physik.uni-bonn.de List currently connected clients from host server: sudo netstat -a | grep :nfs Report status of current NFS4 mounts on client: mount -l -t nfs4 Ping remote host from client, to see connection options: showmount -e penelope.physik.uni-bonn.de Mount a directory on client sudo mount -t nfs4 penelope.physik.uni-bonn.de:/export/disk/users /faust/user sudo mount -t nfs4 penelope.physik.uni-bonn.de:/export/disk/cadence /cadence Setup of new NFS server on Fedora: ip should be: 131.220.165.56 locations: /export/disk/users and /export/disk/cadence NFSv4 exports exist in a single pseudo filesystem , where the real directories are mounted with the --bind option. Here is some additional information regarding this fact. Let's say we want to export our users' home directories in /home/users . First we create the export filesystem: sudo mkdir /export sudo mkdir /export/users and mount the real users directory with: sudo mount --bind /home/users /export/users To save us from retyping this after every reboot we add the following line to /etc/fstab : /home/users /export/users none bind 0 0 Actually setting up NFS: a good tutorial: https://www.redhat.com/sysadmin/nfs-server-client Let Penelope read from noyce LDAP server/ make sure UID and GIDs are correct: https://docs.fedoraproject.org/en-US/fedora/latest/system-administrators-guide/servers/Directory_Servers/ Start NFS Server on Penelope sudo systemctl start nfs-server.service sudo systemctl enable nfs-server.service List the current versions of NFS running: (NFS3 vs 4) rpcinfo -p | grep nfs create mount points For legacy export locations: sudo mkdir -p /export/disks The -p creates necessary parent directories. sudo mount --bind /mnt/md0/ /export/disk/ (using bind mount, rather than regular, as we both locations must be readable, rather than one being a /dev/ location) this is bad form, one should instead add both the ext4 mount and the bind mount to the /etc/fstab file: sudo vim /etc/fstab /dev/md127 /mnt/md127 ext4 defaults,nofail,discard 0 0 /mnt/md127/users /export/disk/users none bind # old bind for tools /mnt/md127/tools /export/disk/cadence none bind # new bind mount for tools /mnt/md127/tools /export/disk/tools none bind After saving, make sure to restart the systemctl daemon and remount: sudo systemctl daemon-reload sudo mount -a Properly set permissions of export directory before exporting: Now that our directory to be exported is availalb, we must create/config the /etc/exports NFS file: (don't use the last location, as it is just for backups) /export/disk/cadence apollo(rw,sync,no_root_squash,no_all_squash) faust02(rw,sync,no_root_squash,no_all_squash) asiclab*(ro,sync,root_squash,no_all_squash) jupiter(ro,sync,root_squash,no_all_squash) noyce(ro,sync,root_squash,no_all_squash) juno(rw,sync,root_squash,no_all_squash) /export/disk/users asiclab*(rw,root_squash,async,no_subtree_check) apollo(rw,async,no_root_squash,no_subtree_check) faust02(rw,async,no_root_squash,no_all_squash,no_subtree_check) jupiter(rw,async,root_squash,no_all_squash,no_subtree_check) noyce(rw,async,root_squash,no_all_squash,no_subtree_check) penelope(rw,async,root_squash,no_all_squash,no_subtree_check) juno(rw,async,root_squash,no_all_squash,no_subtree_check) /export/backup apollo(rw,sync,no_root_squash,no_all_squash) Run export command on sudo exportfs -rav This command should only be run once, unless you change something. If you see complaints about stale file handles from the clients, one should do: exportfs -ua cat /proc/fs/nfs/exports this file should be empty after previous command. Also, debug here for duplicates exportfs -a Allow through firewall: https://www.redhat.com/sysadmin/nfs-server-client https://www.redhat.com/sysadmin/nfs-server-client sudo firewall-cmd --permanent --add-service=nfs sudo firewall-cmd --permanent --add-service=rpc-bind (needed for NFS3 configs) sudo firewall-cmd --permanent --add-service=mountd (needed for NFS3 configs) sudo firewall-cmd --reload more complicated commands, for NFS4 only: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_file_systems/configuring-an-nfsv4-only-server_managing-file-systems#benefits-and-drawbacks-of-an-nfsv4-only-server_configuring-an-nfsv4-only-server The default installation installs NFS, but neither enables it nor configures the firewall accordingly. [\u2026]# systemctl enable nfs-server --now [\u2026]# systemctl status nfs-server [\u2026]# firewall-cmd --permanent --add-service=nfs [\u2026]# firewall-cmd --reload Older, potentially redundat notes: automatically mount files system $ sudo vim /etc/fstab Add lines: penelope.physik.uni-bonn.de:/export/disk/users /faust/user nfs4 defaults 0 0 penelope.physik.uni-bonn.de:/export/disk/cadence /cadence nfs4 ro 0 0 options for the mounting drives are: https://help.ubuntu.com/community/Fstab#Options https://wiki.archlinux.org/title/Fstab Updated mount points: cd / sudo mkdir users sudo mkdir tools penelope.physik.uni-bonn.de:/export/disk/users /users nfs4 defaults 0 0 penelope.physik.uni-bonn.de:/export/disk/tools /tools nfs4 ro 0 0 tutorial to write make sure primary group is 'faust', else nobody permission will happen https://unix.stackexchange.com/questions/186568/what-is-nobody-user-and-group The nobody user is a pseudo user in many Unixes and Linux distributions, which is used to represent the user/group with the lowest permissions. In the best case that user and its group are not assigned to any file or directory (as owner). One application that makes use of this group through is NFS. If the owner of a file or directory in a mounted NFS share doesn't exist at the local system, it is replaced by the nobody user and its group. Take back any files that have been made under user or personal group name, and assign them to faust Note: the general status of the penelope server can be found by checking the webserver status page. Trying to debug slow speeds on NFS server. Programs like Typora take forever to open on an NFS user, but with local users is very fast. Read first about Raid Array Speeds: https://www.raid-calculator.com/raid-types-reference.aspx Our 5 disk, 14.6TB per disk machine in Raid 6 has: Capacity 43.8 TB Speed gain 3x read speed, no write speed gain Fault tolerance 2-drive failure On the server side: https://linux.die.net/man/5/exports When mounting: noatime: Setting this value disables the NFS server from updating the inodes access time. As most applications do not necessarily need this value, you can safely disable this updating. nocto: Suppress the retrieval of new attributes when creating a file. This page has lots of things tried: https://serverfault.com/questions/682000/nfs-poor-write-performance","title":"Client NFS Mounting (For Fedora)"},{"location":"file_server/#client-nfs-mounting-for-fedora","text":"$ cd / $ sudo mkdir users $ sudo mkdir tools $ sudo vim /etc/fstab penelope.physik.uni-bonn.de:/export/disk/users /users nfs4 defaults 0 0 penelope.physik.uni-bonn.de:/export/disk/tools /tools nfs4 ro 0 0 # To remount, but only works sudo mount -a","title":"Client NFS Mounting (For Fedora)"},{"location":"file_server/#base-drive-configurtion","text":"Could have used BTRFS as it has built-in volume management, but EXT4 - LVM combination is simple and good enough for our uses mount mkfs.ext4","title":"Base Drive Configurtion"},{"location":"file_server/#some-basic-justification-as-to-why-raid6-was-chosen","text":"Should I use a raid configuration? If you buy in batches, they fail in batches. Rebuild time is large. The sensible approach moving forward is raid6, raidz2, raiddp. (two disk partiy) Never choose raid5. Use Raid6, raid10, raidz2, or raidz3. 'raid' is used commonly for different configs, where you combine multiple drives. the parameters are mirroring, parity, striping, and if so, how fast? Instant or minutes after ZFS and BTRFS are two options, which aren't 'RAID' in the traditional sense, even though people say it. ZFS and BTRFS is a combination of a file system (based on copy-on-write COW principle) with a logical volume manager. Btrfs is intended to address the lack of pooling, snapshots, checksums, and integral multi-device spanning in Linux file systems. For performance, EXT4 is very fast. In (4 drive) RAID 10, if one drive is failed, there\u2019s a 1-in-3 chance that a second drive failure will take out the whole array. RAID 6 doesn\u2019t have this risk. Also, when an array has more than 4 drives RAID 6 starts to make even more sense, as you can get more useable space out of the drives while still having two-point-failure redundancy. Hardware support for RAID has been deprecated, and so EXT4 on LVM is similar to BTRFS, but is two components so it is more clunky. BTRFS has better snapshotting, and better data integrity. Caching and COW on","title":"some basic justification as to why RAID6 was chosen"},{"location":"file_server/#raid-array","text":"Tutorial for creating RAID6 array: https://www.digitalocean.com/community/tutorials/how-to-create-raid-arrays-with-mdadm-on-ubuntu-22-04 lsblk sudo mdadm --create --verbose /dev/md/0 --level=6 --raid-devices=5 /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf sudo mdadm --query /dev/md/0 array will take time to mirror, but in the mean time can be used. Monitor with: cat /proc/mdstat sudo mkfs.ext4 -F /dev/md/0","title":"Raid Array"},{"location":"file_server/#mounting","text":"","title":"Mounting"},{"location":"file_server/#integrity-check","text":"The RAID array should be checked on a regular basis to detect emerging issues early. This can be handled by a cron job, but the Fedora default method is systemd. sudo systemctl list-timers --all lists the active timers. In our case, we are interested in raid-check.timer which we can edit with sudo systemctl edit raid-check.timer For example, the entry OnCalendar=*-*-* Fri 19:00:00/4w schedules a check on Friday at 19:00 every 4 weeks. The change is applied with sudo systemctl daemon-reload sudo systemctl restart raid-check.timer To turn the automatic check off, run the following commands: sudo systemctl stop raid-check.timer sudo systemctl disable raid-check.timer sudo systemctl daemon-reload sudo systemctl restart raid-check.timer","title":"Integrity check"},{"location":"file_server/#nfs-server","text":"","title":"NFS Server"},{"location":"file_server/#remirror-to-export","text":"mount Mounting points /cadence and /faust/user","title":"Remirror to Export"},{"location":"file_server/#backup-system-for-file-server","text":"We use IBM Spectrum Protect (SP), provided by the HRZ to backup our Penelope file server. The services comes with an annual fee, based on the used data volume. Email notifications will be sent to the contacts after each scheduled activity.","title":"Backup System for File Server"},{"location":"file_server/#installation","text":"The setup procedure for the IBM Tivoli Sorage Manager software (old name for IBM SP) is described on the HRZ Confluence page: https://confluence.team.uni-bonn.de/display/HRZDOK/Einrichtung#. Here's a short summary: Download latest TSM client from http://www-01.ibm.com/support/docview.wss?rs=663&uid=swg21239415 or wget ftp://ftp.software.ibm.com/storage/tivoli-storage-management/patches/client/v8r1/Linux/ Unpack installation archive tar -xvf [filename.tar] Installation (in this order) rpm -ivh gskcrypt* rpm -ivh gskssl64* yum localinstall TIVsm-API64.x86_64.rpm rpm -ivh TIVsm-BA.x86_64.rpm rpm -ivh TIVsm-APIcit.x86_64.rpm rpm -ivh TIVsm-BAcit.x86_64.rpm","title":"Installation"},{"location":"file_server/#configuration","text":"Navigation in installation directory cd /opt/tivoli/tsm/client/ba/bin Create the file \"dsm.opt\" and add the configurations sudo vi dsm.opt Insert the following lines (Ctrl + Shift + V) Servername tsm3.rhrz.uni-bonn.de Domain all-local Subdir yes Create the file \"dsm.sys\" and insert the following configuration sudo vi dsm.sys Insert the following lines (Str + Shift + V) but replace [nodename] with the name assigned by HRZ Servername tsm3.rhrz.uni-bonn.de CommMethod tcpip TCPPort 1500 TCPClientPort 1501 WEBPorts 1501,0 NODEname [nodename] TCPServeraddress tsm2.rhrz.uni-bonn.de PASSWORDAccess generate INCLEXCL /opt/tivoli/tsm/client/ba/bin/dsm.excl_incl SCHEDLOGNAME /var/log/tsm/dsmsched.log ERRORLOGNAME /var/log/tsm/dsmerror.log SCHEDLOGRETENTION 7 S ERRORLOGRETENTION 7 S schedmode prompted managedservices schedule 4. Create Include/Exclude file (leave empty, or read more here) sudo touch /opt/tivoli/tsm/client/ba/bin/dsm.excl_incl Running the command line programme sudo dsmc Change password sudo dsmc set passsword [old passwort] [new passwort]","title":"Configuration"},{"location":"file_server/#includeexclude-list","text":"Next we have to define which files and folder to be backed up. This is managed with the file \u00b4/opt/tivoli/tsm/client/ba/bin/dsm.excl_incl\u00b4 as defined in \u00b4dsm.sys\u00b4. Allowed statements are Include/Exclude Statement Options include Includes files in backup exclude Exclude files from backup. Exclude a directory including all subdirectories and files it contains. exclude.dir Subdirectories and files. It is not possible to include options to override exclude.dir exclude.fs Exclude a file system Example: If we want to backup the /home directory but exclude the asiclab user, we would define include /home/* exclude /home/asiclab/*","title":"Include/exclude list"},{"location":"file_server/#commands-for-data-store-and-restore","text":"Start by \u00b4$ sudo dsmc\u00b4. Enter in command line after prompt Protect>. This is only a subset. Partitions can also be saved and restored, but this seems to be less relevant for us.","title":"Commands for data store and restore"},{"location":"file_server/#data-backup","text":"Save the whole folder bash incr /[path]/* Back up individual files bash incr /[path]/[file] Example of a backup procedure: ``` Protect> incr /mnt/md127/vm/* -su=yes Incremental backup of volume '/mnt/md127/vm/ ' Successful incremental backup of '/mnt/md127/vm/ ' Total number of objects inspected: 5 Total number of objects backed up: 0 Total number of objects updated: 0 Total number of objects rebound: 0 Total number of objects deleted: 0 Total number of objects expired: 0 Total number of objects failed: 0 Total number of objects encrypted: 0 Total number of objects grew: 0 Total number of retries: 0 Total number of bytes inspected: 11.69 GB Total number of bytes transferred: 0 B Data transfer time: 0.00 sec Network data transfer rate: 0.00 KB/sec Aggregate data transfer rate: 0.00 KB/sec Objects compressed by: 0% Total data reduction ratio: 100.00% Elapsed processing time: 00:00:01 ```","title":"Data backup"},{"location":"file_server/#query-for-data-backup","text":"Includes/Excludes bash q inclexcl Files bash q ba /[path]/* -subdir=yes","title":"Query for data backup"},{"location":"file_server/#restore-data","text":"When restoring data, make a concious decision about the destination path. Most of the times, it is better to restore data to a temporary folder instead of the original location. 1. Individual files bash rest /[source-path]/[soruce-file] Multiple files, folders and partitions bash rest /[source-path]/* /[destination-path]/ -su=yes Display what is backed up on TSM nodes (and subsequent restore to second folder path when files are selected) bash restore -subdir=yes -pick \"/*\" \"/[destination-path]/\" In this mode, you can interactivly browse the backup and select folders and files. Example: ``` restore -subdir=yes -pick \"/mnt/md127/vm/*\" \"/tmp/restore-test/\" Scrollable PICK Window - Restore # Backup Date/Time File Size A/I File ---------------------------------------------------------------------------------------------------------------- 1. | 03/03/2023 13:29:47 4.00 KB A /mnt/md127/vm 2. | 03/03/2023 13:29:47 5.00 GB A /mnt/md127/vm/noyce.physik.uni-bonn.de-disk1-2022-11-09.img 3. | 03/03/2023 13:29:47 4.00 KB A /mnt/md127/vm/tests 4. | 03/03/2023 13:31:16 5.00 GB A /mnt/md127/vm/tests/noyce.physik.uni-bonn.de-disk1.img 5. | 03/03/2023 13:32:42 1.69 GB A /mnt/md127/vm/tests/noyce.physik.uni-bonn.de-disk1.qcow2 | | | | | | | | | | | | | | | | 0---------10--------20--------30--------40--------50--------60--------70--------80--------90--------100-------11 =Up =Down =Top =Bottom =Right =Left =Goto Line # <#>=Toggle Entry <+>=Select All <->=Deselect All <#:#+>=Select A Range <#:#->=Deselect A Range =Ok =Cancel pick> ``` If the files already exist in the destination, you will be prompted to descide whether you want to replace or skip the object in questions. ``` --- User Action is Required --- File '/tmp/restore-test/vm/tests/noyce.physik.uni-bonn.de-disk1.img' exists Select an appropriate action 1. Replace this object 2. Replace all objects that already exist 3. Skip this object 4. Skip all objects that already exist A. Abort this operation Action [1,2,3,4,A] : ```","title":"Restore data"},{"location":"file_server/#raid-server-management","text":"lsblk command is useful foor checking drive names mdadm --query option Free space df -h Used space du (??options?) List Hardware devices: lsblk To check the status of current NFS or SMB shares, use the following (TYPE = nfs4, smb, autofs,cifs,etc) mount -l -t TYPE","title":"Raid Server management"},{"location":"file_server/#software-raid-re-setup","text":"https://serverfault.com/questions/32709/how-do-i-move-a-linux-software-raid-to-a-new-machine mdadm --assemble --scan --verbose /dev/md{number} /dev/{disk1} /dev/{disk2} /dev/{disk3} /dev/{disk4} it may be automatically detected and rebuilt, and so you'll just need to: sudo mkdir mnt/raid/ sudo mount /dev/md127/ /mnt/raid/","title":"Software Raid Re-Setup:"},{"location":"file_server/#nfs","text":"List PIDs of NFS shares on a host/server machine: service nfs status (CentOS6) or systemctl status nfs (CentOS7) Show status of nfs-mountd service on NFS clients (doesn't exist on CentOS 6) systemctl status nfs-mountd On host, show info on what clients are mounting an NFS server (should show nothing on client machines) showmount --all Resolve DNS names Reverse resolve a IP address to a name, via the local DNS: nslookup 131.220.166.32 (etc) Resolve a name to an IP: nslookup faust02.physik.uni-bonn.de List currently connected clients from host server: sudo netstat -a | grep :nfs Report status of current NFS4 mounts on client: mount -l -t nfs4 Ping remote host from client, to see connection options: showmount -e penelope.physik.uni-bonn.de Mount a directory on client sudo mount -t nfs4 penelope.physik.uni-bonn.de:/export/disk/users /faust/user sudo mount -t nfs4 penelope.physik.uni-bonn.de:/export/disk/cadence /cadence","title":"NFS"},{"location":"file_server/#setup-of-new-nfs-server-on-fedora","text":"ip should be: 131.220.165.56 locations: /export/disk/users and /export/disk/cadence NFSv4 exports exist in a single pseudo filesystem , where the real directories are mounted with the --bind option. Here is some additional information regarding this fact. Let's say we want to export our users' home directories in /home/users . First we create the export filesystem: sudo mkdir /export sudo mkdir /export/users and mount the real users directory with: sudo mount --bind /home/users /export/users To save us from retyping this after every reboot we add the following line to /etc/fstab : /home/users /export/users none bind 0 0","title":"Setup of new NFS server on Fedora:"},{"location":"file_server/#actually-setting-up-nfs","text":"a good tutorial: https://www.redhat.com/sysadmin/nfs-server-client Let Penelope read from noyce LDAP server/ make sure UID and GIDs are correct: https://docs.fedoraproject.org/en-US/fedora/latest/system-administrators-guide/servers/Directory_Servers/ Start NFS Server on Penelope sudo systemctl start nfs-server.service sudo systemctl enable nfs-server.service List the current versions of NFS running: (NFS3 vs 4) rpcinfo -p | grep nfs create mount points For legacy export locations: sudo mkdir -p /export/disks The -p creates necessary parent directories. sudo mount --bind /mnt/md0/ /export/disk/ (using bind mount, rather than regular, as we both locations must be readable, rather than one being a /dev/ location) this is bad form, one should instead add both the ext4 mount and the bind mount to the /etc/fstab file: sudo vim /etc/fstab /dev/md127 /mnt/md127 ext4 defaults,nofail,discard 0 0 /mnt/md127/users /export/disk/users none bind # old bind for tools /mnt/md127/tools /export/disk/cadence none bind # new bind mount for tools /mnt/md127/tools /export/disk/tools none bind After saving, make sure to restart the systemctl daemon and remount: sudo systemctl daemon-reload sudo mount -a Properly set permissions of export directory before exporting: Now that our directory to be exported is availalb, we must create/config the /etc/exports NFS file: (don't use the last location, as it is just for backups) /export/disk/cadence apollo(rw,sync,no_root_squash,no_all_squash) faust02(rw,sync,no_root_squash,no_all_squash) asiclab*(ro,sync,root_squash,no_all_squash) jupiter(ro,sync,root_squash,no_all_squash) noyce(ro,sync,root_squash,no_all_squash) juno(rw,sync,root_squash,no_all_squash) /export/disk/users asiclab*(rw,root_squash,async,no_subtree_check) apollo(rw,async,no_root_squash,no_subtree_check) faust02(rw,async,no_root_squash,no_all_squash,no_subtree_check) jupiter(rw,async,root_squash,no_all_squash,no_subtree_check) noyce(rw,async,root_squash,no_all_squash,no_subtree_check) penelope(rw,async,root_squash,no_all_squash,no_subtree_check) juno(rw,async,root_squash,no_all_squash,no_subtree_check) /export/backup apollo(rw,sync,no_root_squash,no_all_squash) Run export command on sudo exportfs -rav This command should only be run once, unless you change something. If you see complaints about stale file handles from the clients, one should do: exportfs -ua cat /proc/fs/nfs/exports this file should be empty after previous command. Also, debug here for duplicates exportfs -a Allow through firewall: https://www.redhat.com/sysadmin/nfs-server-client https://www.redhat.com/sysadmin/nfs-server-client sudo firewall-cmd --permanent --add-service=nfs sudo firewall-cmd --permanent --add-service=rpc-bind (needed for NFS3 configs) sudo firewall-cmd --permanent --add-service=mountd (needed for NFS3 configs) sudo firewall-cmd --reload more complicated commands, for NFS4 only: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_file_systems/configuring-an-nfsv4-only-server_managing-file-systems#benefits-and-drawbacks-of-an-nfsv4-only-server_configuring-an-nfsv4-only-server The default installation installs NFS, but neither enables it nor configures the firewall accordingly. [\u2026]# systemctl enable nfs-server --now [\u2026]# systemctl status nfs-server [\u2026]# firewall-cmd --permanent --add-service=nfs [\u2026]# firewall-cmd --reload","title":"Actually setting up NFS:"},{"location":"file_server/#older-potentially-redundat-notes","text":"","title":"Older, potentially redundat notes:"},{"location":"file_server/#automatically-mount-files-system","text":"$ sudo vim /etc/fstab Add lines: penelope.physik.uni-bonn.de:/export/disk/users /faust/user nfs4 defaults 0 0 penelope.physik.uni-bonn.de:/export/disk/cadence /cadence nfs4 ro 0 0 options for the mounting drives are: https://help.ubuntu.com/community/Fstab#Options https://wiki.archlinux.org/title/Fstab","title":"automatically mount files system"},{"location":"file_server/#updated-mount-points","text":"cd / sudo mkdir users sudo mkdir tools penelope.physik.uni-bonn.de:/export/disk/users /users nfs4 defaults 0 0 penelope.physik.uni-bonn.de:/export/disk/tools /tools nfs4 ro 0 0","title":"Updated mount points:"},{"location":"file_server/#tutorial-to-write","text":"make sure primary group is 'faust', else nobody permission will happen https://unix.stackexchange.com/questions/186568/what-is-nobody-user-and-group The nobody user is a pseudo user in many Unixes and Linux distributions, which is used to represent the user/group with the lowest permissions. In the best case that user and its group are not assigned to any file or directory (as owner). One application that makes use of this group through is NFS. If the owner of a file or directory in a mounted NFS share doesn't exist at the local system, it is replaced by the nobody user and its group. Take back any files that have been made under user or personal group name, and assign them to faust Note: the general status of the penelope server can be found by checking the webserver status page.","title":"tutorial to write"},{"location":"file_server/#trying-to-debug-slow-speeds-on-nfs-server","text":"Programs like Typora take forever to open on an NFS user, but with local users is very fast. Read first about Raid Array Speeds: https://www.raid-calculator.com/raid-types-reference.aspx Our 5 disk, 14.6TB per disk machine in Raid 6 has: Capacity 43.8 TB Speed gain 3x read speed, no write speed gain Fault tolerance 2-drive failure On the server side: https://linux.die.net/man/5/exports When mounting: noatime: Setting this value disables the NFS server from updating the inodes access time. As most applications do not necessarily need this value, you can safely disable this updating. nocto: Suppress the retrieval of new attributes when creating a file. This page has lots of things tried: https://serverfault.com/questions/682000/nfs-poor-write-performance","title":"Trying to debug slow speeds on NFS server."},{"location":"git/","text":"Fix for when a commit has been made in a detached head state, and you want to update the real branch to include this commit: https://stackoverflow.com/questions/7124486/what-to-do-with-commit-made-in-a-detached-head https://stackoverflow.com/questions/10228760/how-do-i-fix-a-git-detached-head The latter page includes info about git pop and git stash Github Contribution Workflow This explains why you should Fork a project you plan to contribute to, if you're not already a contributor in the direct project: https://opensource.com/article/19/11/first-open-source-contribution-fork-clone Once I have my own fork, this explains how I should go about making modifications to my Fork. Of course, I don't need to create a branch on my Fork, until i have have a change to make. https://docs.github.com/en/get-started/quickstart/github-flow I'm not sure where I'm going to run the latest versions of Jupiter Notebook, etc, with python, and so I should probably not install anything on my laptop or on the server. Let's just download the zip file from Github for now, and explore it. How to install Git and Github-CLI: https://github.com/git-guides/install-git I need to understand: Origin and Remote: These refer to remote repositories, which are pulled from with clone and pull , and pushed to with push . Master (branch) Branches (labels you put on commits) commits trees HEAD is the 'you are here' indicator. You get there by checking out a commit. To get back to to the master branches latest commit git checkout <branch name> or git switch <branch name> To view remote origin repository: git remote -v To report current branch git branch -v Report commits, optionally filtered by branch: git log <branch> The command to list all branches in local and remote repositories is: git branch -a If you require only listing the remote branches from Git Bash then use this command: git branch -r You may also use the show-branch command for seeing the branches and their commits as follows: git show-branch Using Git to restore a deleted, uncommitted file: If your changes have not been staged or committed: The command you should use in this scenario is git restore FILENAME View changes to unstaged files before staging and comitting, optinally filtering by a specific file git diff <filename> Update information on remote repositories git fetch --all View differences between state of current local repository, against a remote branch, for example \"origin/develop\" git diff origin/develop If the remote repository has been updated, I can check for remote references updates with: git fetch --dry-run --verbose To merge a remote branch with a local branch git checkout aLocalBranch if not already on that branch (could just be master!) git merge origin/aRemoteBranch Stage and commit changes git add -A && git commit -m \"Your Message\" Recurse submodules Getting the repository: git clone -b develop https://github.com/ucb-art/bag.git --recurse-submodules Figuring out what changes I've made to local: git status git log git info git fetch --dry-run git fetch --dry-run --all git remote update git show-branch *develop git config --global user.name \"Kennedy Caisley\" git config --global user.email kcaisley@uni-bonn.de git config --global core.editor vim To copy down submodule, if forgot to do during cloning: git submodule update --init --recursive --remote To reset all unstaged/uncommitted change: git checkout . git pull = git fetch + git merge Taking a local branch, and placing it on github, in a fresh repository is done via this instructions: https://docs.github.com/en/get-started/importing-your-projects-to-github/importing-source-code-to-github/adding-locally-hosted-code-to-github $ git remote add origin Sets the new remote, naming it as oriring, $ git remote -v Verifies the new remote URLc In the last stage we do this: The command git push -u origin main is used to push the local branch named \"main\" to a remote repository named \"origin\". The -u option is used to set the \"upstream\" relationship between the local branch and the remote branch. Here's what happens when you run the command: The local branch \"main\" is pushed to the remote repository \"origin\". The \"origin/main\" branch is created on the remote repository. The local branch \"main\" is configured to track the remote branch \"origin/main\". This means that in the future, you can simply use the command git push to push your local changes to the remote branch, without having to specify the remote repository and branch names. The \"upstream\" relationship between the local branch \"main\" and the remote branch \"origin/main\" is established. This makes it easier for Git to determine the correct branch to merge with when pulling updates from the remote repository. In summary, the git push -u origin main command is used to push a local branch to a remote repository and establish an upstream relationship between the local and remote branches. This makes it easier to keep the branches in sync and ensures that Git knows which branch to merge with when pulling updates from the remote repository. The command git push origin:develop develop is used to push a local branch named \"develop\" to a remote repository named \"origin\", and create a remote branch named \"develop\". The colon (:) in origin: is used to delete the remote branch named \"develop\", if it already exists. Here's what happens when you run the command: The remote branch named \"develop\" is deleted, if it already exists. The local branch named \"develop\" is pushed to the remote repository \"origin\". A remote branch named \"develop\" is created on the remote repository \"origin\". In summary, the git push origin:develop develop command is used to delete the remote branch named \"develop\" (if it exists), and create a new remote branch named \"develop\" with the latest changes from the local branch named \"develop\". This can be useful when you want to force a fresh start for the remote branch, or when you want to create a new remote branch with the same name as a previously deleted branch. The command git push origin/develop develop is used to push a local branch named \"develop\" to a remote branch named \"develop\". The \"origin/develop\" syntax is used to specify the remote branch to push to. Here's what happens when you run the command: The local branch named \"develop\" is pushed to the remote branch named \"develop\" on the remote repository \"origin\". The remote branch \"origin/develop\" is updated with the latest changes from the local branch \"develop\". In summary, the git push origin/develop develop command is used to push a local branch to a specific remote branch. This can be useful when you want to update a specific remote branch with the latest changes from a local branch, or when you want to create a new remote branch with the same name as a local branch. I can push anywhere I want, but if I want to be able to simply type git push, I have to specify which repo is the default to push to fatal: The current branch develop_fedora has no upstream branch. To push the current branch and set the remote as upstream, use git push --set-upstream origin develop_fedora To have this happen automatically for branches without a tracking Notes about using git submodules for dependency management: A submodule always result into a specific SHA1 being recorded by the parent repo: it is called a gitlink (special entry in the index of the parent repo). So simply add the repo as a submodule (it does not matter what branch/commit end up being actually recorded). cd /path/to/parent/repo git submodule add /url/of/submodule/repo Then go into the submodule folder and checkout the exact sHA1 or tag you want (the one corresponding to a release) cd mysubmodule git checkout Go back to the parent repo, add, commit and push: you will record the new submodule state (which is to say you will record the SHA1 you want) cd .. git add . git commit -m \"set submodule to a release SHA1\" git push Whenever you will clone --recursive your parent repo, your submodule willbe checked out to that release SHA1 again.","title":"Fix for when a commit has been made in a detached head state, and you want to update the real branch to include this commit:"},{"location":"git/#fix-for-when-a-commit-has-been-made-in-a-detached-head-state-and-you-want-to-update-the-real-branch-to-include-this-commit","text":"https://stackoverflow.com/questions/7124486/what-to-do-with-commit-made-in-a-detached-head https://stackoverflow.com/questions/10228760/how-do-i-fix-a-git-detached-head The latter page includes info about git pop and git stash","title":"Fix for when a commit has been made in a detached head state, and you want to update the real branch to include this commit:"},{"location":"git/#github-contribution-workflow","text":"This explains why you should Fork a project you plan to contribute to, if you're not already a contributor in the direct project: https://opensource.com/article/19/11/first-open-source-contribution-fork-clone Once I have my own fork, this explains how I should go about making modifications to my Fork. Of course, I don't need to create a branch on my Fork, until i have have a change to make. https://docs.github.com/en/get-started/quickstart/github-flow I'm not sure where I'm going to run the latest versions of Jupiter Notebook, etc, with python, and so I should probably not install anything on my laptop or on the server. Let's just download the zip file from Github for now, and explore it. How to install Git and Github-CLI: https://github.com/git-guides/install-git I need to understand: Origin and Remote: These refer to remote repositories, which are pulled from with clone and pull , and pushed to with push . Master (branch) Branches (labels you put on commits) commits trees HEAD is the 'you are here' indicator. You get there by checking out a commit.","title":"Github Contribution Workflow"},{"location":"git/#to-get-back-to-to-the-master-branches-latest-commit","text":"git checkout <branch name> or git switch <branch name>","title":"To get back to to the master branches latest commit"},{"location":"git/#to-view-remote-origin-repository","text":"git remote -v","title":"To view remote origin repository:"},{"location":"git/#to-report-current-branch","text":"git branch -v","title":"To report current branch"},{"location":"git/#report-commits-optionally-filtered-by-branch","text":"git log <branch>","title":"Report commits, optionally filtered by branch:"},{"location":"git/#the-command-to-list-all-branches-in-local-and-remote-repositories-is","text":"git branch -a","title":"The command to list all branches in local and remote repositories is:"},{"location":"git/#if-you-require-only-listing-the-remote-branches-from-git-bash-then-use-this-command","text":"git branch -r","title":"If you require only listing the remote branches from Git Bash then use this command:"},{"location":"git/#you-may-also-use-the-show-branch-command-for-seeing-the-branches-and-their-commits-as-follows","text":"git show-branch","title":"You may also use the show-branch command for seeing the branches and their commits as follows:"},{"location":"git/#using-git-to-restore-a-deleted-uncommitted-file","text":"If your changes have not been staged or committed: The command you should use in this scenario is git restore FILENAME","title":"Using Git to restore a deleted, uncommitted file:"},{"location":"git/#view-changes-to-unstaged-files-before-staging-and-comitting-optinally-filtering-by-a-specific-file","text":"git diff <filename>","title":"View changes to unstaged files before staging and comitting, optinally filtering by a specific file"},{"location":"git/#update-information-on-remote-repositories","text":"git fetch --all","title":"Update information on remote repositories"},{"location":"git/#view-differences-between-state-of-current-local-repository-against-a-remote-branch-for-example-origindevelop","text":"git diff origin/develop","title":"View differences between state of current local repository, against a remote branch, for example \"origin/develop\""},{"location":"git/#if-the-remote-repository-has-been-updated-i-can-check-for-remote-references-updates-with","text":"git fetch --dry-run --verbose","title":"If the remote repository has been updated, I can check for remote references updates with:"},{"location":"git/#to-merge-a-remote-branch-with-a-local-branch","text":"git checkout aLocalBranch if not already on that branch (could just be master!) git merge origin/aRemoteBranch","title":"To merge a remote branch with a local branch"},{"location":"git/#stage-and-commit-changes","text":"git add -A && git commit -m \"Your Message\"","title":"Stage and commit changes"},{"location":"git/#recurse-submodules","text":"Getting the repository: git clone -b develop https://github.com/ucb-art/bag.git --recurse-submodules Figuring out what changes I've made to local: git status git log git info git fetch --dry-run git fetch --dry-run --all git remote update git show-branch *develop git config --global user.name \"Kennedy Caisley\" git config --global user.email kcaisley@uni-bonn.de git config --global core.editor vim To copy down submodule, if forgot to do during cloning: git submodule update --init --recursive --remote To reset all unstaged/uncommitted change: git checkout . git pull = git fetch + git merge Taking a local branch, and placing it on github, in a fresh repository is done via this instructions: https://docs.github.com/en/get-started/importing-your-projects-to-github/importing-source-code-to-github/adding-locally-hosted-code-to-github $ git remote add origin","title":"Recurse submodules"},{"location":"git/#sets-the-new-remote-naming-it-as-oriring","text":"$ git remote -v","title":"Sets the new remote, naming it as oriring,"},{"location":"git/#verifies-the-new-remote-urlc","text":"In the last stage we do this: The command git push -u origin main is used to push the local branch named \"main\" to a remote repository named \"origin\". The -u option is used to set the \"upstream\" relationship between the local branch and the remote branch. Here's what happens when you run the command: The local branch \"main\" is pushed to the remote repository \"origin\". The \"origin/main\" branch is created on the remote repository. The local branch \"main\" is configured to track the remote branch \"origin/main\". This means that in the future, you can simply use the command git push to push your local changes to the remote branch, without having to specify the remote repository and branch names. The \"upstream\" relationship between the local branch \"main\" and the remote branch \"origin/main\" is established. This makes it easier for Git to determine the correct branch to merge with when pulling updates from the remote repository. In summary, the git push -u origin main command is used to push a local branch to a remote repository and establish an upstream relationship between the local and remote branches. This makes it easier to keep the branches in sync and ensures that Git knows which branch to merge with when pulling updates from the remote repository. The command git push origin:develop develop is used to push a local branch named \"develop\" to a remote repository named \"origin\", and create a remote branch named \"develop\". The colon (:) in origin: is used to delete the remote branch named \"develop\", if it already exists. Here's what happens when you run the command: The remote branch named \"develop\" is deleted, if it already exists. The local branch named \"develop\" is pushed to the remote repository \"origin\". A remote branch named \"develop\" is created on the remote repository \"origin\". In summary, the git push origin:develop develop command is used to delete the remote branch named \"develop\" (if it exists), and create a new remote branch named \"develop\" with the latest changes from the local branch named \"develop\". This can be useful when you want to force a fresh start for the remote branch, or when you want to create a new remote branch with the same name as a previously deleted branch. The command git push origin/develop develop is used to push a local branch named \"develop\" to a remote branch named \"develop\". The \"origin/develop\" syntax is used to specify the remote branch to push to. Here's what happens when you run the command: The local branch named \"develop\" is pushed to the remote branch named \"develop\" on the remote repository \"origin\". The remote branch \"origin/develop\" is updated with the latest changes from the local branch \"develop\". In summary, the git push origin/develop develop command is used to push a local branch to a specific remote branch. This can be useful when you want to update a specific remote branch with the latest changes from a local branch, or when you want to create a new remote branch with the same name as a local branch.","title":"Verifies the new remote URLc"},{"location":"git/#i-can-push-anywhere-i-want-but-if-i-want-to-be-able-to-simply-type-git-push-i-have-to-specify-which-repo-is-the-default-to-push-to","text":"fatal: The current branch develop_fedora has no upstream branch. To push the current branch and set the remote as upstream, use git push --set-upstream origin develop_fedora To have this happen automatically for branches without a tracking","title":"I can push anywhere I want, but if I want to be able to simply type git push, I have to specify which repo is the default to push to"},{"location":"git/#notes-about-using-git-submodules-for-dependency-management","text":"A submodule always result into a specific SHA1 being recorded by the parent repo: it is called a gitlink (special entry in the index of the parent repo). So simply add the repo as a submodule (it does not matter what branch/commit end up being actually recorded). cd /path/to/parent/repo git submodule add /url/of/submodule/repo Then go into the submodule folder and checkout the exact sHA1 or tag you want (the one corresponding to a release) cd mysubmodule git checkout Go back to the parent repo, add, commit and push: you will record the new submodule state (which is to say you will record the SHA1 you want) cd .. git add . git commit -m \"set submodule to a release SHA1\" git push Whenever you will clone --recursive your parent repo, your submodule willbe checked out to that release SHA1 again.","title":"Notes about using git submodules for dependency management:"},{"location":"license_server/","text":"FlexNet License Server Configuration The license server is running on faust02 and can be accessed via web interface http://faust02.physik.uni-bonn.de:8888 to check its status, update licenses, read log files etc. For the admin section, you need to use the root account credentials. The binaries and license files live on penelope in /cadence/other/lmadmin and are mounted on faust02. lmadmin The FlexNet binary ( /cadence/other/lmadmin/lmadmin ) manages all licenses and provides the web interface. After a reboot, this binary has to be started manually. Licenses and vendor deamons Licenses are separated by vendor. Each license file (and the matching vendor daemon binary) is stored in a separate folder in /cadence/other/lmadmin/license . Client Configuration To access the licenses from a client, add the server address to your environment: export LM_LICENSE_FILE=8000@faust02.physik.uni-bonn.de . Add this line to your ~/.bashrc or application-specific startup script if needed.","title":"FlexNet License Server Configuration"},{"location":"license_server/#flexnet-license-server-configuration","text":"The license server is running on faust02 and can be accessed via web interface http://faust02.physik.uni-bonn.de:8888 to check its status, update licenses, read log files etc. For the admin section, you need to use the root account credentials. The binaries and license files live on penelope in /cadence/other/lmadmin and are mounted on faust02.","title":"FlexNet License Server Configuration"},{"location":"license_server/#lmadmin","text":"The FlexNet binary ( /cadence/other/lmadmin/lmadmin ) manages all licenses and provides the web interface. After a reboot, this binary has to be started manually.","title":"lmadmin"},{"location":"license_server/#licenses-and-vendor-deamons","text":"Licenses are separated by vendor. Each license file (and the matching vendor daemon binary) is stored in a separate folder in /cadence/other/lmadmin/license .","title":"Licenses and vendor deamons"},{"location":"license_server/#client-configuration","text":"To access the licenses from a client, add the server address to your environment: export LM_LICENSE_FILE=8000@faust02.physik.uni-bonn.de . Add this line to your ~/.bashrc or application-specific startup script if needed.","title":"Client Configuration"},{"location":"network_configuration/","text":"Hostname and IP Address References DNS Server Setup network asiclab006 should be 131.220.163.233 . can be checked with ip address command, and checking the inet address of the eno1 device. 6. Double check hostname and time synchronization Both are important for trouble-free server operation. Just in case you missed its configuration during installation or it is incorrect, now is the opportunity to fix it. Check for correct hostname [\u2026]# hostnamectl Set hostname if required: [\u2026]# hostnamectl set-hostname <YourFQDN> Control of time zone, time synchronisation, time [\u2026]# timedatectl Correct time zone if necessary: [\u2026]# timedatectl set-timezone <ZONE> If necessary, activate time synchronisation: timedatectl set-ntp true Correct time if necessary: [\u2026]# timedatectl set-time <TIME> Configuring Ethernet with Static IP: The /etc/sysconfig/network-scripts/ifcfg-interface_name file was deprecated, and now nmcli is used, alongside it's keyfile format . It's not recommended to configure this file manually, even though you technically can (and inform NetworkManager). Instead the better way to proceed is to use the nmcli setup. The history of the migration from ifcfg to keyfiles is details in this Fedora magazine post : Here's the old ifcfg file: DEVICE=\"INTERFACE_NAME\" BOOTPROTO=\"static\" PEERDNS=no IPADDR=\"static_IP_as_in_/etc/hosts\" GATEWAY=\"131.220.163.254\" NETMASK=\"255.255.248.0\" IPV6INIT=\"no\" NM_CONTROLLED=\"no\" ONBOOT=\"yes\" As we can see, the netmask is 255.255.248.0 which corresponds to /21. To create a new set of keyfiles, the best approach is something like: $ nmcli con modify eth0 ipv4.method manual $ nmcli con modify eth0 ipv4.addresses 10.0.0.10/8 $ nmcli con modify eth0 ipv4.gateway 10.0.0.1 $ nmcli con modify eth0 ipv4.dns 10.0.0.2,10.0.0.3 nmcli connection modify Wired\\ connection\\ 1 ipv4.method manual nmcli connection modify Wired\\ connection\\ 1 ipv4.addresses 131.220.165.187/21 nmcli connection modify Wired\\ connection\\ 1 ipv4.gateway 131.220.163.254 nmcli connection modify Wired\\ connection\\ 1 ipv6.method disabled nmcli connection up Wired\\ connection\\ 1 For more information, check out $ man nmcli-examples and $ man nm-settings-nmcli To see the connection profile, and current status of the connection: nmcli -p connection show Wired\\ connection\\ 1 | less Damn, it looks like nearly all of the settings for this computer were simply copied over from the network somehow. Take the resolve DNS settings: $ resolvectl status Link 3 (eno1) Current Scopes: DNS LLMNR/IPv4 LLMNR/IPv6 Protocols: +DefaultRoute +LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported Current DNS Server: 131.220.16.220 DNS Servers: 131.220.16.220 131.220.14.203 131.220.18.138 DNS Domain: physik.uni-bonn.de Other notes, found from CentOS stuff: ip -br link show show network interfaces, in simple format ip -s link show [em1] show statistics on specific interface, where em1 example is interface name ip link set [em1] up enable network interfaces if they are down, but will not always work, if there is a real problem with the hardware. em1 is example interface name ip addr shows address of all the network interfaces in the machine the above is primarily for the physical layer more advice for the data link layer and networking layer can be found here: https://www.redhat.com/sysadmin/beginners-guide-network-troubleshooting-linux Debugging hostnamectl shows info about the machines network characteristics, and can be used to change host name ethtool [em1] prob status of different interfaces, where em1 is example interface visudo edit the sudoers file su - switch user to to the root user ip -br link show show network interfaces, in simple format ip -s link show [em1] show statistics on specific interface, where em1 example is interface name ip link set [em1] up enable network interfaces if they are down, but will not always work, if there is a real problem with the hardware. em1 is example interface name ip addr shows address of all the network interfaces in the machine the above is primarily for the physical layer more advice for the data link layer and networking layer can be found here: https://www.redhat.com/sysadmin/beginners-guide-network-troubleshooting-linux","title":"Hostname and IP Address References"},{"location":"network_configuration/#hostname-and-ip-address-references","text":"","title":"Hostname and IP Address References"},{"location":"network_configuration/#dns-server","text":"Setup network asiclab006 should be 131.220.163.233 . can be checked with ip address command, and checking the inet address of the eno1 device.","title":"DNS Server"},{"location":"network_configuration/#6-double-check-hostname-and-time-synchronization","text":"Both are important for trouble-free server operation. Just in case you missed its configuration during installation or it is incorrect, now is the opportunity to fix it. Check for correct hostname [\u2026]# hostnamectl Set hostname if required: [\u2026]# hostnamectl set-hostname <YourFQDN> Control of time zone, time synchronisation, time [\u2026]# timedatectl Correct time zone if necessary: [\u2026]# timedatectl set-timezone <ZONE> If necessary, activate time synchronisation: timedatectl set-ntp true Correct time if necessary: [\u2026]# timedatectl set-time <TIME>","title":"6. Double check hostname and time synchronization"},{"location":"network_configuration/#configuring-ethernet-with-static-ip","text":"The /etc/sysconfig/network-scripts/ifcfg-interface_name file was deprecated, and now nmcli is used, alongside it's keyfile format . It's not recommended to configure this file manually, even though you technically can (and inform NetworkManager). Instead the better way to proceed is to use the nmcli setup. The history of the migration from ifcfg to keyfiles is details in this Fedora magazine post : Here's the old ifcfg file: DEVICE=\"INTERFACE_NAME\" BOOTPROTO=\"static\" PEERDNS=no IPADDR=\"static_IP_as_in_/etc/hosts\" GATEWAY=\"131.220.163.254\" NETMASK=\"255.255.248.0\" IPV6INIT=\"no\" NM_CONTROLLED=\"no\" ONBOOT=\"yes\" As we can see, the netmask is 255.255.248.0 which corresponds to /21. To create a new set of keyfiles, the best approach is something like: $ nmcli con modify eth0 ipv4.method manual $ nmcli con modify eth0 ipv4.addresses 10.0.0.10/8 $ nmcli con modify eth0 ipv4.gateway 10.0.0.1 $ nmcli con modify eth0 ipv4.dns 10.0.0.2,10.0.0.3 nmcli connection modify Wired\\ connection\\ 1 ipv4.method manual nmcli connection modify Wired\\ connection\\ 1 ipv4.addresses 131.220.165.187/21 nmcli connection modify Wired\\ connection\\ 1 ipv4.gateway 131.220.163.254 nmcli connection modify Wired\\ connection\\ 1 ipv6.method disabled nmcli connection up Wired\\ connection\\ 1 For more information, check out $ man nmcli-examples and $ man nm-settings-nmcli To see the connection profile, and current status of the connection: nmcli -p connection show Wired\\ connection\\ 1 | less Damn, it looks like nearly all of the settings for this computer were simply copied over from the network somehow. Take the resolve DNS settings: $ resolvectl status Link 3 (eno1) Current Scopes: DNS LLMNR/IPv4 LLMNR/IPv6 Protocols: +DefaultRoute +LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported Current DNS Server: 131.220.16.220 DNS Servers: 131.220.16.220 131.220.14.203 131.220.18.138 DNS Domain: physik.uni-bonn.de Other notes, found from CentOS stuff: ip -br link show show network interfaces, in simple format ip -s link show [em1] show statistics on specific interface, where em1 example is interface name ip link set [em1] up enable network interfaces if they are down, but will not always work, if there is a real problem with the hardware. em1 is example interface name ip addr shows address of all the network interfaces in the machine the above is primarily for the physical layer more advice for the data link layer and networking layer can be found here: https://www.redhat.com/sysadmin/beginners-guide-network-troubleshooting-linux","title":"Configuring Ethernet with Static IP:"},{"location":"network_configuration/#debugging","text":"hostnamectl shows info about the machines network characteristics, and can be used to change host name ethtool [em1] prob status of different interfaces, where em1 is example interface visudo edit the sudoers file su - switch user to to the root user ip -br link show show network interfaces, in simple format ip -s link show [em1] show statistics on specific interface, where em1 example is interface name ip link set [em1] up enable network interfaces if they are down, but will not always work, if there is a real problem with the hardware. em1 is example interface name ip addr shows address of all the network interfaces in the machine the above is primarily for the physical layer more advice for the data link layer and networking layer can be found here: https://www.redhat.com/sysadmin/beginners-guide-network-troubleshooting-linux","title":"Debugging"},{"location":"package_management/","text":"DNF and RPM package management: Check current version of an installed RPM rpm -q <packagename> Check current versions of remote and installed versions of a package dnf info <packagename> List contents of installed RPM rpm -ql <packagename> List contents of remote repository package: dnf repoquery -l spdlog Show the last time a rpm package was upgraded/installed: rpm -qa --last Show history of explicity dnf install requests: sudo dnf history Show more info about an ID history number: sudo dnf history info # Dnf undo history item: sudo dnf history undo/rollback #","title":"DNF and RPM package management:"},{"location":"package_management/#dnf-and-rpm-package-management","text":"Check current version of an installed RPM rpm -q <packagename> Check current versions of remote and installed versions of a package dnf info <packagename> List contents of installed RPM rpm -ql <packagename> List contents of remote repository package: dnf repoquery -l spdlog Show the last time a rpm package was upgraded/installed: rpm -qa --last Show history of explicity dnf install requests: sudo dnf history Show more info about an ID history number: sudo dnf history info # Dnf undo history item: sudo dnf history undo/rollback #","title":"DNF and RPM package management:"},{"location":"printer_config/","text":"Configuring printers Access to network printers in the FTD is provided via CUPS, which is the standard client-server printing system used on Unix-like systems. It consisting of a server responsible for managing print queues and printers (in this case managed by the PI), and a client allowing users to submit print jobs. The server is installed on the computer or device that the printers are connected to, while the client can be installed on any device that needs to print. The client and server communicate over the network using the Internet Printing Protocol (IPP), providing an efficient way to manage printing tasks on Unix-like systems. To configure a machine's CUPS client to connect to the PI printer server ( cups.physik.uni-bonn.de ), you should create a /etc/cups/client.conf file: touch /etc/cups/client.conf And then edit the empty file, so that is contains the line: ServerName cups.physik.uni-bonn.de All printers on the FTD network should now be available. To check, you can view a summary of available network printers with the following command: lpstat -t","title":"Printer config"},{"location":"printer_config/#configuring-printers","text":"Access to network printers in the FTD is provided via CUPS, which is the standard client-server printing system used on Unix-like systems. It consisting of a server responsible for managing print queues and printers (in this case managed by the PI), and a client allowing users to submit print jobs. The server is installed on the computer or device that the printers are connected to, while the client can be installed on any device that needs to print. The client and server communicate over the network using the Internet Printing Protocol (IPP), providing an efficient way to manage printing tasks on Unix-like systems. To configure a machine's CUPS client to connect to the PI printer server ( cups.physik.uni-bonn.de ), you should create a /etc/cups/client.conf file: touch /etc/cups/client.conf And then edit the empty file, so that is contains the line: ServerName cups.physik.uni-bonn.de All printers on the FTD network should now be available. To check, you can view a summary of available network printers with the following command: lpstat -t","title":"Configuring printers"},{"location":"remote_connection/","text":"Wireguard As of Fedora 38, which is built on Gnome 44, you can simply import Wireguard .conf files directly in the Settings application, under the Network tab. SSH Basics SSH can be enabled on hosts graphically on workstations in settings, and on Fedora servers via: sudo systemctl enable sshd sudo systemctl start sshd Enabling Older OpenSSH Algorithms To enable SSH-RSA (and outdated protocol) on new machines To permit using old RSA keys for OpenSSH 8.8+, add the following lines to your sshd_config: HostKeyAlgorithms=ssh-rsa,ssh-rsa-cert-v01@openssh.com PubkeyAcceptedAlgorithms=+ssh-rsa,ssh-rsa-cert-v01@openssh.com Note: If you're trying to connect to, or from an older system, you may run into issues where the version keys supported is deprecated. An alternative way to enable ssh-rsa is to just create a .ssh/config file: Host ssh.dev.azure.com User git PubkeyAcceptedAlgorithms +ssh-rsa HostkeyAlgorithms +ssh-rsa Passwordless Authentication For passwordless authentication, one can generate SSH keys, and move it to another computer for authentication ssh-keygen -t ed25519 ssh-copy-id -i ~/.ssh/asiclab008.pub kcaisley@asiclab008.physik.uni-bonn.de # alternatively ssh-copy-id -i ~/.ssh/id_ed25519.pub asiclab@asiclab07 To enable SSH public key authentication on a Fedora host, make sure you go to /etc/ssh/sshd_config and uncomment the line PubkeyAuthentication yes Github Access via Key Pairs Instructions for SSH key authentication. Create a new key, if necessary, and following prompts. ssh-keygen -t ed25519 -C \"your_email@example.com\" Print out public key, and paste into Github. cat id_ed25519.pub Test the connection ssh -T git@github.com If you have issues, try changing the permissions of the key pair using chmod 600 {key} and add the keys to the SSH agent with ssh-add . To allow copying down repos: To add commits, and push back to github, make sure your configure your local git with your username and password, matching those of Github: git config --global user.email \"you@example.com\" git config --global user.name \"Username\" Check settings with: git config --get user.email Clone down a repository, to proceed with work. For example: git clone -b develop git@github.com:SiLab-Bonn/pybag.git --recurse-submodules Sleep, suspend, and hibernate settings: sudo systemctl status sleep.target suspend.target hibernate.target hybrid-sleep.target sudo systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target sudo systemctl unmask sleep.target suspend.target hibernate.target hybrid-sleep.target There are multiple methods of suspending available, notably: Suspend to RAM (aka suspend, aka sleep) The S3 sleeping state as defined by ACPI. Works by cutting off power to most parts of the machine aside from the RAM, which is required to restore the machine's state. Because of the large power savings, it is advisable for laptops to automatically enter this mode when the computer is running on batteries and the lid is closed (or the user is inactive for some time). Suspend to disk (aka hibernate) The S4 sleeping state as defined by ACPI. Saves the machine's state into swap space and completely powers off the machine. When the machine is powered on, the state is restored. Until then, there is zero power consumption. Hybrid suspend A hybrid of suspending and hibernating, sometimes called suspend to both . Saves the machine's state into swap space, but does not power off the machine. Instead, it invokes the default suspend. Therefore, if the battery is not depleted, the system can resume instantly. If the battery is depleted, the system can be resumed from disk, which is much slower than resuming from RAM, but the machine's state has not been lost. Thunderbird Email sudo dnf install thunderbird Uni-Bonn email setup described here is summarized below: Incoming Server Settings: Protocol: IMAP Hostname: mail.uni-bonn.de Port: 933 Connection Security: SSL/TLS Authentication Method: Normal Password Username: kcaisley@uni-bonn.de Outgoing Server Settings: Protocol: IMAP Server Name: mail.uni-bonn.de Port: 587 Connection Security: STARTTLS Authentication Method: Normal Password Username: kcaisley@uni-bonn.de SiRUSH SMB Server Sirrush is shared via smb. You can simply use a file manager and go to smb://sirrush.physik.uni-bonn.de and log in with silab/pidub12. One can only access the /silab directory with this login. If you also want to access project folder, an account has to be made for you. X11 Forwarding VNC For CentO 7, this is the article that has worked: https://linuxize.com/post/how-to-install-and-configure-vnc-on-centos-7/ RDP Enable in Workstation settings, then using gnome connections, check the RDP connection type. The format is: rdp://asiclab008.physik.uni-bonn.de:3389 SFTP Samba SMB/Samba/CIFS report status of connections on SMB server (noyce) (must be >=CentOS7) smbstatus ftp-like client to access SMB/CIFS resources on servers, with -U flag to check current config for a user, to ping and repo available servers of remote server from local client machine smbclient -L noyce.physik.uni-bonn.de -U kcaisley to add folders shares to the on the server: Edit the file \"/etc/samba/smb.conf\" sudo vim /etc/samba/smb.conf Once \"smb.conf\" has loaded, add this to the very end of the file: [<folder_name>] path = /home/<user_name>/<folder_name> valid users = <user_name> read only = no Tip: There Should be in the spaces between the lines, and note que also there should be a single space both before and after each of the equal signs. To access your network share from the client machines sudo apt-get install smbclient # List all shares: smbclient -L //<HOST_IP_OR_NAME>/<folder_name> -U <user> # connect: smbclient //<HOST_IP_OR_NAME>/<folder_name> -U <user> Othere SMB commands: NetBIOS over TCP/IP client used to lookup NetBIOS names, part of samba suite. nmblookup __SAMBA__ List info about machines that respond to SMB name queries on a subnet. Uses nmblookup and smbclient as backend. findsmb","title":"Wireguard"},{"location":"remote_connection/#wireguard","text":"As of Fedora 38, which is built on Gnome 44, you can simply import Wireguard .conf files directly in the Settings application, under the Network tab.","title":"Wireguard"},{"location":"remote_connection/#ssh","text":"","title":"SSH"},{"location":"remote_connection/#basics","text":"SSH can be enabled on hosts graphically on workstations in settings, and on Fedora servers via: sudo systemctl enable sshd sudo systemctl start sshd","title":"Basics"},{"location":"remote_connection/#enabling-older-openssh-algorithms","text":"To enable SSH-RSA (and outdated protocol) on new machines To permit using old RSA keys for OpenSSH 8.8+, add the following lines to your sshd_config: HostKeyAlgorithms=ssh-rsa,ssh-rsa-cert-v01@openssh.com PubkeyAcceptedAlgorithms=+ssh-rsa,ssh-rsa-cert-v01@openssh.com Note: If you're trying to connect to, or from an older system, you may run into issues where the version keys supported is deprecated. An alternative way to enable ssh-rsa is to just create a .ssh/config file: Host ssh.dev.azure.com User git PubkeyAcceptedAlgorithms +ssh-rsa HostkeyAlgorithms +ssh-rsa","title":"Enabling Older OpenSSH Algorithms"},{"location":"remote_connection/#passwordless-authentication","text":"For passwordless authentication, one can generate SSH keys, and move it to another computer for authentication ssh-keygen -t ed25519 ssh-copy-id -i ~/.ssh/asiclab008.pub kcaisley@asiclab008.physik.uni-bonn.de # alternatively ssh-copy-id -i ~/.ssh/id_ed25519.pub asiclab@asiclab07 To enable SSH public key authentication on a Fedora host, make sure you go to /etc/ssh/sshd_config and uncomment the line PubkeyAuthentication yes","title":"Passwordless Authentication"},{"location":"remote_connection/#github-access-via-key-pairs","text":"Instructions for SSH key authentication. Create a new key, if necessary, and following prompts. ssh-keygen -t ed25519 -C \"your_email@example.com\" Print out public key, and paste into Github. cat id_ed25519.pub Test the connection ssh -T git@github.com If you have issues, try changing the permissions of the key pair using chmod 600 {key} and add the keys to the SSH agent with ssh-add . To allow copying down repos: To add commits, and push back to github, make sure your configure your local git with your username and password, matching those of Github: git config --global user.email \"you@example.com\" git config --global user.name \"Username\" Check settings with: git config --get user.email Clone down a repository, to proceed with work. For example: git clone -b develop git@github.com:SiLab-Bonn/pybag.git --recurse-submodules","title":"Github Access via Key Pairs"},{"location":"remote_connection/#sleep-suspend-and-hibernate-settings","text":"sudo systemctl status sleep.target suspend.target hibernate.target hybrid-sleep.target sudo systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target sudo systemctl unmask sleep.target suspend.target hibernate.target hybrid-sleep.target There are multiple methods of suspending available, notably: Suspend to RAM (aka suspend, aka sleep) The S3 sleeping state as defined by ACPI. Works by cutting off power to most parts of the machine aside from the RAM, which is required to restore the machine's state. Because of the large power savings, it is advisable for laptops to automatically enter this mode when the computer is running on batteries and the lid is closed (or the user is inactive for some time). Suspend to disk (aka hibernate) The S4 sleeping state as defined by ACPI. Saves the machine's state into swap space and completely powers off the machine. When the machine is powered on, the state is restored. Until then, there is zero power consumption. Hybrid suspend A hybrid of suspending and hibernating, sometimes called suspend to both . Saves the machine's state into swap space, but does not power off the machine. Instead, it invokes the default suspend. Therefore, if the battery is not depleted, the system can resume instantly. If the battery is depleted, the system can be resumed from disk, which is much slower than resuming from RAM, but the machine's state has not been lost.","title":"Sleep, suspend, and hibernate settings:"},{"location":"remote_connection/#thunderbird-email","text":"sudo dnf install thunderbird Uni-Bonn email setup described here is summarized below:","title":"Thunderbird Email"},{"location":"remote_connection/#incoming-server-settings","text":"Protocol: IMAP Hostname: mail.uni-bonn.de Port: 933 Connection Security: SSL/TLS Authentication Method: Normal Password Username: kcaisley@uni-bonn.de","title":"Incoming Server Settings:"},{"location":"remote_connection/#outgoing-server-settings","text":"Protocol: IMAP Server Name: mail.uni-bonn.de Port: 587 Connection Security: STARTTLS Authentication Method: Normal Password Username: kcaisley@uni-bonn.de","title":"Outgoing Server Settings:"},{"location":"remote_connection/#sirush-smb-server","text":"Sirrush is shared via smb. You can simply use a file manager and go to smb://sirrush.physik.uni-bonn.de and log in with silab/pidub12. One can only access the /silab directory with this login. If you also want to access project folder, an account has to be made for you.","title":"SiRUSH SMB Server"},{"location":"remote_connection/#x11-forwarding","text":"","title":"X11 Forwarding"},{"location":"remote_connection/#vnc","text":"For CentO 7, this is the article that has worked: https://linuxize.com/post/how-to-install-and-configure-vnc-on-centos-7/","title":"VNC"},{"location":"remote_connection/#rdp","text":"Enable in Workstation settings, then using gnome connections, check the RDP connection type. The format is: rdp://asiclab008.physik.uni-bonn.de:3389","title":"RDP"},{"location":"remote_connection/#sftp","text":"","title":"SFTP"},{"location":"remote_connection/#samba","text":"","title":"Samba"},{"location":"remote_connection/#smbsambacifs","text":"report status of connections on SMB server (noyce) (must be >=CentOS7) smbstatus ftp-like client to access SMB/CIFS resources on servers, with -U flag to check current config for a user, to ping and repo available servers of remote server from local client machine smbclient -L noyce.physik.uni-bonn.de -U kcaisley to add folders shares to the on the server: Edit the file \"/etc/samba/smb.conf\" sudo vim /etc/samba/smb.conf Once \"smb.conf\" has loaded, add this to the very end of the file: [<folder_name>] path = /home/<user_name>/<folder_name> valid users = <user_name> read only = no Tip: There Should be in the spaces between the lines, and note que also there should be a single space both before and after each of the equal signs. To access your network share from the client machines sudo apt-get install smbclient # List all shares: smbclient -L //<HOST_IP_OR_NAME>/<folder_name> -U <user> # connect: smbclient //<HOST_IP_OR_NAME>/<folder_name> -U <user>","title":"SMB/Samba/CIFS"},{"location":"remote_connection/#othere-smb-commands","text":"NetBIOS over TCP/IP client used to lookup NetBIOS names, part of samba suite. nmblookup __SAMBA__ List info about machines that respond to SMB name queries on a subnet. Uses nmblookup and smbclient as backend. findsmb","title":"Othere SMB commands:"},{"location":"setup_pdk/","text":"Europratice Kits To download files larger than 100 MB you have to use the Querio FTP server instead. You can login to the FTP server using the same user name and password used to login to Querio. To access the files on the FTP server, For security reasons, the FTP server will only accept encrypted connections. Therefore you have to configure your FTP client to use implicit SSL/TLS encryption. The table below gives an overview of the FTP server settings that your FTP client requires in order to connect. Note that your FTP client may not need all of these settings to be explicitly defined. For example the port and transfer mode are usually automatically set by the FTP client. Server/host: data.europractice-ic.com Port: 990 User name: Same as your Querio user name. Password: Same as your Querio password. Security/encryption: SSL/TLS implicit Transfer mode: Passive The following transcript shows how to download: [asiclab@penelope kits]$ sudo dnf install lftp [asiclab@penelope kits]$ lftp ftps://data.europractice-ic.com lftp data.europractice-ic.com:~> set ssl:verify-certificate no lftp data.europractice-ic.com:~> login kennedycaisley Password: lftp kennedycaisley@data.europractice-ic.com:~> pwd ftps://kennedycaisley@data.europractice-ic.com lftp kennedycaisley@data.europractice-ic.com:~> pwd ftps://kennedycaisley@data.europractice-ic.com lftp kennedycaisley@data.europractice-ic.com:~> ls dr-x------ 1 Querio Querio 0 Jul 28 2022 CERN lftp kennedycaisley@data.europractice-ic.com:/> cd CERN/ lftp kennedycaisley@data.europractice-ic.com:/CERN> mirror TSMC28 As seen above, the easiest method to recursively download an entire directory is to have an identically named on on the local machine, and to use the lftp ~> mirror <directory name> command. Also note that set ssl:verify-certificate no is unsecure, as it could allow for man in the middle attacks, but the better method shown here didn't work, and I'm not paid enough to figure out why. Joining the split .gz files: cat HEP_DesignKit_TSMC28_HPCplusRF_v1.0.tar.gz.part_* > HEP_DesignKit_TSMC28_HPCplusRF_v1.0.tar.gz To untar the files, with progress bar: pv HEP_DesignKit_TSMC28_HPCplusRF_v1.0.tar.gz | tar -xz To run the installPdk.pl file, a perl module is needed? No we don't need this, as we are using the CERN standard PDK install. sudo dnf install perl-FileHandle HEP_DesignKit_TSMC28_HPCplusRF_v1.0/pdk/1P9M_5X1Y1Z1U_UT_AlRDL/cdsPDK/pdkInstall.pl 65nm PDK notes PDK Install Directory Structure Techfile/ : Includes all temporary files which are used for install program Assura/ : Assura LVS/QRC command files Calibre/ : Calibre DRC/LVS/XRC command files CCI/ : Calibre Star-RCXT CCI flow technology files directory PDK_doc/ : Includes all documents, please check ReleaseNote before using this PDK. REVISION : Revision history cds.lib : cds library mapping file display.drf : Virtuoso display file readme : readme file ReleaseNote.txt : PDK detail information pdkInstall.pl : PDK installation utility pdkInstall.cfg : PDK installation configuraion skill/ : skill directory includes all callback and utility models/ : hspice/spectre/eldo models techfile : Virtuoso tech file mapfile : layout editor mapfile tsmcN65/ : PDK library SPICE Models PROCESS : 65nm Mixed Signal RF SALICIDE Low-K IMD (1.2/2.5/over-drive 3.3V)(CRN65LP) MODEL : BSIM4 ( V4.5 ) DOC. NO. : T-N65-CM-SP-007 VERSION : V1.7 DATE : March 21, 2012 PROCESS : 65nm Mixed Signal RF SALICIDE Low-K IMD (1.2/3.3V)(CRN65LP) MODEL : BSIM4 ( V4.5 ) DOC. NO. : T-N65-CM-SP-012 VERSION : v1.5 DATE : March 21, 2012 SPECTRE VERSION : 7.1.0.048 HSPICE VERSION : A-2008.03 ELDO VERSION : v2009.2a Moreover, this PDK has been tested to work with the following software versions: *ICFB IC6.1.5.500.5 cadence *Spectre sub-version 7.2.0.477.isr16 cadence *Hspice 2008.03-SP1 sysnopsys *ELDO 2009.2 mentor *Calibre v2009.3_15.12 cadence *Assura AV4.1_USR2_HF9-615 mentor *QRC EXT101_2_HF3 mentor (for resistance specifically) *StarRC E-2010.12-SP2 synopsys *Perl v5.12.2 *ncsim 08.20-s003 for digital sim, from cadence 28nm Install https://asic-support-28.web.cern.ch/documents/cern_tsmc28HPCplus_pdk_final.pdf https://asic-support-28.web.cern.ch/tech-docs/pdk_install/ We can just follow the basic untar and copy procedure. Everthing is already setup with: # In cds.lib # DO NOT MODIFY LINES BELOW # base Cadence Virtuoso and PDK libraries (including MSOA) INCLUDE $PDK_PATH/$PDK_RELEASE/pdk/$PDK_OPTION/cdsPDK_MSOA/cds.lib # PDK Digital libraries INCLUDE $PDK_PATH/$PDK_RELEASE/TSMCHOME/digital/Back_End/cdk/cds.lib.$PDK_OPTION # In start.sh setenv PDK_PATH <folder where you want to install the PDK> setenv PDK_RELEASE HEP_DesignKit_TSMC28_HPCplusRF_v1.0 setenv PDK_OPTION 1P9M_5X1Y1Z1U_UT_AlRDL","title":"Setup pdk"},{"location":"setup_pdk/#europratice-kits","text":"To download files larger than 100 MB you have to use the Querio FTP server instead. You can login to the FTP server using the same user name and password used to login to Querio. To access the files on the FTP server, For security reasons, the FTP server will only accept encrypted connections. Therefore you have to configure your FTP client to use implicit SSL/TLS encryption. The table below gives an overview of the FTP server settings that your FTP client requires in order to connect. Note that your FTP client may not need all of these settings to be explicitly defined. For example the port and transfer mode are usually automatically set by the FTP client. Server/host: data.europractice-ic.com Port: 990 User name: Same as your Querio user name. Password: Same as your Querio password. Security/encryption: SSL/TLS implicit Transfer mode: Passive The following transcript shows how to download: [asiclab@penelope kits]$ sudo dnf install lftp [asiclab@penelope kits]$ lftp ftps://data.europractice-ic.com lftp data.europractice-ic.com:~> set ssl:verify-certificate no lftp data.europractice-ic.com:~> login kennedycaisley Password: lftp kennedycaisley@data.europractice-ic.com:~> pwd ftps://kennedycaisley@data.europractice-ic.com lftp kennedycaisley@data.europractice-ic.com:~> pwd ftps://kennedycaisley@data.europractice-ic.com lftp kennedycaisley@data.europractice-ic.com:~> ls dr-x------ 1 Querio Querio 0 Jul 28 2022 CERN lftp kennedycaisley@data.europractice-ic.com:/> cd CERN/ lftp kennedycaisley@data.europractice-ic.com:/CERN> mirror TSMC28 As seen above, the easiest method to recursively download an entire directory is to have an identically named on on the local machine, and to use the lftp ~> mirror <directory name> command. Also note that set ssl:verify-certificate no is unsecure, as it could allow for man in the middle attacks, but the better method shown here didn't work, and I'm not paid enough to figure out why.","title":"Europratice Kits"},{"location":"setup_pdk/#joining-the-split-gz-files","text":"cat HEP_DesignKit_TSMC28_HPCplusRF_v1.0.tar.gz.part_* > HEP_DesignKit_TSMC28_HPCplusRF_v1.0.tar.gz","title":"Joining the split .gz files:"},{"location":"setup_pdk/#to-untar-the-files-with-progress-bar","text":"pv HEP_DesignKit_TSMC28_HPCplusRF_v1.0.tar.gz | tar -xz To run the installPdk.pl file, a perl module is needed? No we don't need this, as we are using the CERN standard PDK install. sudo dnf install perl-FileHandle HEP_DesignKit_TSMC28_HPCplusRF_v1.0/pdk/1P9M_5X1Y1Z1U_UT_AlRDL/cdsPDK/pdkInstall.pl","title":"To untar the files, with progress bar:"},{"location":"setup_pdk/#65nm-pdk-notes","text":"PDK Install Directory Structure Techfile/ : Includes all temporary files which are used for install program Assura/ : Assura LVS/QRC command files Calibre/ : Calibre DRC/LVS/XRC command files CCI/ : Calibre Star-RCXT CCI flow technology files directory PDK_doc/ : Includes all documents, please check ReleaseNote before using this PDK. REVISION : Revision history cds.lib : cds library mapping file display.drf : Virtuoso display file readme : readme file ReleaseNote.txt : PDK detail information pdkInstall.pl : PDK installation utility pdkInstall.cfg : PDK installation configuraion skill/ : skill directory includes all callback and utility models/ : hspice/spectre/eldo models techfile : Virtuoso tech file mapfile : layout editor mapfile tsmcN65/ : PDK library","title":"65nm PDK notes"},{"location":"setup_pdk/#spice-models","text":"PROCESS : 65nm Mixed Signal RF SALICIDE Low-K IMD (1.2/2.5/over-drive 3.3V)(CRN65LP) MODEL : BSIM4 ( V4.5 ) DOC. NO. : T-N65-CM-SP-007 VERSION : V1.7 DATE : March 21, 2012 PROCESS : 65nm Mixed Signal RF SALICIDE Low-K IMD (1.2/3.3V)(CRN65LP) MODEL : BSIM4 ( V4.5 ) DOC. NO. : T-N65-CM-SP-012 VERSION : v1.5 DATE : March 21, 2012 SPECTRE VERSION : 7.1.0.048 HSPICE VERSION : A-2008.03 ELDO VERSION : v2009.2a Moreover, this PDK has been tested to work with the following software versions: *ICFB IC6.1.5.500.5 cadence *Spectre sub-version 7.2.0.477.isr16 cadence *Hspice 2008.03-SP1 sysnopsys *ELDO 2009.2 mentor *Calibre v2009.3_15.12 cadence *Assura AV4.1_USR2_HF9-615 mentor *QRC EXT101_2_HF3 mentor (for resistance specifically) *StarRC E-2010.12-SP2 synopsys *Perl v5.12.2 *ncsim 08.20-s003 for digital sim, from cadence","title":"SPICE Models"},{"location":"setup_pdk/#28nm-install","text":"https://asic-support-28.web.cern.ch/documents/cern_tsmc28HPCplus_pdk_final.pdf https://asic-support-28.web.cern.ch/tech-docs/pdk_install/ We can just follow the basic untar and copy procedure. Everthing is already setup with: # In cds.lib # DO NOT MODIFY LINES BELOW # base Cadence Virtuoso and PDK libraries (including MSOA) INCLUDE $PDK_PATH/$PDK_RELEASE/pdk/$PDK_OPTION/cdsPDK_MSOA/cds.lib # PDK Digital libraries INCLUDE $PDK_PATH/$PDK_RELEASE/TSMCHOME/digital/Back_End/cdk/cds.lib.$PDK_OPTION # In start.sh setenv PDK_PATH <folder where you want to install the PDK> setenv PDK_RELEASE HEP_DesignKit_TSMC28_HPCplusRF_v1.0 setenv PDK_OPTION 1P9M_5X1Y1Z1U_UT_AlRDL","title":"28nm Install"},{"location":"setup_synopsys/","text":"SFTP Download Instructions Use the command-line sftp utility to connect to the Synopsys Electronic File Transfer (EFT) system using the SFTP protocol: sftp @eft.synopsys.com (For example: sftp johndoe@eft.synopsys.com) Enter your SolvNetPlus password At the sftp > prompt, enter the following commands to download the Synopsys tools: cd site19237 cd MyProducts cd rev/installer_v5.6 Enter ls or dir to see the list of product files get filename to retrieve the file(s) Enter \"quit\" to log off the server NOTE: For products that use the Synopsys Installer, the product files will be named .spf or .part0n. Typically, you will need to download one or more \"common\" and OS \"platform\" files (for example, common.spf and linux64.spf). The Synopsys Installer is a separate download, in the site_nnnn_/MyProducts/rev/installer_version directory. For assistance on using the Synopsys Installer, see the Installation Guide .** Container and Installer Instructions: https://solvnetplus.synopsys.com/s/article/Synopsys-Container-Installation-Configuration-1576165810868 Setup Script #setup TCAD stuffs export LM_LICENSE_FILE=8000@faust02.physik.uni-bonn.de export STROOT=/tools/synopsys/installs/sentaurus/R_2020.09-SP1 export PATH=$STROOT/bin:$PATH export STDB=/users/kcaisley/tcad alias ll='ls -l' alias la='ls -la' echo set up dependencies","title":"SFTP Download Instructions"},{"location":"setup_synopsys/#sftp-download-instructions","text":"Use the command-line sftp utility to connect to the Synopsys Electronic File Transfer (EFT) system using the SFTP protocol: sftp @eft.synopsys.com (For example: sftp johndoe@eft.synopsys.com) Enter your SolvNetPlus password At the sftp > prompt, enter the following commands to download the Synopsys tools: cd site19237 cd MyProducts cd rev/installer_v5.6 Enter ls or dir to see the list of product files get filename to retrieve the file(s) Enter \"quit\" to log off the server NOTE: For products that use the Synopsys Installer, the product files will be named .spf or .part0n. Typically, you will need to download one or more \"common\" and OS \"platform\" files (for example, common.spf and linux64.spf). The Synopsys Installer is a separate download, in the site_nnnn_/MyProducts/rev/installer_version directory. For assistance on using the Synopsys Installer, see the Installation Guide .**","title":"SFTP Download Instructions"},{"location":"setup_synopsys/#container-and-installer-instructions","text":"https://solvnetplus.synopsys.com/s/article/Synopsys-Container-Installation-Configuration-1576165810868","title":"Container and Installer Instructions:"},{"location":"setup_synopsys/#setup-script","text":"#setup TCAD stuffs export LM_LICENSE_FILE=8000@faust02.physik.uni-bonn.de export STROOT=/tools/synopsys/installs/sentaurus/R_2020.09-SP1 export PATH=$STROOT/bin:$PATH export STDB=/users/kcaisley/tcad alias ll='ls -l' alias la='ls -la' echo set up dependencies","title":"Setup Script"},{"location":"setup_xilinx/","text":"To make Vivado work: sudo dnf install ncurses-compat-libs source /tools/xilinx/Vivado/2022.2/settings64_fedora.sh Installation on penelope, as asiclab Following: http://www.diy.ind.in/linux/64-install-xilinx-platform-cable-usb-ii-driver cd /tools/xilinx dnf install git sudo git clone git://git.zerfleddert.de/usb-driver sudo dnf install make gcc glibc-devel cd usb-driver sudo nano Makefile #comment out the line about '-m32', to disable 32bit build sudo make On client The one-time initial setup is as follows Digilent JTAG adapter cd [workdir] cp -r /tools/xilinx/14.7/ISE_DS/common/bin/lin64/digilent . sudo ./install_digilent.sh Xilinx JTAG adapter sudo dnf install fxload libusb1 libusb1-devel libusb-compat-0.1 libusb-compat-0.1-devel sudo ./setup_pcusb /tools/xilinx/14.7/ISE_DS/ISE sudo udevadm control --reload-rules Unplug and replug - and you should see a red light. You may need to set the following env variable after sourcing a settings file (doesn't appear necessary)? export LD_PRELOAD=/opt/Xilinx/usb-driver/libusb-driver.so #not needed Environment Then finally, from user home directory, or some local work dir: cd [workdir] source setup_ise.sh impact & setup_ise.sh (/tools/xilinx/FPGA_tutorial) sets paths for the Xilinx tools and the licence server source /tools/xilinx/14.7/ISE_DS/settings64.sh export XILINXD_LICENSE_FILE=8000@faust02.physik.uni-bonn.de This last block is all to should need to on subsequent use. Make sure the gcc gdb and dumptool thing is installed. Make sure permissions are perserved when copying the driver install and projects Try and understand why install digital install script needed /etc/hotplug/usb needed to be manually created remember that Impact bitstream programmer isn't synced with ISE, and so you must manually select a new project's bitstream to be loaded.","title":"To make Vivado work:"},{"location":"setup_xilinx/#to-make-vivado-work","text":"sudo dnf install ncurses-compat-libs source /tools/xilinx/Vivado/2022.2/settings64_fedora.sh","title":"To make Vivado work:"},{"location":"setup_xilinx/#installation-on-penelope-as-asiclab","text":"Following: http://www.diy.ind.in/linux/64-install-xilinx-platform-cable-usb-ii-driver cd /tools/xilinx dnf install git sudo git clone git://git.zerfleddert.de/usb-driver sudo dnf install make gcc glibc-devel cd usb-driver sudo nano Makefile #comment out the line about '-m32', to disable 32bit build sudo make","title":"Installation on penelope, as asiclab"},{"location":"setup_xilinx/#on-client","text":"The one-time initial setup is as follows","title":"On client"},{"location":"setup_xilinx/#digilent-jtag-adapter","text":"cd [workdir] cp -r /tools/xilinx/14.7/ISE_DS/common/bin/lin64/digilent . sudo ./install_digilent.sh","title":"Digilent JTAG adapter"},{"location":"setup_xilinx/#xilinx-jtag-adapter","text":"sudo dnf install fxload libusb1 libusb1-devel libusb-compat-0.1 libusb-compat-0.1-devel sudo ./setup_pcusb /tools/xilinx/14.7/ISE_DS/ISE sudo udevadm control --reload-rules Unplug and replug - and you should see a red light. You may need to set the following env variable after sourcing a settings file (doesn't appear necessary)? export LD_PRELOAD=/opt/Xilinx/usb-driver/libusb-driver.so #not needed","title":"Xilinx JTAG adapter"},{"location":"setup_xilinx/#environment","text":"Then finally, from user home directory, or some local work dir: cd [workdir] source setup_ise.sh impact & setup_ise.sh (/tools/xilinx/FPGA_tutorial) sets paths for the Xilinx tools and the licence server source /tools/xilinx/14.7/ISE_DS/settings64.sh export XILINXD_LICENSE_FILE=8000@faust02.physik.uni-bonn.de This last block is all to should need to on subsequent use. Make sure the gcc gdb and dumptool thing is installed. Make sure permissions are perserved when copying the driver install and projects Try and understand why install digital install script needed /etc/hotplug/usb needed to be manually created remember that Impact bitstream programmer isn't synced with ISE, and so you must manually select a new project's bitstream to be loaded.","title":"Environment"},{"location":"third_party/","text":"H.264 Support As of Fedora 37+, H.264 decoders were removed from the based distribution due to legal reasons (alongside H.265). To install alternative H.264 decoders, you can follow the instructions found here: sudo dnf config-manager --set-enabled fedora-cisco-openh264 and then install the plugins: sudo dnf install gstreamer1-plugin-openh264 mozilla-openh264 Afterwards you need open Firefox, go to menu -> Add-ons -> Plugins and enable OpenH264 plugin. You can do a simple test whether your H.264 works in RTC on this page (check Require H.264 video). how to install and uninstall standalone rpm packages To install, just click it. To uninstall. Execute the following command to discover the name of the installed package: rpm -qa | grep PackageName This returns PackageName, the RPM name of your Micro Focus product which is used to identify the install package. Execute the following command to uninstall the product: rpm -e PackageName How to install vscode VS Code is currently only shipped in a yum repository, so first add the repository: sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc Then install install the key sudo sh -c 'echo -e \"[code]\\nname=Visual Studio Code\\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\\nenabled=1\\ngpgcheck=1\\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc\" > /etc/yum.repos.d/vscode.repo' Update the package cache dnf check-update and install the package using dnf (Fedora 22 and above): sudo dnf install code Additional packages: ngspice wol xclock xpdf pandoc inkscape evolution code seahorse texlive iperf apptainer Zoom Simplest way is just to download the official Fedora.rpm from https:/zoom.us. Then just double click on downloaded file in Files GUI. Will figure out Zoom install via command line later. https://eu02web.zoom.us/support/down4j Python venvs If you're normally used to only Anaconda, stop, and take a deep breath. Weigh the value of your sanity, and then look into how to create Python venvs. This is a new-ish feature in Python, and will make your life better.","title":"H.264 Support"},{"location":"third_party/#h264-support","text":"As of Fedora 37+, H.264 decoders were removed from the based distribution due to legal reasons (alongside H.265). To install alternative H.264 decoders, you can follow the instructions found here: sudo dnf config-manager --set-enabled fedora-cisco-openh264 and then install the plugins: sudo dnf install gstreamer1-plugin-openh264 mozilla-openh264 Afterwards you need open Firefox, go to menu -> Add-ons -> Plugins and enable OpenH264 plugin. You can do a simple test whether your H.264 works in RTC on this page (check Require H.264 video).","title":"H.264 Support"},{"location":"third_party/#how-to-install-and-uninstall-standalone-rpm-packages","text":"To install, just click it. To uninstall. Execute the following command to discover the name of the installed package: rpm -qa | grep PackageName This returns PackageName, the RPM name of your Micro Focus product which is used to identify the install package. Execute the following command to uninstall the product: rpm -e PackageName","title":"how to install and uninstall standalone rpm packages"},{"location":"third_party/#how-to-install-vscode","text":"VS Code is currently only shipped in a yum repository, so first add the repository: sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc Then install install the key sudo sh -c 'echo -e \"[code]\\nname=Visual Studio Code\\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\\nenabled=1\\ngpgcheck=1\\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc\" > /etc/yum.repos.d/vscode.repo' Update the package cache dnf check-update and install the package using dnf (Fedora 22 and above): sudo dnf install code","title":"How to install vscode"},{"location":"third_party/#additional-packages","text":"ngspice wol xclock xpdf pandoc inkscape evolution code seahorse texlive iperf apptainer","title":"Additional packages:"},{"location":"third_party/#zoom","text":"Simplest way is just to download the official Fedora.rpm from https:/zoom.us. Then just double click on downloaded file in Files GUI. Will figure out Zoom install via command line later. https://eu02web.zoom.us/support/down4j","title":"Zoom"},{"location":"third_party/#python-venvs","text":"If you're normally used to only Anaconda, stop, and take a deep breath. Weigh the value of your sanity, and then look into how to create Python venvs. This is a new-ish feature in Python, and will make your life better.","title":"Python venvs"},{"location":"user_management/","text":"Credentials asiclabwin001.physik.uni-bonn.de: admin %nobody<3cds% TL;DR Short-Term Approach for Setting up New Fedora Machine # create groups sudo groupadd -g 200 faust sudo groupadd -g 1001 icdesign sudo groupadd -g 1003 tsmcpdk sudo groupadd -g 1004 tsmcpdk28 # create user sudo useradd -u 37838 -g faust --no-create-home -d /faust/user/kcaisley kcaisley # add user to groups sudo usermod -a -G icdesign kcaisley sudo usermod -a -G tsmcpdk kcaisley sudo usermod -a -G tsmcpdk28 kcaisley sudo usermod -a -G wheel kcaisley # change user password sudo passwd kcaisley Managing Users and Groups: Root is disabled as a login user on Fedora. The asiclab account, with UID = 1000 and GID = 1000 should be created as the default local account on the machine. To list all users on a machine Listing users on a machine This can mean different things: 1) The files for which the UID is set to a certain number. 2) The accounts defined for login in /etc/passwd 3) The groups of UIDs that own all the folders in a home directory Approach 1 and 2 can be done via: cat /etc/passwd To check groups of a user groups kcaisley To check full group and user info of user id kcaisley You can omit username for current user. To check all members in a group getent groups icdesign To list all groups, and their members Groups can be supplied from both /etc/group and from LDAP. The combination of both these will be show in: getent group To add a group with gid sudo groupadd -g 1001 icdesign When you add users to a group, using the -g commands makes it the users primary group, where as the -G flag makes it a secondary group. The primary group is the default GID assigned newly created or copied files. To delete a group: sudo groupdel kcaisley To change the UID of a user sudo usermod -u 2002 kcaisley To change the GID of a group sudo groupmod -g 1001 icdesign To create a group in linux sudo usermod -a -G groupname username To become another user su \u2013 <username> To change a user's password passwd kcaisley To recursively change all UID and GID set on files in a directory chown -R ownername:groupname foldername The groupname can be omitted if not desired. Listing GIDs and UIDs and permissions of Files ls -la When running ls -l , the second column is the number of hardlinks (which is equal to the number of directories, sorta?) Anyways, I can just think of it as the approximate number of directories inside this one. To recursively change file permission chown command doesn't work recursively on hidden files, and so using chmod is the best approach. This affects everything in the current working directory and below. sudo chmod -R 775 . To enable Wheel Group (replacement for sudoers) Tutorial: Adding a user to sudoers and wheel group On Fedora, it is the wheel group the user has to be added to, as this group has full admin privileges. Add a user to the group using the following command: sudo usermod -aG wheel username If adding the user to the group does not work immediately, you may have to edit the /etc/sudoers file to uncomment the line with the group name: $ sudo visudo ... %wheel ALL=(ALL) ALL ... Then logout and back in again. To set the default permissions for new files After users are locally created, and login, check the umask bit to make sure they are creating files properly: umask The umask utility is used to control the file-creation mode mask, which determines the initial value of file permission bits for newly created files. This page on the Arch wiki has good info. running umask shows bits, and umask -S in the derived permissions. Note that the bits are a mask of what should not be set. So 7777 - umask = chmod, sorta. For example umask = 0022 yeilds effectively a mod = 7755, or u=rwx,g=rx,o=rx And umask 0077 means that permission will be 7700 Users and Groups Scheme Local users are UID 1000-1999, and have matching group IDs. LDAP users are UID 2000-2999, and don't have matching groups. User groups are in range 3000-3999. UID: asiclab 1000 (local on each computer) user1 2001 user2 2002 user3 2003 ...etc GID: asiclab 1000, etc (matching local users on each computer) base 3001 (all user directories, default for tools directory) icdesign 3002 (access to cadence/mentor/synonsys tools) tsmc65 3003 tsmc28 3004 FreeIPA Setup: Ansible has both playbooks (which can call modules), or there is the other idea of 'roles'. The FreeIPA with Ansible package uses the latter: https://devops.stackexchange.com/questions/9832/ansible-whats-the-difference-between-task-role-play-and-playbook Starting from Fresh Fedora install sudo firewall-cmd --add-service=freeipa-4 --permanent sudo dnf install freeipa-server Accept all defaults usering ENTER, and at the end type 'yes' to accept to proposed settings. https://serverfault.com/questions/1069847/how-should-we-automount-home-directories-stored-at-different-nfs-paths-at-home how does the homedir work? https://www.freeipa.org/page/Quick_Start_Guide#Web_User_Interface LDAP or freeipa will only be a source for account information: username, password, address, fax number, home directory location. Similar to a phone book. To get file \"sync\" you need a network home directory provided by something like NFS (or SMB if you want Windows support). Create a share on one machine, (for me this is /space/homedirs/$username), mount it on all of your other machines in the same place, and set your homedir in LDAP to be that new location. Combine this with autofs, which you can also manage in FreeIPA, it will mount your homedir from NFS only when required, and when logged off it unmounts the NFS share. I hate stuck NFS mounts. But don't use 'softmounts' This can introduce silent data corruption. This explains the difference between hard and soft mounts Some potential firewall fixes, in case there are probs: https://kenmoini.com/post/2022/04/qnap-nfs-home-directories/ ## Enable Firewalld systemctl enable --now firewalld ## [Optional] Enable Cockpit systemctl enable --now cockpit.socket ## Open the needed Firewall ports firewall-cmd --add-service=cockpit --permanent firewall-cmd --add-service=dns --permanent firewall-cmd --add-service=freeipa-ldap --permanent firewall-cmd --add-service=freeipa-ldaps --permanent firewall-cmd --add-service=http --permanent firewall-cmd --add-service=https --permanent firewall-cmd --add-service=ssh --permanent firewall-cmd --add-port=88/tcp --permanent firewall-cmd --add-port=88/udp --permanent firewall-cmd --add-port=464/tcp --permanent firewall-cmd --add-port=464/udp --permanent firewall-cmd --add-port=8080/tcp --permanent An in depth look at IPA/NFS home directories: https://blog.khmersite.net/2020/09/automating-home-directory-with-ipa/ Adding clients to FreeIPA domain https://fedoramagazine.org/join-fedora-linux-enterprise-domain/ sudo realm join asiclabwin001.physik.uni-bonn.de -v authenticate as admin account. Backing up LDAP data FreeIPA has commands for backup and restore of LDAP data: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/linux_domain_identity_authentication_and_policy_guide/backup-restore sudo ipa-backup --data --online Both flags are use, as they backup data only, and do that backup without restarting the server. Data is saved to /var/lib/ipa/backup/ SSSD Client with LDAP Provider To ping a remote LDAP server and see the users it's providing from a Fedora Linux clientfirst install the ldapsearch tool on your Fedora client if it's not already installed. You can do this by running the following command: sudo dnf install -y openldap-clients sssd sssd-ldap nss-pam-ldapd sssd-common Also, you may need package openssl Also, be sure to get the sssctl command. I can't recall what package providers it. Maybe sssctl-tools or something? Old command from Piotr authconfig --enableldap --enableldapauth --ldapserver=noyce.physik.uni-bonn.de --ldapbasedn=dc=faust,dc=de --enablerfc2307bis --enableforcelegacy --update New command: authselect apply-changes sudo sssctl {config-check,domain-list,user-show,user-checks, debug-level 0x0070, cache-remove,} sudo authselect list sudo authselect select sssd sudo vim /etc/sssd/sssd.conf sudo vim /etc/openldap/ldap.conf # but I think this doesn't apply to our client side? id kcaisley man {many differe docs!} Create and fill out /etc/sssd/sssd.conf with: [domain/default] id_provider = ldap auth_provider = ldap ldap_uri = ldap://noyce.physik.uni-bonn.de ldap_search_base = dc=faust,dc=de cache_credentials = True [sssd] services = nss, pam domains = default [nss] homedir_substring = /faust/user Then sudo vim /etc/openldap/ldap.conf My final config: [domain/default] id_provider = ldap auth_provider = ldap ldap_uri = ldap://noyce.physik.uni-bonn.de ldap_search_base = dc=faust,dc=de ldap_group_search_base = ou=group,dc=faust,dc=de ldap_tls_reqcert = never ldap_schema = rfc2307bis ldap_default_bind_dn = cn=root,dc=faust,dc=de ldap_default_authtok_type = password ldap_default_authtok = %Silab246% cache_credentials = true [sssd] config_file_version = 2 services = nss, pam domains = default [nss] homedir_substring = /faust/use # See ldap.conf(5) for details # This file should be world readable but not world writable. BASE dc=faust,dc=de URI ldap://noyce.physik.uni-bonn.de #SIZELIMIT 12 #TIMELIMIT 15 #DEREF never # When no CA certificates are specified the Shared System Certificates # are in use. In order to have these available along with the ones specified # by TLS_CACERTDIR one has to include them explicitly: #TLS_CACERT /etc/pki/tls/cert.pem # System-wide Crypto Policies provide up to date cipher suite which should # be used unless one needs a finer grinded selection of ciphers. Hence, the # PROFILE=SYSTEM value represents the default behavior which is in place # when no explicit setting is used. (see openssl-ciphers(1) for more info) #TLS_CIPHER_SUITE PROFILE=SYSTEM # Turning this off breaks GSSAPI used with krb5 when rdns = false SASL_NOCANON on finally sudo authselect apply-changes systemctl restart sssd systemctl enable sssd to check contents of LDAP server: ldapsearch -x -H ldap://noyce.physik.uni-bonn.de -b dc=faust,dc=de misc commands: sudo cat /var/log/sssd/sssd.log sudo cat /var/log/sssd/sssd_nss.log sudo cat /var/log/sssd/sssd_pam.log man sssd-ldap [nss] [cache_req_common_process_dp_reply] (0x3f7c0): [CID#265] CR #557: Could not get account info [1432158212]: SSSD is offline [asiclab@asiclab008 ~]$ sudo systemctl status sssd \u25cf sssd.service - System Security Services Daemon Loaded: loaded (/usr/lib/systemd/system/sssd.service; enabled; preset: enabled) Active: active (running) since Tue 2023-03-28 16:26:27 CEST; 12min ago Main PID: 868 (sssd) Tasks: 4 (limit: 76850) Memory: 56.6M CPU: 338ms CGroup: /system.slice/sssd.service \u251c\u2500868 /usr/sbin/sssd -i --logger=files \u251c\u2500922 /usr/libexec/sssd/sssd_be --domain default --uid 0 --gid 0 --logger=files \u251c\u2500939 /usr/libexec/sssd/sssd_nss --uid 0 --gid 0 --logger=files \u2514\u2500940 /usr/libexec/sssd/sssd_pam --uid 0 --gid 0 --logger=files Mar 28 16:26:26 fedora systemd[1]: Starting sssd.service - System Security Services Daemon... Mar 28 16:26:26 fedora sssd[868]: Starting up Mar 28 16:26:26 fedora sssd_be[922]: Starting up Mar 28 16:26:27 fedora sssd_nss[939]: Starting up Mar 28 16:26:27 fedora sssd_pam[940]: Starting up Mar 28 16:26:27 fedora systemd[1]: Started sssd.service - System Security Services Daemon. Mar 28 16:26:51 asiclab008.physik.uni-bonn.de sssd_be[922]: Could not start TLS encryption. unknown error Mar 28 16:28:15 asiclab008.physik.uni-bonn.de sssd_be[922]: Backend is online SEE ALSO sssd(8), sssd.conf(5), sssd-ldap(5), sssd-krb5(5), sssd-simple(5), sssd-ipa(5), sssd-ad(5), sssd-files(5), sssd-sudo(5), sssd-session-recording(5), sss_cache(8), sss_debuglevel(8), sss_obfuscate(8), sss_seed(8), sssd_krb5_locator_plugin(8), sss_ssh_authorizedkeys(8), sss_ssh_knownhostsproxy(8), sssd-ifp(5), pam_sss(8). sss_rpcidmapd(5) sssd-systemtap(5) AUTHORS The SSSD upstream - https://github.com/SSSD/sssd/","title":"Credentials"},{"location":"user_management/#credentials","text":"asiclabwin001.physik.uni-bonn.de: admin %nobody<3cds%","title":"Credentials"},{"location":"user_management/#tldr-short-term-approach-for-setting-up-new-fedora-machine","text":"# create groups sudo groupadd -g 200 faust sudo groupadd -g 1001 icdesign sudo groupadd -g 1003 tsmcpdk sudo groupadd -g 1004 tsmcpdk28 # create user sudo useradd -u 37838 -g faust --no-create-home -d /faust/user/kcaisley kcaisley # add user to groups sudo usermod -a -G icdesign kcaisley sudo usermod -a -G tsmcpdk kcaisley sudo usermod -a -G tsmcpdk28 kcaisley sudo usermod -a -G wheel kcaisley # change user password sudo passwd kcaisley","title":"TL;DR Short-Term Approach for Setting up New Fedora Machine"},{"location":"user_management/#managing-users-and-groups","text":"Root is disabled as a login user on Fedora. The asiclab account, with UID = 1000 and GID = 1000 should be created as the default local account on the machine.","title":"Managing Users and Groups:"},{"location":"user_management/#to-list-all-users-on-a-machine","text":"Listing users on a machine This can mean different things: 1) The files for which the UID is set to a certain number. 2) The accounts defined for login in /etc/passwd 3) The groups of UIDs that own all the folders in a home directory Approach 1 and 2 can be done via: cat /etc/passwd","title":"To list all users on a machine"},{"location":"user_management/#to-check-groups-of-a-user","text":"groups kcaisley","title":"To check groups of a user"},{"location":"user_management/#to-check-full-group-and-user-info-of-user","text":"id kcaisley You can omit username for current user.","title":"To check full group and user info of user"},{"location":"user_management/#to-check-all-members-in-a-group","text":"getent groups icdesign","title":"To check all members in a group"},{"location":"user_management/#to-list-all-groups-and-their-members","text":"Groups can be supplied from both /etc/group and from LDAP. The combination of both these will be show in: getent group","title":"To list all groups, and their members"},{"location":"user_management/#to-add-a-group-with-gid","text":"sudo groupadd -g 1001 icdesign When you add users to a group, using the -g commands makes it the users primary group, where as the -G flag makes it a secondary group. The primary group is the default GID assigned newly created or copied files.","title":"To add a group with gid"},{"location":"user_management/#to-delete-a-group","text":"sudo groupdel kcaisley","title":"To delete a group:"},{"location":"user_management/#to-change-the-uid-of-a-user","text":"sudo usermod -u 2002 kcaisley","title":"To change the UID of a user"},{"location":"user_management/#to-change-the-gid-of-a-group","text":"sudo groupmod -g 1001 icdesign","title":"To change the GID of a group"},{"location":"user_management/#to-create-a-group-in-linux","text":"sudo usermod -a -G groupname username","title":"To create a group in linux"},{"location":"user_management/#to-become-another-user","text":"su \u2013 <username>","title":"To become another user"},{"location":"user_management/#to-change-a-users-password","text":"passwd kcaisley","title":"To change a user's password"},{"location":"user_management/#to-recursively-change-all-uid-and-gid-set-on-files-in-a-directory","text":"chown -R ownername:groupname foldername The groupname can be omitted if not desired.","title":"To recursively change all UID and GID set on files in a directory"},{"location":"user_management/#listing-gids-and-uids-and-permissions-of-files","text":"ls -la When running ls -l , the second column is the number of hardlinks (which is equal to the number of directories, sorta?) Anyways, I can just think of it as the approximate number of directories inside this one.","title":"Listing GIDs and UIDs and permissions of Files"},{"location":"user_management/#to-recursively-change-file-permission","text":"chown command doesn't work recursively on hidden files, and so using chmod is the best approach. This affects everything in the current working directory and below. sudo chmod -R 775 .","title":"To recursively change file permission"},{"location":"user_management/#to-enable-wheel-group-replacement-for-sudoers","text":"Tutorial: Adding a user to sudoers and wheel group On Fedora, it is the wheel group the user has to be added to, as this group has full admin privileges. Add a user to the group using the following command: sudo usermod -aG wheel username If adding the user to the group does not work immediately, you may have to edit the /etc/sudoers file to uncomment the line with the group name: $ sudo visudo ... %wheel ALL=(ALL) ALL ... Then logout and back in again.","title":"To enable Wheel Group (replacement for sudoers)"},{"location":"user_management/#to-set-the-default-permissions-for-new-files","text":"After users are locally created, and login, check the umask bit to make sure they are creating files properly: umask The umask utility is used to control the file-creation mode mask, which determines the initial value of file permission bits for newly created files. This page on the Arch wiki has good info. running umask shows bits, and umask -S in the derived permissions. Note that the bits are a mask of what should not be set. So 7777 - umask = chmod, sorta. For example umask = 0022 yeilds effectively a mod = 7755, or u=rwx,g=rx,o=rx And umask 0077 means that permission will be 7700","title":"To set the default permissions for new files"},{"location":"user_management/#users-and-groups-scheme","text":"Local users are UID 1000-1999, and have matching group IDs. LDAP users are UID 2000-2999, and don't have matching groups. User groups are in range 3000-3999. UID: asiclab 1000 (local on each computer) user1 2001 user2 2002 user3 2003 ...etc GID: asiclab 1000, etc (matching local users on each computer) base 3001 (all user directories, default for tools directory) icdesign 3002 (access to cadence/mentor/synonsys tools) tsmc65 3003 tsmc28 3004","title":"Users and Groups Scheme"},{"location":"user_management/#freeipa-setup","text":"Ansible has both playbooks (which can call modules), or there is the other idea of 'roles'. The FreeIPA with Ansible package uses the latter: https://devops.stackexchange.com/questions/9832/ansible-whats-the-difference-between-task-role-play-and-playbook Starting from Fresh Fedora install sudo firewall-cmd --add-service=freeipa-4 --permanent sudo dnf install freeipa-server Accept all defaults usering ENTER, and at the end type 'yes' to accept to proposed settings. https://serverfault.com/questions/1069847/how-should-we-automount-home-directories-stored-at-different-nfs-paths-at-home how does the homedir work? https://www.freeipa.org/page/Quick_Start_Guide#Web_User_Interface LDAP or freeipa will only be a source for account information: username, password, address, fax number, home directory location. Similar to a phone book. To get file \"sync\" you need a network home directory provided by something like NFS (or SMB if you want Windows support). Create a share on one machine, (for me this is /space/homedirs/$username), mount it on all of your other machines in the same place, and set your homedir in LDAP to be that new location. Combine this with autofs, which you can also manage in FreeIPA, it will mount your homedir from NFS only when required, and when logged off it unmounts the NFS share. I hate stuck NFS mounts. But don't use 'softmounts' This can introduce silent data corruption. This explains the difference between hard and soft mounts","title":"FreeIPA Setup:"},{"location":"user_management/#some-potential-firewall-fixes-in-case-there-are-probs","text":"https://kenmoini.com/post/2022/04/qnap-nfs-home-directories/ ## Enable Firewalld systemctl enable --now firewalld ## [Optional] Enable Cockpit systemctl enable --now cockpit.socket ## Open the needed Firewall ports firewall-cmd --add-service=cockpit --permanent firewall-cmd --add-service=dns --permanent firewall-cmd --add-service=freeipa-ldap --permanent firewall-cmd --add-service=freeipa-ldaps --permanent firewall-cmd --add-service=http --permanent firewall-cmd --add-service=https --permanent firewall-cmd --add-service=ssh --permanent firewall-cmd --add-port=88/tcp --permanent firewall-cmd --add-port=88/udp --permanent firewall-cmd --add-port=464/tcp --permanent firewall-cmd --add-port=464/udp --permanent firewall-cmd --add-port=8080/tcp --permanent","title":"Some potential firewall fixes, in case there are probs:"},{"location":"user_management/#an-in-depth-look-at-ipanfs-home-directories","text":"https://blog.khmersite.net/2020/09/automating-home-directory-with-ipa/","title":"An in depth look at IPA/NFS home directories:"},{"location":"user_management/#adding-clients-to-freeipa-domain","text":"https://fedoramagazine.org/join-fedora-linux-enterprise-domain/ sudo realm join asiclabwin001.physik.uni-bonn.de -v authenticate as admin account.","title":"Adding clients to FreeIPA domain"},{"location":"user_management/#backing-up-ldap-data","text":"FreeIPA has commands for backup and restore of LDAP data: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/linux_domain_identity_authentication_and_policy_guide/backup-restore sudo ipa-backup --data --online Both flags are use, as they backup data only, and do that backup without restarting the server. Data is saved to /var/lib/ipa/backup/","title":"Backing up LDAP data"},{"location":"user_management/#sssd-client-with-ldap-provider","text":"To ping a remote LDAP server and see the users it's providing from a Fedora Linux clientfirst install the ldapsearch tool on your Fedora client if it's not already installed. You can do this by running the following command: sudo dnf install -y openldap-clients sssd sssd-ldap nss-pam-ldapd sssd-common Also, you may need package openssl Also, be sure to get the sssctl command. I can't recall what package providers it. Maybe sssctl-tools or something? Old command from Piotr authconfig --enableldap --enableldapauth --ldapserver=noyce.physik.uni-bonn.de --ldapbasedn=dc=faust,dc=de --enablerfc2307bis --enableforcelegacy --update New command: authselect apply-changes sudo sssctl {config-check,domain-list,user-show,user-checks, debug-level 0x0070, cache-remove,} sudo authselect list sudo authselect select sssd sudo vim /etc/sssd/sssd.conf sudo vim /etc/openldap/ldap.conf # but I think this doesn't apply to our client side? id kcaisley man {many differe docs!} Create and fill out /etc/sssd/sssd.conf with: [domain/default] id_provider = ldap auth_provider = ldap ldap_uri = ldap://noyce.physik.uni-bonn.de ldap_search_base = dc=faust,dc=de cache_credentials = True [sssd] services = nss, pam domains = default [nss] homedir_substring = /faust/user Then sudo vim /etc/openldap/ldap.conf My final config: [domain/default] id_provider = ldap auth_provider = ldap ldap_uri = ldap://noyce.physik.uni-bonn.de ldap_search_base = dc=faust,dc=de ldap_group_search_base = ou=group,dc=faust,dc=de ldap_tls_reqcert = never ldap_schema = rfc2307bis ldap_default_bind_dn = cn=root,dc=faust,dc=de ldap_default_authtok_type = password ldap_default_authtok = %Silab246% cache_credentials = true [sssd] config_file_version = 2 services = nss, pam domains = default [nss] homedir_substring = /faust/use # See ldap.conf(5) for details # This file should be world readable but not world writable. BASE dc=faust,dc=de URI ldap://noyce.physik.uni-bonn.de #SIZELIMIT 12 #TIMELIMIT 15 #DEREF never # When no CA certificates are specified the Shared System Certificates # are in use. In order to have these available along with the ones specified # by TLS_CACERTDIR one has to include them explicitly: #TLS_CACERT /etc/pki/tls/cert.pem # System-wide Crypto Policies provide up to date cipher suite which should # be used unless one needs a finer grinded selection of ciphers. Hence, the # PROFILE=SYSTEM value represents the default behavior which is in place # when no explicit setting is used. (see openssl-ciphers(1) for more info) #TLS_CIPHER_SUITE PROFILE=SYSTEM # Turning this off breaks GSSAPI used with krb5 when rdns = false SASL_NOCANON on finally sudo authselect apply-changes systemctl restart sssd systemctl enable sssd to check contents of LDAP server: ldapsearch -x -H ldap://noyce.physik.uni-bonn.de -b dc=faust,dc=de misc commands: sudo cat /var/log/sssd/sssd.log sudo cat /var/log/sssd/sssd_nss.log sudo cat /var/log/sssd/sssd_pam.log man sssd-ldap [nss] [cache_req_common_process_dp_reply] (0x3f7c0): [CID#265] CR #557: Could not get account info [1432158212]: SSSD is offline [asiclab@asiclab008 ~]$ sudo systemctl status sssd \u25cf sssd.service - System Security Services Daemon Loaded: loaded (/usr/lib/systemd/system/sssd.service; enabled; preset: enabled) Active: active (running) since Tue 2023-03-28 16:26:27 CEST; 12min ago Main PID: 868 (sssd) Tasks: 4 (limit: 76850) Memory: 56.6M CPU: 338ms CGroup: /system.slice/sssd.service \u251c\u2500868 /usr/sbin/sssd -i --logger=files \u251c\u2500922 /usr/libexec/sssd/sssd_be --domain default --uid 0 --gid 0 --logger=files \u251c\u2500939 /usr/libexec/sssd/sssd_nss --uid 0 --gid 0 --logger=files \u2514\u2500940 /usr/libexec/sssd/sssd_pam --uid 0 --gid 0 --logger=files Mar 28 16:26:26 fedora systemd[1]: Starting sssd.service - System Security Services Daemon... Mar 28 16:26:26 fedora sssd[868]: Starting up Mar 28 16:26:26 fedora sssd_be[922]: Starting up Mar 28 16:26:27 fedora sssd_nss[939]: Starting up Mar 28 16:26:27 fedora sssd_pam[940]: Starting up Mar 28 16:26:27 fedora systemd[1]: Started sssd.service - System Security Services Daemon. Mar 28 16:26:51 asiclab008.physik.uni-bonn.de sssd_be[922]: Could not start TLS encryption. unknown error Mar 28 16:28:15 asiclab008.physik.uni-bonn.de sssd_be[922]: Backend is online SEE ALSO sssd(8), sssd.conf(5), sssd-ldap(5), sssd-krb5(5), sssd-simple(5), sssd-ipa(5), sssd-ad(5), sssd-files(5), sssd-sudo(5), sssd-session-recording(5), sss_cache(8), sss_debuglevel(8), sss_obfuscate(8), sss_seed(8), sssd_krb5_locator_plugin(8), sss_ssh_authorizedkeys(8), sss_ssh_knownhostsproxy(8), sssd-ifp(5), pam_sss(8). sss_rpcidmapd(5) sssd-systemtap(5) AUTHORS The SSSD upstream - https://github.com/SSSD/sssd/","title":"SSSD Client with LDAP Provider"}]}